&&&& RUNNING TensorRT.trtexec [TensorRT v8001] # trtexec --onnx=default_model.onnx --calib=model.calib --int8 --saveEngine=tmp.trt --verbose
[12/28/2021-10:07:22] [I] === Model Options ===
[12/28/2021-10:07:22] [I] Format: ONNX
[12/28/2021-10:07:22] [I] Model: default_model.onnx
[12/28/2021-10:07:22] [I] Output:
[12/28/2021-10:07:22] [I] === Build Options ===
[12/28/2021-10:07:22] [I] Max batch: explicit
[12/28/2021-10:07:22] [I] Workspace: 16 MiB
[12/28/2021-10:07:22] [I] minTiming: 1
[12/28/2021-10:07:22] [I] avgTiming: 8
[12/28/2021-10:07:22] [I] Precision: FP32+INT8
[12/28/2021-10:07:22] [I] Calibration: model.calib
[12/28/2021-10:07:22] [I] Refit: Disabled
[12/28/2021-10:07:22] [I] Sparsity: Disabled
[12/28/2021-10:07:22] [I] Safe mode: Disabled
[12/28/2021-10:07:22] [I] Restricted mode: Disabled
[12/28/2021-10:07:22] [I] Save engine: tmp.trt
[12/28/2021-10:07:22] [I] Load engine: 
[12/28/2021-10:07:22] [I] NVTX verbosity: 0
[12/28/2021-10:07:22] [I] Tactic sources: Using default tactic sources
[12/28/2021-10:07:22] [I] timingCacheMode: local
[12/28/2021-10:07:22] [I] timingCacheFile: 
[12/28/2021-10:07:22] [I] Input(s)s format: fp32:CHW
[12/28/2021-10:07:22] [I] Output(s)s format: fp32:CHW
[12/28/2021-10:07:22] [I] Input build shapes: model
[12/28/2021-10:07:22] [I] Input calibration shapes: model
[12/28/2021-10:07:22] [I] === System Options ===
[12/28/2021-10:07:22] [I] Device: 0
[12/28/2021-10:07:22] [I] DLACore: 
[12/28/2021-10:07:22] [I] Plugins:
[12/28/2021-10:07:22] [I] === Inference Options ===
[12/28/2021-10:07:22] [I] Batch: Explicit
[12/28/2021-10:07:22] [I] Input inference shapes: model
[12/28/2021-10:07:22] [I] Iterations: 10
[12/28/2021-10:07:22] [I] Duration: 3s (+ 200ms warm up)
[12/28/2021-10:07:22] [I] Sleep time: 0ms
[12/28/2021-10:07:22] [I] Streams: 1
[12/28/2021-10:07:22] [I] ExposeDMA: Disabled
[12/28/2021-10:07:22] [I] Data transfers: Enabled
[12/28/2021-10:07:22] [I] Spin-wait: Disabled
[12/28/2021-10:07:22] [I] Multithreading: Disabled
[12/28/2021-10:07:22] [I] CUDA Graph: Disabled
[12/28/2021-10:07:22] [I] Separate profiling: Disabled
[12/28/2021-10:07:22] [I] Time Deserialize: Disabled
[12/28/2021-10:07:22] [I] Time Refit: Disabled
[12/28/2021-10:07:22] [I] Skip inference: Disabled
[12/28/2021-10:07:22] [I] Inputs:
[12/28/2021-10:07:22] [I] === Reporting Options ===
[12/28/2021-10:07:22] [I] Verbose: Enabled
[12/28/2021-10:07:22] [I] Averages: 10 inferences
[12/28/2021-10:07:22] [I] Percentile: 99
[12/28/2021-10:07:22] [I] Dump refittable layers:Disabled
[12/28/2021-10:07:22] [I] Dump output: Disabled
[12/28/2021-10:07:22] [I] Profile: Disabled
[12/28/2021-10:07:22] [I] Export timing to JSON file: 
[12/28/2021-10:07:22] [I] Export output to JSON file: 
[12/28/2021-10:07:22] [I] Export profile to JSON file: 
[12/28/2021-10:07:22] [I] 
[12/28/2021-10:07:22] [I] === Device Information ===
[12/28/2021-10:07:22] [I] Selected Device: NVIDIA GeForce RTX 3090
[12/28/2021-10:07:22] [I] Compute Capability: 8.6
[12/28/2021-10:07:22] [I] SMs: 82
[12/28/2021-10:07:22] [I] Compute Clock Rate: 1.695 GHz
[12/28/2021-10:07:22] [I] Device Global Memory: 24260 MiB
[12/28/2021-10:07:22] [I] Shared Memory per SM: 100 KiB
[12/28/2021-10:07:22] [I] Memory Bus Width: 384 bits (ECC disabled)
[12/28/2021-10:07:22] [I] Memory Clock Rate: 9.751 GHz
[12/28/2021-10:07:22] [I] 
[12/28/2021-10:07:22] [I] TensorRT version: 8001
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Proposal version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::Split version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[12/28/2021-10:07:22] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[12/28/2021-10:07:22] [I] [TRT] [MemUsageChange] Init CUDA: CPU +534, GPU +0, now: CPU 541, GPU 3269 (MiB)
[12/28/2021-10:07:23] [I] Start parsing network model
[12/28/2021-10:07:23] [I] [TRT] ----------------------------------------------------------------
[12/28/2021-10:07:23] [I] [TRT] Input filename:   default_model.onnx
[12/28/2021-10:07:23] [I] [TRT] ONNX IR version:  0.0.6
[12/28/2021-10:07:23] [I] [TRT] Opset version:    9
[12/28/2021-10:07:23] [I] [TRT] Producer name:    pytorch
[12/28/2021-10:07:23] [I] [TRT] Producer version: 1.8
[12/28/2021-10:07:23] [I] [TRT] Domain:           
[12/28/2021-10:07:23] [I] [TRT] Model version:    0
[12/28/2021-10:07:23] [I] [TRT] Doc string:       
[12/28/2021-10:07:23] [I] [TRT] ----------------------------------------------------------------
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::ScatterND version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Proposal version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::Split version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[12/28/2021-10:07:23] [V] [TRT] Adding network input: actual_input_1 with dtype: float32, dimensions: (32, 3, 32, 32)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: actual_input_1 for ONNX tensor: actual_input_1
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: fc.module.weight
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: fc.module.bias
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 271
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 272
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 274
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 275
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 277
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 278
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 280
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 281
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 283
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 284
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 286
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 287
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 289
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 290
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 292
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 293
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 295
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 296
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 298
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 299
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 301
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 302
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 304
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 305
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 307
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 308
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 310
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 311
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 313
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 314
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 316
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 317
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 319
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 320
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 322
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 323
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 325
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 326
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 328
[12/28/2021-10:07:23] [V] [TRT] Importing initializer: 329
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_2 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: actual_input_1
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 271
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 272
[12/28/2021-10:07:23] [V] [TRT] Conv_2 [Conv] inputs: [actual_input_1 -> (32, 3, 32, 32)[FLOAT]], [271 -> (64, 3, 7, 7)[FLOAT]], [272 -> (64)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 3, 32, 32)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_2 for ONNX node: Conv_2
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 64, 16, 16)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 270 for ONNX tensor: 270
[12/28/2021-10:07:23] [V] [TRT] Conv_2 [Conv] outputs: [270 -> (32, 64, 16, 16)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_5 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 270
[12/28/2021-10:07:23] [V] [TRT] Relu_5 [Relu] inputs: [270 -> (32, 64, 16, 16)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_5 for ONNX node: Relu_5
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 129 for ONNX tensor: 129
[12/28/2021-10:07:23] [V] [TRT] Relu_5 [Relu] outputs: [129 -> (32, 64, 16, 16)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: MaxPool_8 [MaxPool]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 129
[12/28/2021-10:07:23] [V] [TRT] MaxPool_8 [MaxPool] inputs: [129 -> (32, 64, 16, 16)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: MaxPool_8 for ONNX node: MaxPool_8
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 132 for ONNX tensor: 132
[12/28/2021-10:07:23] [V] [TRT] MaxPool_8 [MaxPool] outputs: [132 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_11 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 132
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 274
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 275
[12/28/2021-10:07:23] [V] [TRT] Conv_11 [Conv] inputs: [132 -> (32, 64, 8, 8)[FLOAT]], [274 -> (64, 64, 3, 3)[FLOAT]], [275 -> (64)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_11 for ONNX node: Conv_11
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 273 for ONNX tensor: 273
[12/28/2021-10:07:23] [V] [TRT] Conv_11 [Conv] outputs: [273 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_14 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 273
[12/28/2021-10:07:23] [V] [TRT] Relu_14 [Relu] inputs: [273 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_14 for ONNX node: Relu_14
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 139 for ONNX tensor: 139
[12/28/2021-10:07:23] [V] [TRT] Relu_14 [Relu] outputs: [139 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_17 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 139
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 277
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 278
[12/28/2021-10:07:23] [V] [TRT] Conv_17 [Conv] inputs: [139 -> (32, 64, 8, 8)[FLOAT]], [277 -> (64, 64, 3, 3)[FLOAT]], [278 -> (64)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_17 for ONNX node: Conv_17
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 276 for ONNX tensor: 276
[12/28/2021-10:07:23] [V] [TRT] Conv_17 [Conv] outputs: [276 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_18 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 276
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 132
[12/28/2021-10:07:23] [V] [TRT] Add_18 [Add] inputs: [276 -> (32, 64, 8, 8)[FLOAT]], [132 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_18 for ONNX node: Add_18
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 144 for ONNX tensor: 144
[12/28/2021-10:07:23] [V] [TRT] Add_18 [Add] outputs: [144 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_21 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 144
[12/28/2021-10:07:23] [V] [TRT] Relu_21 [Relu] inputs: [144 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_21 for ONNX node: Relu_21
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 147 for ONNX tensor: 147
[12/28/2021-10:07:23] [V] [TRT] Relu_21 [Relu] outputs: [147 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_24 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 147
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 280
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 281
[12/28/2021-10:07:23] [V] [TRT] Conv_24 [Conv] inputs: [147 -> (32, 64, 8, 8)[FLOAT]], [280 -> (64, 64, 3, 3)[FLOAT]], [281 -> (64)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_24 for ONNX node: Conv_24
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 279 for ONNX tensor: 279
[12/28/2021-10:07:23] [V] [TRT] Conv_24 [Conv] outputs: [279 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_27 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 279
[12/28/2021-10:07:23] [V] [TRT] Relu_27 [Relu] inputs: [279 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_27 for ONNX node: Relu_27
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 154 for ONNX tensor: 154
[12/28/2021-10:07:23] [V] [TRT] Relu_27 [Relu] outputs: [154 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_30 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 154
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 283
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 284
[12/28/2021-10:07:23] [V] [TRT] Conv_30 [Conv] inputs: [154 -> (32, 64, 8, 8)[FLOAT]], [283 -> (64, 64, 3, 3)[FLOAT]], [284 -> (64)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_30 for ONNX node: Conv_30
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 282 for ONNX tensor: 282
[12/28/2021-10:07:23] [V] [TRT] Conv_30 [Conv] outputs: [282 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_31 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 282
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 147
[12/28/2021-10:07:23] [V] [TRT] Add_31 [Add] inputs: [282 -> (32, 64, 8, 8)[FLOAT]], [147 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_31 for ONNX node: Add_31
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 159 for ONNX tensor: 159
[12/28/2021-10:07:23] [V] [TRT] Add_31 [Add] outputs: [159 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_34 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 159
[12/28/2021-10:07:23] [V] [TRT] Relu_34 [Relu] inputs: [159 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_34 for ONNX node: Relu_34
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 162 for ONNX tensor: 162
[12/28/2021-10:07:23] [V] [TRT] Relu_34 [Relu] outputs: [162 -> (32, 64, 8, 8)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_37 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 162
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 286
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 287
[12/28/2021-10:07:23] [V] [TRT] Conv_37 [Conv] inputs: [162 -> (32, 64, 8, 8)[FLOAT]], [286 -> (128, 64, 3, 3)[FLOAT]], [287 -> (128)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_37 for ONNX node: Conv_37
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 285 for ONNX tensor: 285
[12/28/2021-10:07:23] [V] [TRT] Conv_37 [Conv] outputs: [285 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_40 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 285
[12/28/2021-10:07:23] [V] [TRT] Relu_40 [Relu] inputs: [285 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_40 for ONNX node: Relu_40
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 169 for ONNX tensor: 169
[12/28/2021-10:07:23] [V] [TRT] Relu_40 [Relu] outputs: [169 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_43 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 169
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 289
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 290
[12/28/2021-10:07:23] [V] [TRT] Conv_43 [Conv] inputs: [169 -> (32, 128, 4, 4)[FLOAT]], [289 -> (128, 128, 3, 3)[FLOAT]], [290 -> (128)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_43 for ONNX node: Conv_43
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 288 for ONNX tensor: 288
[12/28/2021-10:07:23] [V] [TRT] Conv_43 [Conv] outputs: [288 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_46 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 162
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 292
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 293
[12/28/2021-10:07:23] [V] [TRT] Conv_46 [Conv] inputs: [162 -> (32, 64, 8, 8)[FLOAT]], [292 -> (128, 64, 1, 1)[FLOAT]], [293 -> (128)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 64, 8, 8)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_46 for ONNX node: Conv_46
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 291 for ONNX tensor: 291
[12/28/2021-10:07:23] [V] [TRT] Conv_46 [Conv] outputs: [291 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_47 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 288
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 291
[12/28/2021-10:07:23] [V] [TRT] Add_47 [Add] inputs: [288 -> (32, 128, 4, 4)[FLOAT]], [291 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_47 for ONNX node: Add_47
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 178 for ONNX tensor: 178
[12/28/2021-10:07:23] [V] [TRT] Add_47 [Add] outputs: [178 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_50 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 178
[12/28/2021-10:07:23] [V] [TRT] Relu_50 [Relu] inputs: [178 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_50 for ONNX node: Relu_50
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 181 for ONNX tensor: 181
[12/28/2021-10:07:23] [V] [TRT] Relu_50 [Relu] outputs: [181 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_53 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 181
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 295
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 296
[12/28/2021-10:07:23] [V] [TRT] Conv_53 [Conv] inputs: [181 -> (32, 128, 4, 4)[FLOAT]], [295 -> (128, 128, 3, 3)[FLOAT]], [296 -> (128)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_53 for ONNX node: Conv_53
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 294 for ONNX tensor: 294
[12/28/2021-10:07:23] [V] [TRT] Conv_53 [Conv] outputs: [294 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_56 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 294
[12/28/2021-10:07:23] [V] [TRT] Relu_56 [Relu] inputs: [294 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_56 for ONNX node: Relu_56
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 188 for ONNX tensor: 188
[12/28/2021-10:07:23] [V] [TRT] Relu_56 [Relu] outputs: [188 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_59 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 188
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 298
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 299
[12/28/2021-10:07:23] [V] [TRT] Conv_59 [Conv] inputs: [188 -> (32, 128, 4, 4)[FLOAT]], [298 -> (128, 128, 3, 3)[FLOAT]], [299 -> (128)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_59 for ONNX node: Conv_59
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 297 for ONNX tensor: 297
[12/28/2021-10:07:23] [V] [TRT] Conv_59 [Conv] outputs: [297 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_60 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 297
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 181
[12/28/2021-10:07:23] [V] [TRT] Add_60 [Add] inputs: [297 -> (32, 128, 4, 4)[FLOAT]], [181 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_60 for ONNX node: Add_60
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 193 for ONNX tensor: 193
[12/28/2021-10:07:23] [V] [TRT] Add_60 [Add] outputs: [193 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_63 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 193
[12/28/2021-10:07:23] [V] [TRT] Relu_63 [Relu] inputs: [193 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_63 for ONNX node: Relu_63
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 196 for ONNX tensor: 196
[12/28/2021-10:07:23] [V] [TRT] Relu_63 [Relu] outputs: [196 -> (32, 128, 4, 4)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_66 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 196
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 301
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 302
[12/28/2021-10:07:23] [V] [TRT] Conv_66 [Conv] inputs: [196 -> (32, 128, 4, 4)[FLOAT]], [301 -> (256, 128, 3, 3)[FLOAT]], [302 -> (256)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_66 for ONNX node: Conv_66
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 300 for ONNX tensor: 300
[12/28/2021-10:07:23] [V] [TRT] Conv_66 [Conv] outputs: [300 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_69 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 300
[12/28/2021-10:07:23] [V] [TRT] Relu_69 [Relu] inputs: [300 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_69 for ONNX node: Relu_69
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 203 for ONNX tensor: 203
[12/28/2021-10:07:23] [V] [TRT] Relu_69 [Relu] outputs: [203 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_72 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 203
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 304
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 305
[12/28/2021-10:07:23] [V] [TRT] Conv_72 [Conv] inputs: [203 -> (32, 256, 2, 2)[FLOAT]], [304 -> (256, 256, 3, 3)[FLOAT]], [305 -> (256)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_72 for ONNX node: Conv_72
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 303 for ONNX tensor: 303
[12/28/2021-10:07:23] [V] [TRT] Conv_72 [Conv] outputs: [303 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_75 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 196
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 307
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 308
[12/28/2021-10:07:23] [V] [TRT] Conv_75 [Conv] inputs: [196 -> (32, 128, 4, 4)[FLOAT]], [307 -> (256, 128, 1, 1)[FLOAT]], [308 -> (256)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 128, 4, 4)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_75 for ONNX node: Conv_75
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 306 for ONNX tensor: 306
[12/28/2021-10:07:23] [V] [TRT] Conv_75 [Conv] outputs: [306 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_76 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 303
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 306
[12/28/2021-10:07:23] [V] [TRT] Add_76 [Add] inputs: [303 -> (32, 256, 2, 2)[FLOAT]], [306 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_76 for ONNX node: Add_76
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 212 for ONNX tensor: 212
[12/28/2021-10:07:23] [V] [TRT] Add_76 [Add] outputs: [212 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_79 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 212
[12/28/2021-10:07:23] [V] [TRT] Relu_79 [Relu] inputs: [212 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_79 for ONNX node: Relu_79
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 215 for ONNX tensor: 215
[12/28/2021-10:07:23] [V] [TRT] Relu_79 [Relu] outputs: [215 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_82 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 215
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 310
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 311
[12/28/2021-10:07:23] [V] [TRT] Conv_82 [Conv] inputs: [215 -> (32, 256, 2, 2)[FLOAT]], [310 -> (256, 256, 3, 3)[FLOAT]], [311 -> (256)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_82 for ONNX node: Conv_82
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 309 for ONNX tensor: 309
[12/28/2021-10:07:23] [V] [TRT] Conv_82 [Conv] outputs: [309 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_85 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 309
[12/28/2021-10:07:23] [V] [TRT] Relu_85 [Relu] inputs: [309 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_85 for ONNX node: Relu_85
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 222 for ONNX tensor: 222
[12/28/2021-10:07:23] [V] [TRT] Relu_85 [Relu] outputs: [222 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_88 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 222
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 313
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 314
[12/28/2021-10:07:23] [V] [TRT] Conv_88 [Conv] inputs: [222 -> (32, 256, 2, 2)[FLOAT]], [313 -> (256, 256, 3, 3)[FLOAT]], [314 -> (256)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_88 for ONNX node: Conv_88
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 312 for ONNX tensor: 312
[12/28/2021-10:07:23] [V] [TRT] Conv_88 [Conv] outputs: [312 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_89 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 312
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 215
[12/28/2021-10:07:23] [V] [TRT] Add_89 [Add] inputs: [312 -> (32, 256, 2, 2)[FLOAT]], [215 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_89 for ONNX node: Add_89
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 227 for ONNX tensor: 227
[12/28/2021-10:07:23] [V] [TRT] Add_89 [Add] outputs: [227 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_92 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 227
[12/28/2021-10:07:23] [V] [TRT] Relu_92 [Relu] inputs: [227 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_92 for ONNX node: Relu_92
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 230 for ONNX tensor: 230
[12/28/2021-10:07:23] [V] [TRT] Relu_92 [Relu] outputs: [230 -> (32, 256, 2, 2)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_95 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 230
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 316
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 317
[12/28/2021-10:07:23] [V] [TRT] Conv_95 [Conv] inputs: [230 -> (32, 256, 2, 2)[FLOAT]], [316 -> (512, 256, 3, 3)[FLOAT]], [317 -> (512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_95 for ONNX node: Conv_95
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 315 for ONNX tensor: 315
[12/28/2021-10:07:23] [V] [TRT] Conv_95 [Conv] outputs: [315 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_98 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 315
[12/28/2021-10:07:23] [V] [TRT] Relu_98 [Relu] inputs: [315 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_98 for ONNX node: Relu_98
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 237 for ONNX tensor: 237
[12/28/2021-10:07:23] [V] [TRT] Relu_98 [Relu] outputs: [237 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_101 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 237
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 319
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 320
[12/28/2021-10:07:23] [V] [TRT] Conv_101 [Conv] inputs: [237 -> (32, 512, 1, 1)[FLOAT]], [319 -> (512, 512, 3, 3)[FLOAT]], [320 -> (512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_101 for ONNX node: Conv_101
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 318 for ONNX tensor: 318
[12/28/2021-10:07:23] [V] [TRT] Conv_101 [Conv] outputs: [318 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_104 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 230
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 322
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 323
[12/28/2021-10:07:23] [V] [TRT] Conv_104 [Conv] inputs: [230 -> (32, 256, 2, 2)[FLOAT]], [322 -> (512, 256, 1, 1)[FLOAT]], [323 -> (512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 256, 2, 2)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_104 for ONNX node: Conv_104
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 321 for ONNX tensor: 321
[12/28/2021-10:07:23] [V] [TRT] Conv_104 [Conv] outputs: [321 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_105 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 318
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 321
[12/28/2021-10:07:23] [V] [TRT] Add_105 [Add] inputs: [318 -> (32, 512, 1, 1)[FLOAT]], [321 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_105 for ONNX node: Add_105
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 246 for ONNX tensor: 246
[12/28/2021-10:07:23] [V] [TRT] Add_105 [Add] outputs: [246 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_108 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 246
[12/28/2021-10:07:23] [V] [TRT] Relu_108 [Relu] inputs: [246 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_108 for ONNX node: Relu_108
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 249 for ONNX tensor: 249
[12/28/2021-10:07:23] [V] [TRT] Relu_108 [Relu] outputs: [249 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_111 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 249
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 325
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 326
[12/28/2021-10:07:23] [V] [TRT] Conv_111 [Conv] inputs: [249 -> (32, 512, 1, 1)[FLOAT]], [325 -> (512, 512, 3, 3)[FLOAT]], [326 -> (512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_111 for ONNX node: Conv_111
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 324 for ONNX tensor: 324
[12/28/2021-10:07:23] [V] [TRT] Conv_111 [Conv] outputs: [324 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_114 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 324
[12/28/2021-10:07:23] [V] [TRT] Relu_114 [Relu] inputs: [324 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_114 for ONNX node: Relu_114
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 256 for ONNX tensor: 256
[12/28/2021-10:07:23] [V] [TRT] Relu_114 [Relu] outputs: [256 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Conv_117 [Conv]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 256
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 328
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 329
[12/28/2021-10:07:23] [V] [TRT] Conv_117 [Conv] inputs: [256 -> (32, 512, 1, 1)[FLOAT]], [328 -> (512, 512, 3, 3)[FLOAT]], [329 -> (512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Convolution input dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Conv_117 for ONNX node: Conv_117
[12/28/2021-10:07:23] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512
[12/28/2021-10:07:23] [V] [TRT] Convolution output dimensions: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 327 for ONNX tensor: 327
[12/28/2021-10:07:23] [V] [TRT] Conv_117 [Conv] outputs: [327 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Add_118 [Add]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 327
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 249
[12/28/2021-10:07:23] [V] [TRT] Add_118 [Add] inputs: [327 -> (32, 512, 1, 1)[FLOAT]], [249 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Add_118 for ONNX node: Add_118
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 261 for ONNX tensor: 261
[12/28/2021-10:07:23] [V] [TRT] Add_118 [Add] outputs: [261 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Relu_121 [Relu]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 261
[12/28/2021-10:07:23] [V] [TRT] Relu_121 [Relu] inputs: [261 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Relu_121 for ONNX node: Relu_121
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 264 for ONNX tensor: 264
[12/28/2021-10:07:23] [V] [TRT] Relu_121 [Relu] outputs: [264 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: GlobalAveragePool_122 [GlobalAveragePool]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 264
[12/28/2021-10:07:23] [V] [TRT] GlobalAveragePool_122 [GlobalAveragePool] inputs: [264 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: GlobalAveragePool_122 for ONNX node: GlobalAveragePool_122
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 265 for ONNX tensor: 265
[12/28/2021-10:07:23] [V] [TRT] GlobalAveragePool_122 [GlobalAveragePool] outputs: [265 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Flatten_123 [Flatten]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 265
[12/28/2021-10:07:23] [V] [TRT] Flatten_123 [Flatten] inputs: [265 -> (32, 512, 1, 1)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Flatten_123 for ONNX node: Flatten_123
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: 266 for ONNX tensor: 266
[12/28/2021-10:07:23] [V] [TRT] Flatten_123 [Flatten] outputs: [266 -> (32, 512)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Parsing node: Gemm_126 [Gemm]
[12/28/2021-10:07:23] [V] [TRT] Searching for input: 266
[12/28/2021-10:07:23] [V] [TRT] Searching for input: fc.module.weight
[12/28/2021-10:07:23] [V] [TRT] Searching for input: fc.module.bias
[12/28/2021-10:07:23] [V] [TRT] Gemm_126 [Gemm] inputs: [266 -> (32, 512)[FLOAT]], [fc.module.weight -> (10, 512)[FLOAT]], [fc.module.bias -> (10)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] GEMM: using FC layer instead of MM because all criteria were met.
[12/28/2021-10:07:23] [V] [TRT] Original shape: (32, 512), unsqueezing to: (32, 512, 1, 1)
[12/28/2021-10:07:23] [V] [TRT] Registering layer: Gemm_126 for ONNX node: Gemm_126
[12/28/2021-10:07:23] [V] [TRT] Original shape: (32, 10, 1, 1), squeezing to: (32, 10)
[12/28/2021-10:07:23] [V] [TRT] Registering tensor: output1_0 for ONNX tensor: output1
[12/28/2021-10:07:23] [V] [TRT] Gemm_126 [Gemm] outputs: [output1 -> (32, 10)[FLOAT]], 
[12/28/2021-10:07:23] [V] [TRT] Marking output1_0 as output: output1
[12/28/2021-10:07:23] [I] Finish parsing network model
[12/28/2021-10:07:23] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 582, GPU 3269 (MiB)
[12/28/2021-10:07:23] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best
[12/28/2021-10:07:23] [I] [TRT] [MemUsageSnapshot] Builder begin: CPU 582 MiB, GPU 3269 MiB
[12/28/2021-10:07:23] [V] [TRT] Original: 51 layers
[12/28/2021-10:07:23] [V] [TRT] After dead-layer removal: 51 layers
[12/28/2021-10:07:23] [V] [TRT] Convert layer type of Gemm_126 from FULLY_CONNECTED to CONVOLUTION
[12/28/2021-10:07:23] [V] [TRT] Removing shuffle_between_(Unnamed Layer* 48) [Shuffle]_output_and_Gemm_126
[12/28/2021-10:07:23] [V] [TRT] After scale fusion: 51 layers
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_5 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_14 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_21 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_27 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_34 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_40 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_50 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_56 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_63 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_69 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_79 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_85 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_92 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_98 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_108 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_114 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of Relu_121 from ACTIVATION to POINTWISE
[12/28/2021-10:07:23] [V] [TRT] After vertical fusions: 51 layers
[12/28/2021-10:07:23] [V] [TRT] After final dead-layer removal: 51 layers
[12/28/2021-10:07:23] [V] [TRT] After concat removal: 51 layers
[12/28/2021-10:07:23] [V] [TRT] After tensor merging: 51 layers
[12/28/2021-10:07:23] [I] [TRT] Reading Calibration Cache for calibrator: EntropyCalibration2
[12/28/2021-10:07:23] [I] [TRT] Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales.
[12/28/2021-10:07:23] [I] [TRT] To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache.
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: actual_input_1 scale and zero-point Quantization(scale: {0.0203247,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 270 scale and zero-point Quantization(scale: {0.0761741,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 129 scale and zero-point Quantization(scale: {0.0297118,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 132 scale and zero-point Quantization(scale: {0.0225558,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 273 scale and zero-point Quantization(scale: {0.0523693,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 139 scale and zero-point Quantization(scale: {0.0209975,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 276 scale and zero-point Quantization(scale: {0.0370378,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 147 scale and zero-point Quantization(scale: {0.0256307,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 279 scale and zero-point Quantization(scale: {0.0829737,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 154 scale and zero-point Quantization(scale: {0.0315438,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 282 scale and zero-point Quantization(scale: {0.0415631,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 162 scale and zero-point Quantization(scale: {0.0339319,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 285 scale and zero-point Quantization(scale: {0.0664687,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 169 scale and zero-point Quantization(scale: {0.0277645,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 288 scale and zero-point Quantization(scale: {0.0462997,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 291 scale and zero-point Quantization(scale: {0.0460983,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 181 scale and zero-point Quantization(scale: {0.0296639,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 294 scale and zero-point Quantization(scale: {0.0453227,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 188 scale and zero-point Quantization(scale: {0.0232894,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 297 scale and zero-point Quantization(scale: {0.0299404,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 196 scale and zero-point Quantization(scale: {0.0280925,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 300 scale and zero-point Quantization(scale: {0.0301538,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 203 scale and zero-point Quantization(scale: {0.0142102,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 303 scale and zero-point Quantization(scale: {0.0270986,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 306 scale and zero-point Quantization(scale: {0.0111315,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 215 scale and zero-point Quantization(scale: {0.0161066,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 309 scale and zero-point Quantization(scale: {0.0264487,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 222 scale and zero-point Quantization(scale: {0.0172282,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 312 scale and zero-point Quantization(scale: {0.0193074,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 230 scale and zero-point Quantization(scale: {0.0195712,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 315 scale and zero-point Quantization(scale: {0.0349623,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 237 scale and zero-point Quantization(scale: {0.0256173,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 318 scale and zero-point Quantization(scale: {0.0344227,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 321 scale and zero-point Quantization(scale: {0.0323227,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 249 scale and zero-point Quantization(scale: {0.0315835,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 324 scale and zero-point Quantization(scale: {0.0246684,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 256 scale and zero-point Quantization(scale: {0.0138321,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 327 scale and zero-point Quantization(scale: {0.104862,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 264 scale and zero-point Quantization(scale: {0.0211667,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: 266 scale and zero-point Quantization(scale: {0.0593414,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] INT8 Inference Tensor scales and zero-points: output1 scale and zero-point Quantization(scale: {0.0723407,}, zero-point: {})
[12/28/2021-10:07:23] [V] [TRT] Configuring builder for Int8 Mode completed in 0.00171673 seconds.
[12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [12/28/2021-10:07:23] [V] [TRT] Applying generic optimizations to the graph for inference.
[12/28/2021-10:07:23] [V] [TRT] Original: 51 layers
[12/28/2021-10:07:23] [V] [TRT] After dead-layer removal: 51 layers
[12/28/2021-10:07:23] [V] [TRT] ShuffleShuffleFusion: Fusing Flatten_123 with (Unnamed Layer* 48) [Shuffle]
[12/28/2021-10:07:23] [V] [TRT] Removing Flatten_123 + (Unnamed Layer* 48) [Shuffle]
[12/28/2021-10:07:23] [V] [TRT] After Myelin optimization: 49 layers
[12/28/2021-10:07:23] [V] [TRT] Convert layer type of Gemm_126 from FULLY_CONNECTED to CONVOLUTION
[12/28/2021-10:07:23] [V] [TRT] Removing shuffle_between_265_and_Gemm_126
[12/28/2021-10:07:23] [V] [TRT] After scale fusion: 49 layers
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_2 with Relu_5
[12/28/2021-10:07:23] [V] [TRT] ConvActPoolFusion: Fusing Conv_2 + Relu_5 with MaxPool_8
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_11 with Relu_14
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_17 with Add_18
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_17 + Add_18 with Relu_21
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_24 with Relu_27
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_30 with Add_31
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_30 + Add_31 with Relu_34
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_37 with Relu_40
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_43 with Add_47
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_43 + Add_47 with Relu_50
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_53 with Relu_56
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_59 with Add_60
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_59 + Add_60 with Relu_63
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_66 with Relu_69
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_72 with Add_76
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_72 + Add_76 with Relu_79
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_82 with Relu_85
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_88 with Add_89
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_88 + Add_89 with Relu_92
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_95 with Relu_98
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_101 with Add_105
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_101 + Add_105 with Relu_108
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_111 with Relu_114
[12/28/2021-10:07:23] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_117 with Add_118
[12/28/2021-10:07:23] [V] [TRT] ConvReluFusion: Fusing Conv_117 + Add_118 with Relu_121
[12/28/2021-10:07:23] [V] [TRT] Swap the layer type of GlobalAveragePool_122 from REDUCE to POOLING
[12/28/2021-10:07:23] [V] [TRT] After vertical fusions: 23 layers
[12/28/2021-10:07:23] [V] [TRT] After dupe layer removal: 23 layers
[12/28/2021-10:07:23] [V] [TRT] After final dead-layer removal: 23 layers
[12/28/2021-10:07:23] [V] [TRT] After tensor merging: 23 layers
[12/28/2021-10:07:23] [V] [TRT] After concat removal: 23 layers
[12/28/2021-10:07:23] [V] [TRT] Graph construction and optimization completed in 0.0263698 seconds.
[12/28/2021-10:07:23] [V] [TRT] Using cublasLt a tactic source
[12/28/2021-10:07:23] [12/28/2021-10:07:23] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +637, GPU +266, now: CPU 1219, GPU 3535 (MiB)
[12/28/2021-10:07:23] [V] [TRT] Using cuDNN as a tactic source
[12/28/2021-10:07:24] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +358, GPU +260, now: CPU 1577, GPU 3795 (MiB)
[12/28/2021-10:07:24] [12/28/2021-10:07:24] [12/28/2021-10:07:24] [V] [TRT] Constructing optimization profile number 0 [1/1].
[12/28/2021-10:07:24] [V] [TRT] Rejecting some int8 implementation of layer GlobalAveragePool_122 due to missing int8 scales for tensor 265 at output index 0
[12/28/2021-10:07:24] [V] [TRT] Rejecting some int8 implementation of layer Gemm_126 due to missing int8 scales for tensor 265 at input index 0
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(3072,1024,32,1) -> Int8(3072,1024,32,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.039916
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.00546
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.00546
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning format combination: Int8(3072,1024,32,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Conv_2 + Relu_5 + MaxPool_8 (ConvActPool)
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1000 Time: 0.014764
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1100 Time: 0.014616
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1110 Time: 0.015808
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1111 Time: 0.014604
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1112 Time: 0.018052
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1120 Time: 0.015908
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1121 Time: 0.014868
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1122 Time: 0.018168
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1130 Time: 0.015992
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1131 Time: 0.0146
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1132 Time: 0.018128
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1140 Time: 0.016064
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1141 Time: 0.014868
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1142 Time: 0.018272
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 1131 Time: 0.0146
[12/28/2021-10:07:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: ConvActPool Tactic: 1131
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007944
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006272
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006272
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007644
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006296
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006296
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008796
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006632
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006632
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00828
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.009992
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 1002 Time: 0.00828
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00748
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.005088
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1 Time: 0.005556
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.005088
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008616
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006304
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006304
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008756
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006352
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006352
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007496
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.005052
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.005052
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007548
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.007776
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 1002 Time: 0.007548
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008736
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006372
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006372
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00778
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.00612
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.00612
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.0077
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006612
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006612
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007616
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.008404
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 1002 Time: 0.007616
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00876
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006356
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006356
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007648
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006132
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006132
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008716
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006728
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006728
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008816
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.008344
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.008344
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008756
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006476
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006476
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00862
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006204
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006204
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007516
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006268
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006268
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007916
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.0066
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.0066
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008076
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.008272
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 1002 Time: 0.008076
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008008
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.005368
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.005368
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008428
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.00656
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.00656
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008952
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.0067
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.0067
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007572
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.00556
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.00556
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.00798
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006492
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006492
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007828
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.006556
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.006556
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.008912
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.00678
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.00678
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1002 Time: 0.007368
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 0 Time: 0.004976
[12/28/2021-10:07:24] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:24] [V] [TRT] Tactic: 1 Time: 0.005572
[12/28/2021-10:07:24] [V] [TRT] Fastest Tactic: 0 Time: 0.004976
[12/28/2021-10:07:24] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudaDepthwiseConvolution)
[12/28/2021-10:07:24] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:24] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (FusedConvActConvolution)
[12/28/2021-10:07:24] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudnnConvolution)
[12/28/2021-10:07:25] [V] [TRT] Tactic: 0 Time: 0.035544
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1 Time: 0.06852
[12/28/2021-10:07:25] [V] [TRT] Tactic: 2 Time: 0.067948
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4 Time: 0.047364
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:25] [V] [TRT] Tactic: 6 Time: 0.027336
[12/28/2021-10:07:25] [V] [TRT] Tactic: 56 Time: 0.035884
[12/28/2021-10:07:25] [V] [TRT] Tactic: 57 Time: 0.071716
[12/28/2021-10:07:25] [V] [TRT] Tactic: 58 Time: 0.067616
[12/28/2021-10:07:25] [V] [TRT] Tactic: 60 Time: 0.047664
[12/28/2021-10:07:25] [V] [TRT] Tactic: 61 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:25] [V] [TRT] Tactic: 62 Time: 0.027284
[12/28/2021-10:07:25] [V] [TRT] Tactic: 112 Time: 0.035756
[12/28/2021-10:07:25] [V] [TRT] Tactic: 113 Time: 0.228024
[12/28/2021-10:07:25] [V] [TRT] Tactic: 114 Time: 0.06794
[12/28/2021-10:07:25] [V] [TRT] Tactic: 116 Time: 0.047776
[12/28/2021-10:07:25] [V] [TRT] Tactic: 117 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:25] [V] [TRT] Tactic: 118 Time: 0.027344
[12/28/2021-10:07:25] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: 62 Time: 0.027284
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4549827808004681195 Time: 0.063076
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5779835512569528575 Time: 0.079892
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:25] [V] [TRT] Tactic: 6053873026024413720 Time: 0.089404
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:25] [V] [TRT] Tactic: 6767548733843469815 Time: 0.069228
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:25] [V] [TRT] Tactic: -6313876406580483184 Time: 0.075608
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:25] [V] [TRT] Tactic: -1123676555321336786 Time: 0.087784
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:25] [V] [TRT] Tactic: -701551393537224327 Time: 0.076976
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.063076
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 62
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudnnConvolution)
[12/28/2021-10:07:25] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:25] [V] [TRT] Tactic: 2860655430572478466 Time: 0.049108
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4474630279712975759 Time: 0.030604
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4479823862704990365 Time: 0.029932
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050012
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5778138195697110003 Time: 0.079344
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:25] [V] [TRT] Tactic: 7155825427510256858 Time: 0.079692
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:25] [V] [TRT] Tactic: 8918020581761223752 Time: 0.078064
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:25] [V] [TRT] Tactic: -4756382386362004279 Time: 0.04918
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:25] [V] [TRT] Tactic: -3855385237722507464 Time: 0.080696
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:25] [V] [TRT] Tactic: -2809379259463049391 Time: 0.07936
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:25] [V] [TRT] Tactic: -504296718212024303 Time: 0.076944
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.029932
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudnnConvolution)
[12/28/2021-10:07:25] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:25] [V] [TRT] Tactic: 2860655430572478466 Time: 0.049304
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4474630279712975759 Time: 0.030552
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4479823862704990365 Time: 0.029908
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050068
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5778138195697110003 Time: 0.079128
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:25] [V] [TRT] Tactic: 7155825427510256858 Time: 0.07962
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:25] [V] [TRT] Tactic: 7342025736444949634 Time: 0.054776
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:25] [V] [TRT] Tactic: 8918020581761223752 Time: 0.078088
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:25] [V] [TRT] Tactic: -7377458734869418330 Time: 0.054148
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:25] [V] [TRT] Tactic: -5457304872213719461 Time: 0.054624
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:25] [V] [TRT] Tactic: -4756382386362004279 Time: 0.049168
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:25] [V] [TRT] Tactic: -3855385237722507464 Time: 0.080516
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:25] [V] [TRT] Tactic: -2809379259463049391 Time: 0.07932
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:25] [V] [TRT] Tactic: -504296718212024303 Time: 0.0769
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.029908
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudaDepthwiseConvolution)
[12/28/2021-10:07:25] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1332468635798226953 Time: 0.035252
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1508480131241957639 Time: 0.033396
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1947019689364377201 Time: 0.025312
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:25] [V] [TRT] Tactic: 3239257003214966313 Time: 0.035004
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5592640619112287921 Time: 0.022408
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:25] [V] [TRT] Tactic: 7621465827583909090 Time: 0.023524
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:25] [V] [TRT] Tactic: -5576936487443445631 Time: 0.027168
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:25] [V] [TRT] Tactic: -2297737319934264721 Time: 0.030888
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:25] [V] [TRT] Tactic: -1425085658556684465 Time: 0.029028
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:25] [V] [TRT] Tactic: -108011214168778087 Time: 0.026032
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.022408
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudaDepthwiseConvolution)
[12/28/2021-10:07:25] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (FusedConvActConvolution)
[12/28/2021-10:07:25] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:25] [V] [TRT] Tactic: 175853789719975416 Time: 0.028776
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:25] [V] [TRT] Tactic: 2171150287007712632 Time: 0.027104
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:25] [V] [TRT] Tactic: 2234457234705232274 Time: 0.02116
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:25] [V] [TRT] Tactic: 5834048089706882838 Time: 0.024828
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:25] [V] [TRT] Tactic: -8626990807754934295 Time: 0.024032
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:25] [V] [TRT] Tactic: -7303593854972602201 Time: 0.020472
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:25] [V] [TRT] Tactic: -6585664687867083638 Time: 0.032364
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:25] [V] [TRT] Tactic: -3730012925709297561 Time: 0.023136
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:25] [V] [TRT] Tactic: -2277259417488004546 Time: 0.032584
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:25] [V] [TRT] Tactic: -683636008127039856 Time: 0.030812
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.020472
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:25] [V] [TRT] Tactic: 984309058095623735 Time: 0.022904
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1100922622480907544 Time: 0.02384
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:25] [V] [TRT] Tactic: 3238312825609165543 Time: 0.032456
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:25] [V] [TRT] Tactic: 3606311198834416176 Time: 0.024684
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:25] [V] [TRT] Tactic: 4325765560739862899 Time: 0.030584
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:25] [V] [TRT] Tactic: -4255737803793506479 Time: 0.032052
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:25] [V] [TRT] Tactic: -3958182351168863467 Time: 0.02022
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:25] [V] [TRT] Tactic: -3111968753064955248 Time: 0.026712
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:25] [V] [TRT] Tactic: -1492575840277333548 Time: 0.028636
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:25] [V] [TRT] Tactic: -868495160148524802 Time: 0.020944
[12/28/2021-10:07:25] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.02022
[12/28/2021-10:07:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:25] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:25] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:25] [V] [TRT] Tactic: 1011019097971850911 Time: 0.0153
[12/28/2021-10:07:25] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1071114551801767124 Time: 0.01008
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:26] [V] [TRT] Tactic: 2623576043214044314 Time: 0.009236
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:26] [V] [TRT] Tactic: 3281631721811475881 Time: 0.0091
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4551754795416974366 Time: 0.008788
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4925112190271421402 Time: 0.009416
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5012796702462679112 Time: 0.020724
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5041593333398049019 Time: 0.008648
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5166018662410176512 Time: 0.021144
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6191867932654611882 Time: 0.01312
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6556170942941957134 Time: 0.01542
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6852868042694587230 Time: 0.009252
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:26] [V] [TRT] Tactic: 8399092794516815300 Time: 0.019516
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:26] [V] [TRT] Tactic: -9132922677633967263 Time: 0.010396
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7988637803896331454 Time: 0.014536
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7865001268126363229 Time: 0.017308
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7606074703023778034 Time: 0.014852
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7413564913826321357 Time: 0.01544
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7282232519526877434 Time: 0.02026
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5942379529065248478 Time: 0.01056
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5603587790314027122 Time: 0.01658
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5334776871777565833 Time: 0.021648
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5157868397078537095 Time: 0.013544
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5100834417027499764 Time: 0.008652
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:26] [V] [TRT] Tactic: -3365360067423513506 Time: 0.008932
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:26] [V] [TRT] Tactic: -2194148180068068313 Time: 0.020712
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1782593837177056527 Time: 0.011152
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1610768292520086910 Time: 0.015568
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1573035963956198975 Time: 0.019036
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1558762241666006941 Time: 0.015804
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1365353082499976145 Time: 0.016896
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:26] [V] [TRT] Tactic: -621838502160440068 Time: 0.016824
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 5041593333398049019 Time: 0.008648
[12/28/2021-10:07:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5041593333398049019
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudaGroupConvolution)
[12/28/2021-10:07:26] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CudaDepthwiseConvolution)
[12/28/2021-10:07:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (FusedConvActConvolution)
[12/28/2021-10:07:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_14 (CaskConvolution)
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:26] [V] [TRT] Tactic: 177040020707947851 Time: 0.008496
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1550399266192842845 Time: 0.008928
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1572887561103143487 Time: 0.009972
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:26] [V] [TRT] Tactic: 2325023763229477890 Time: 0.01322
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:26] [V] [TRT] Tactic: 2985940154541537814 Time: 0.015328
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:26] [V] [TRT] Tactic: 3284282970967328046 Time: 0.008388
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:26] [V] [TRT] Tactic: 3401614690060226673 Time: 0.008248
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:26] [V] [TRT] Tactic: 3512426920013359699 Time: 0.008352
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:26] [V] [TRT] Tactic: 3899284354987683408 Time: 0.021212
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4042202769383439184 Time: 0.009908
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4182625619810185112 Time: 0.017076
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4259547356717612415 Time: 0.010372
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4717285412741024953 Time: 0.015436
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4734519122557206480 Time: 0.019264
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5121596860264626879 Time: 0.019364
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5136656982162849059 Time: 0.008344
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5158259316594207439 Time: 0.009996
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5966973378912044513 Time: 0.012952
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6004789655466615912 Time: 0.00996
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6146901278630392829 Time: 0.018812
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6434020722187266170 Time: 0.019604
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6781129591847482048 Time: 0.010136
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:26] [V] [TRT] Tactic: 7191893591576074000 Time: 0.008508
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:26] [V] [TRT] Tactic: 7438984192263206338 Time: 0.009608
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:26] [V] [TRT] Tactic: 7504901284678552178 Time: 0.012956
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:26] [V] [TRT] Tactic: 8096257414008860171 Time: 0.009736
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:26] [V] [TRT] Tactic: 9143438935315839085 Time: 0.008388
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:26] [V] [TRT] Tactic: -9165697322068360861 Time: 0.0195
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:26] [V] [TRT] Tactic: -9118785798277698619 Time: 0.014748
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:26] [V] [TRT] Tactic: -8263994888336646547 Time: 0.01298
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:26] [V] [TRT] Tactic: -8205948405243401049 Time: 0.008772
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7992068592656168418 Time: 0.009708
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7842775553137511386 Time: 0.013164
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:26] [V] [TRT] Tactic: -7683887278997527517 Time: 0.008712
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:26] [V] [TRT] Tactic: -6400348606759295499 Time: 0.014648
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5980889159865208399 Time: 0.020892
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5766140806760372989 Time: 0.01636
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5709079507616090666 Time: 0.012568
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:26] [V] [TRT] Tactic: -5698636014239116282 Time: 0.018804
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:26] [V] [TRT] Tactic: -4933563390723451692 Time: 0.008336
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:26] [V] [TRT] Tactic: -4516822589357530549 Time: 0.016512
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:26] [V] [TRT] Tactic: -3413217501222406256 Time: 0.019152
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:26] [V] [TRT] Tactic: -3238475748440751107 Time: 0.009456
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:26] [V] [TRT] Tactic: -3182884991006484042 Time: 0.012836
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:26] [V] [TRT] Tactic: -3173468756112541306 Time: 0.0086
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:26] [V] [TRT] Tactic: -2917455979290586480 Time: 0.021292
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:26] [V] [TRT] Tactic: -2571022005763160364 Time: 0.015688
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:26] [V] [TRT] Tactic: -2083778562631872334 Time: 0.010024
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1546787387293556842 Time: 0.01256
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1498626619443284096 Time: 0.010516
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1283580231568512025 Time: 0.008688
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1173968681844185579 Time: 0.008796
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:26] [V] [TRT] Tactic: -762222380308749469 Time: 0.008568
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:26] [V] [TRT] Tactic: -556794153877490941 Time: 0.008436
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:26] [V] [TRT] Tactic: -516725800067794372 Time: 0.018888
[12/28/2021-10:07:26] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:26] [V] [TRT] Tactic: -428104331444385564 Time: 0.016908
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 3401614690060226673 Time: 0.008248
[12/28/2021-10:07:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3401614690060226673
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1002 Time: 0.008636
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.009536
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 1002 Time: 0.008636
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1002 Time: 0.008456
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.01004
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 1002 Time: 0.008456
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1002 Time: 0.007524
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.010068
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 1002 Time: 0.007524
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.009912
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1002 Time: 0.008252
[12/28/2021-10:07:26] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.009956
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 1002 Time: 0.008252
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1), Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudaDepthwiseConvolution)
[12/28/2021-10:07:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (FusedConvActConvolution)
[12/28/2021-10:07:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudnnConvolution)
[12/28/2021-10:07:26] [V] [TRT] Tactic: 0 Time: 0.03916
[12/28/2021-10:07:26] [V] [TRT] Tactic: 1 Time: 0.077532
[12/28/2021-10:07:26] [V] [TRT] Tactic: 2 Time: 0.078452
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4 Time: 0.051232
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6 Time: 0.030352
[12/28/2021-10:07:26] [V] [TRT] Tactic: 56 Time: 0.039024
[12/28/2021-10:07:26] [V] [TRT] Tactic: 57 Time: 0.07442
[12/28/2021-10:07:26] [V] [TRT] Tactic: 58 Time: 0.078356
[12/28/2021-10:07:26] [V] [TRT] Tactic: 60 Time: 0.0509
[12/28/2021-10:07:26] [V] [TRT] Tactic: 61 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:26] [V] [TRT] Tactic: 62 Time: 0.030364
[12/28/2021-10:07:26] [V] [TRT] Tactic: 112 Time: 0.03904
[12/28/2021-10:07:26] [V] [TRT] Tactic: 113 Time: 0.279684
[12/28/2021-10:07:26] [V] [TRT] Tactic: 114 Time: 0.078244
[12/28/2021-10:07:26] [V] [TRT] Tactic: 116 Time: 0.050912
[12/28/2021-10:07:26] [V] [TRT] Tactic: 117 skipped. Scratch requested: 35651584, available: 16777216
[12/28/2021-10:07:26] [V] [TRT] Tactic: 118 Time: 0.030312
[12/28/2021-10:07:26] [V] [TRT] Fastest Tactic: 118 Time: 0.030312
[12/28/2021-10:07:26] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:26] [V] [TRT] Tactic: 4549827808004681195 Time: 0.064208
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:26] [V] [TRT] Tactic: 5779835512569528575 Time: 0.081252
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6053873026024413720 Time: 0.090804
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:26] [V] [TRT] Tactic: 6767548733843469815 Time: 0.069488
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:26] [V] [TRT] Tactic: -6313876406580483184 Time: 0.076708
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:26] [V] [TRT] Tactic: -1123676555321336786 Time: 0.089148
[12/28/2021-10:07:26] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:27] [V] [TRT] Tactic: -701551393537224327 Time: 0.078092
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.064208
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 118
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64), Float(4096,1,512,64) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudnnConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2860655430572478466 Time: 0.0502
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4474630279712975759 Time: 0.03152
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4479823862704990365 Time: 0.030884
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050956
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5778138195697110003 Time: 0.07976
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:27] [V] [TRT] Tactic: 7155825427510256858 Time: 0.080236
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:27] [V] [TRT] Tactic: 8918020581761223752 Time: 0.078448
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:27] [V] [TRT] Tactic: -4756382386362004279 Time: 0.050028
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3855385237722507464 Time: 0.081176
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:27] [V] [TRT] Tactic: -2809379259463049391 Time: 0.079896
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:27] [V] [TRT] Tactic: -504296718212024303 Time: 0.077508
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.030884
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16), Float(1024,1:4,128,16) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudnnConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2860655430572478466 Time: 0.05024
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4474630279712975759 Time: 0.031592
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4479823862704990365 Time: 0.030832
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050948
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5778138195697110003 Time: 0.079944
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:27] [V] [TRT] Tactic: 7155825427510256858 Time: 0.080148
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:27] [V] [TRT] Tactic: 7342025736444949634 Time: 0.056112
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:27] [V] [TRT] Tactic: 8918020581761223752 Time: 0.078636
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7377458734869418330 Time: 0.055388
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5457304872213719461 Time: 0.055872
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:27] [V] [TRT] Tactic: -4756382386362004279 Time: 0.05012
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3855385237722507464 Time: 0.081188
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:27] [V] [TRT] Tactic: -2809379259463049391 Time: 0.079852
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:27] [V] [TRT] Tactic: -504296718212024303 Time: 0.077596
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.030832
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudaDepthwiseConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1332468635798226953 Time: 0.036312
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1508480131241957639 Time: 0.03446
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1947019689364377201 Time: 0.02654
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3239257003214966313 Time: 0.036028
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5592640619112287921 Time: 0.023656
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:27] [V] [TRT] Tactic: 7621465827583909090 Time: 0.024716
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5576936487443445631 Time: 0.028252
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:27] [V] [TRT] Tactic: -2297737319934264721 Time: 0.031956
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1425085658556684465 Time: 0.030152
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:27] [V] [TRT] Tactic: -108011214168778087 Time: 0.027244
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.023656
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Int8(1024,64:4,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudaDepthwiseConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (FusedConvActConvolution)
[12/28/2021-10:07:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:27] [V] [TRT] Tactic: 175853789719975416 Time: 0.030316
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2171150287007712632 Time: 0.028664
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2234457234705232274 Time: 0.022724
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5834048089706882838 Time: 0.026444
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:27] [V] [TRT] Tactic: -8626990807754934295 Time: 0.025592
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7303593854972602201 Time: 0.0221
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:27] [V] [TRT] Tactic: -6585664687867083638 Time: 0.0339
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3730012925709297561 Time: 0.02446
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:27] [V] [TRT] Tactic: -2277259417488004546 Time: 0.034192
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:27] [V] [TRT] Tactic: -683636008127039856 Time: 0.032344
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.0221
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:27] [V] [TRT] Tactic: 984309058095623735 Time: 0.024592
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1100922622480907544 Time: 0.025548
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3238312825609165543 Time: 0.034172
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3606311198834416176 Time: 0.026464
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4325765560739862899 Time: 0.03222
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:27] [V] [TRT] Tactic: -4255737803793506479 Time: 0.033844
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3958182351168863467 Time: 0.021996
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3111968753064955248 Time: 0.028528
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1492575840277333548 Time: 0.030192
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:27] [V] [TRT] Tactic: -868495160148524802 Time: 0.022824
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.021996
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1), Float(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1011019097971850911 Time: 0.016052
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:27] [V] [TRT] Tactic: 1071114551801767124 Time: 0.010656
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2623576043214044314 Time: 0.009416
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3281631721811475881 Time: 0.009316
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4551754795416974366 Time: 0.009416
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4925112190271421402 Time: 0.00912
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5012796702462679112 Time: 0.022184
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5041593333398049019 Time: 0.009124
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5166018662410176512 Time: 0.02196
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6191867932654611882 Time: 0.014496
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6556170942941957134 Time: 0.016376
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6852868042694587230 Time: 0.009624
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:27] [V] [TRT] Tactic: 8399092794516815300 Time: 0.021144
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:27] [V] [TRT] Tactic: -9132922677633967263 Time: 0.011156
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7988637803896331454 Time: 0.015464
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7865001268126363229 Time: 0.018828
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7606074703023778034 Time: 0.01636
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7413564913826321357 Time: 0.0164
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7282232519526877434 Time: 0.021736
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5942379529065248478 Time: 0.011396
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5603587790314027122 Time: 0.01812
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5334776871777565833 Time: 0.022216
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5157868397078537095 Time: 0.014856
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5100834417027499764 Time: 0.008932
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3365360067423513506 Time: 0.008844
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:27] [V] [TRT] Tactic: -2194148180068068313 Time: 0.022012
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1782593837177056527 Time: 0.011716
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1610768292520086910 Time: 0.01706
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1573035963956198975 Time: 0.020784
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1558762241666006941 Time: 0.016868
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:27] [V] [TRT] Tactic: -1365353082499976145 Time: 0.01834
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:27] [V] [TRT] Tactic: -621838502160440068 Time: 0.018268
[12/28/2021-10:07:27] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.008844
[12/28/2021-10:07:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:27] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1), Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudaGroupConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CudaDepthwiseConvolution)
[12/28/2021-10:07:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (FusedConvActConvolution)
[12/28/2021-10:07:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:27] [V] [TRT] --------------- Timing Runner: Conv_17 + Add_18 + Relu_21 (CaskConvolution)
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2325023763229477890 Time: 0.014304
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:27] [V] [TRT] Tactic: 2985940154541537814 Time: 0.016368
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3401614690060226673 Time: 0.008648
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:27] [V] [TRT] Tactic: 3899284354987683408 Time: 0.023052
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4042202769383439184 Time: 0.010396
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4182625619810185112 Time: 0.01828
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4717285412741024953 Time: 0.016564
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:27] [V] [TRT] Tactic: 4734519122557206480 Time: 0.02002
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:27] [V] [TRT] Tactic: 5136656982162849059 Time: 0.008792
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6004789655466615912 Time: 0.010764
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6146901278630392829 Time: 0.01954
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:27] [V] [TRT] Tactic: 6781129591847482048 Time: 0.010816
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:27] [V] [TRT] Tactic: 8096257414008860171 Time: 0.01052
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:27] [V] [TRT] Tactic: -9165697322068360861 Time: 0.020424
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:27] [V] [TRT] Tactic: -9118785798277698619 Time: 0.015744
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:27] [V] [TRT] Tactic: -8263994888336646547 Time: 0.013864
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:27] [V] [TRT] Tactic: -8205948405243401049 Time: 0.009132
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:27] [V] [TRT] Tactic: -7683887278997527517 Time: 0.00908
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:27] [V] [TRT] Tactic: -6400348606759295499 Time: 0.015644
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5980889159865208399 Time: 0.022712
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:27] [V] [TRT] Tactic: -5766140806760372989 Time: 0.017436
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:27] [V] [TRT] Tactic: -4933563390723451692 Time: 0.009088
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:27] [V] [TRT] Tactic: -4516822589357530549 Time: 0.017588
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:27] [V] [TRT] Tactic: -3238475748440751107 Time: 0.010244
[12/28/2021-10:07:27] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:28] [V] [TRT] Tactic: -3182884991006484042 Time: 0.013912
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:28] [V] [TRT] Tactic: -3173468756112541306 Time: 0.008632
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:28] [V] [TRT] Tactic: -2917455979290586480 Time: 0.023056
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:28] [V] [TRT] Tactic: -2571022005763160364 Time: 0.016764
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:28] [V] [TRT] Tactic: -1546787387293556842 Time: 0.013152
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:28] [V] [TRT] Tactic: -1498626619443284096 Time: 0.011072
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:28] [V] [TRT] Tactic: -1283580231568512025 Time: 0.008768
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:28] [V] [TRT] Tactic: -762222380308749469 Time: 0.008932
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:28] [V] [TRT] Tactic: -516725800067794372 Time: 0.01966
[12/28/2021-10:07:28] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:28] [V] [TRT] Tactic: -428104331444385564 Time: 0.017652
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: -3173468756112541306 Time: 0.008632
[12/28/2021-10:07:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3173468756112541306
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008464
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006172
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006172
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008496
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006392
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006392
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.009448
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007532
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.004992
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.004992
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.00758
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.007672
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.00758
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008764
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.00626
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.00626
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007552
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006048
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006048
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008384
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.009872
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.008384
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.0076
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.00662
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.00662
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007632
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.008328
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.007632
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008784
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006368
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006368
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007528
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006096
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006096
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007508
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.010008
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.007508
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008684
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006548
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006548
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.0088
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.008252
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.008252
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008796
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006396
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006396
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008568
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006124
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006124
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.00762
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006364
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006364
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.007948
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006664
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006664
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.00806
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.008432
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.00806
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.005316
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.005316
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008204
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006408
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006408
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008972
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.006704
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.006704
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.009932
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1002 Time: 0.008592
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1002 Time: 0.0075
[12/28/2021-10:07:28] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.005584
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 0 Time: 0.005584
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1), Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64), Float(4096,1,512,64) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16), Float(1024,1:4,128,16) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Float(4096,64,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Int8(1024,64:4,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1), Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1), Float(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1), Int8(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudaDepthwiseConvolution)
[12/28/2021-10:07:28] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (FusedConvActConvolution)
[12/28/2021-10:07:28] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudnnConvolution)
[12/28/2021-10:07:28] [V] [TRT] Tactic: 0 Time: 0.033668
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1 Time: 0.032436
[12/28/2021-10:07:28] [V] [TRT] Tactic: 2 Time: 0.066708
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5 skipped. Scratch requested: 62390272, available: 16777216
[12/28/2021-10:07:28] [V] [TRT] Tactic: 56 Time: 0.03374
[12/28/2021-10:07:28] [V] [TRT] Tactic: 57 Time: 0.042864
[12/28/2021-10:07:28] [V] [TRT] Tactic: 58 Time: 0.067068
[12/28/2021-10:07:28] [V] [TRT] Tactic: 61 skipped. Scratch requested: 62390272, available: 16777216
[12/28/2021-10:07:28] [V] [TRT] Tactic: 112 Time: 0.033728
[12/28/2021-10:07:28] [V] [TRT] Tactic: 113 Time: 0.138604
[12/28/2021-10:07:28] [V] [TRT] Tactic: 114 Time: 0.066948
[12/28/2021-10:07:28] [V] [TRT] Tactic: 117 skipped. Scratch requested: 62390272, available: 16777216
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 1 Time: 0.032436
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4549827808004681195 Time: 0.063412
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5779835512569528575 Time: 0.080524
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:28] [V] [TRT] Tactic: 6053873026024413720 Time: 0.084604
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:28] [V] [TRT] Tactic: 6767548733843469815 Time: 0.065992
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:28] [V] [TRT] Tactic: -6313876406580483184 Time: 0.078328
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:28] [V] [TRT] Tactic: -1123676555321336786 Time: 0.08378
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:28] [V] [TRT] Tactic: -701551393537224327 Time: 0.072104
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.063412
[12/28/2021-10:07:28] [V] [TRT] Setting workspace to 62390272enables more tactics for profiling
[12/28/2021-10:07:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 1
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudnnConvolution)
[12/28/2021-10:07:28] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:28] [V] [TRT] Tactic: 2860655430572478466 Time: 0.049248
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4474630279712975759 Time: 0.030392
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4479823862704990365 Time: 0.02976
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050296
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5778138195697110003 Time: 0.082588
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:28] [V] [TRT] Tactic: 7155825427510256858 Time: 0.078736
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:28] [V] [TRT] Tactic: 8918020581761223752 Time: 0.07718
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:28] [V] [TRT] Tactic: -4756382386362004279 Time: 0.049312
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:28] [V] [TRT] Tactic: -3855385237722507464 Time: 0.083184
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:28] [V] [TRT] Tactic: -2809379259463049391 Time: 0.081636
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:28] [V] [TRT] Tactic: -504296718212024303 Time: 0.076768
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.02976
[12/28/2021-10:07:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudnnConvolution)
[12/28/2021-10:07:28] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:28] [V] [TRT] Tactic: 2860655430572478466 Time: 0.049248
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4474630279712975759 Time: 0.030312
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4479823862704990365 Time: 0.029732
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:28] [V] [TRT] Tactic: 4696204239951173149 Time: 0.050236
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5778138195697110003 Time: 0.082572
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:28] [V] [TRT] Tactic: 7155825427510256858 Time: 0.078776
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:28] [V] [TRT] Tactic: 7342025736444949634 Time: 0.054788
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:28] [V] [TRT] Tactic: 8918020581761223752 Time: 0.077016
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:28] [V] [TRT] Tactic: -7377458734869418330 Time: 0.054168
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:28] [V] [TRT] Tactic: -5457304872213719461 Time: 0.05474
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:28] [V] [TRT] Tactic: -4756382386362004279 Time: 0.049288
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:28] [V] [TRT] Tactic: -3855385237722507464 Time: 0.08328
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:28] [V] [TRT] Tactic: -2809379259463049391 Time: 0.08164
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:28] [V] [TRT] Tactic: -504296718212024303 Time: 0.076712
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.029732
[12/28/2021-10:07:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudaDepthwiseConvolution)
[12/28/2021-10:07:28] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1332468635798226953 Time: 0.034636
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1508480131241957639 Time: 0.033308
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:28] [V] [TRT] Tactic: 1947019689364377201 Time: 0.024384
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:28] [V] [TRT] Tactic: 3239257003214966313 Time: 0.034116
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5592640619112287921 Time: 0.02242
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:28] [V] [TRT] Tactic: 7621465827583909090 Time: 0.023488
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:28] [V] [TRT] Tactic: -5576936487443445631 Time: 0.025844
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:28] [V] [TRT] Tactic: -2297737319934264721 Time: 0.029724
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:28] [V] [TRT] Tactic: -1425085658556684465 Time: 0.026616
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:28] [V] [TRT] Tactic: -108011214168778087 Time: 0.026604
[12/28/2021-10:07:28] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.02242
[12/28/2021-10:07:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:28] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudaDepthwiseConvolution)
[12/28/2021-10:07:28] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (FusedConvActConvolution)
[12/28/2021-10:07:28] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:28] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:28] [V] [TRT] Tactic: 175853789719975416 Time: 0.027652
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:28] [V] [TRT] Tactic: 2171150287007712632 Time: 0.024608
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:28] [V] [TRT] Tactic: 2234457234705232274 Time: 0.021164
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:28] [V] [TRT] Tactic: 5834048089706882838 Time: 0.0235
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:28] [V] [TRT] Tactic: -8626990807754934295 Time: 0.024592
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:28] [V] [TRT] Tactic: -7303593854972602201 Time: 0.020244
[12/28/2021-10:07:28] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:29] [V] [TRT] Tactic: -6585664687867083638 Time: 0.03136
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3730012925709297561 Time: 0.022092
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:29] [V] [TRT] Tactic: -2277259417488004546 Time: 0.031912
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:29] [V] [TRT] Tactic: -683636008127039856 Time: 0.030596
[12/28/2021-10:07:29] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.020244
[12/28/2021-10:07:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:29] [V] [TRT] Tactic: 984309058095623735 Time: 0.022084
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1100922622480907544 Time: 0.024436
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3238312825609165543 Time: 0.031828
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3606311198834416176 Time: 0.023384
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4325765560739862899 Time: 0.03064
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:29] [V] [TRT] Tactic: -4255737803793506479 Time: 0.031336
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3958182351168863467 Time: 0.020172
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3111968753064955248 Time: 0.024424
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1492575840277333548 Time: 0.027484
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:29] [V] [TRT] Tactic: -868495160148524802 Time: 0.021112
[12/28/2021-10:07:29] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.020172
[12/28/2021-10:07:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1011019097971850911 Time: 0.015456
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1071114551801767124 Time: 0.01064
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:29] [V] [TRT] Tactic: 2623576043214044314 Time: 0.008296
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3281631721811475881 Time: 0.008808
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4551754795416974366 Time: 0.008672
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4925112190271421402 Time: 0.008064
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5012796702462679112 Time: 0.022332
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5041593333398049019 Time: 0.007664
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5166018662410176512 Time: 0.024416
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6191867932654611882 Time: 0.014872
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6556170942941957134 Time: 0.015648
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6852868042694587230 Time: 0.009164
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:29] [V] [TRT] Tactic: 8399092794516815300 Time: 0.021368
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:29] [V] [TRT] Tactic: -9132922677633967263 Time: 0.011116
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7988637803896331454 Time: 0.014868
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7865001268126363229 Time: 0.017444
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7606074703023778034 Time: 0.015028
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7413564913826321357 Time: 0.015772
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7282232519526877434 Time: 0.0223
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5942379529065248478 Time: 0.010812
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5603587790314027122 Time: 0.016452
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5334776871777565833 Time: 0.024852
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5157868397078537095 Time: 0.015176
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5100834417027499764 Time: 0.008248
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3365360067423513506 Time: 0.007412
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:29] [V] [TRT] Tactic: -2194148180068068313 Time: 0.022604
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1782593837177056527 Time: 0.011128
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1610768292520086910 Time: 0.015856
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1573035963956198975 Time: 0.02104
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1558762241666006941 Time: 0.016472
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1365353082499976145 Time: 0.016848
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:29] [V] [TRT] Tactic: -621838502160440068 Time: 0.017104
[12/28/2021-10:07:29] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.007412
[12/28/2021-10:07:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudaGroupConvolution)
[12/28/2021-10:07:29] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CudaDepthwiseConvolution)
[12/28/2021-10:07:29] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (FusedConvActConvolution)
[12/28/2021-10:07:29] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_37 + Relu_40 (CaskConvolution)
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:29] [V] [TRT] Tactic: 177040020707947851 Time: 0.00844
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1550399266192842845 Time: 0.00796
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1572887561103143487 Time: 0.010036
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:29] [V] [TRT] Tactic: 2325023763229477890 Time: 0.013296
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:29] [V] [TRT] Tactic: 2985940154541537814 Time: 0.0153
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3284282970967328046 Time: 0.007544
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3401614690060226673 Time: 0.007992
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3512426920013359699 Time: 0.008452
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:29] [V] [TRT] Tactic: 3899284354987683408 Time: 0.021292
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4042202769383439184 Time: 0.010028
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4182625619810185112 Time: 0.017168
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4259547356717612415 Time: 0.010548
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4717285412741024953 Time: 0.015632
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:29] [V] [TRT] Tactic: 4734519122557206480 Time: 0.019324
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5121596860264626879 Time: 0.019424
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5136656982162849059 Time: 0.007548
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5158259316594207439 Time: 0.00988
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:29] [V] [TRT] Tactic: 5966973378912044513 Time: 0.013088
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6004789655466615912 Time: 0.0102
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6146901278630392829 Time: 0.018736
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6434020722187266170 Time: 0.019692
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:29] [V] [TRT] Tactic: 6781129591847482048 Time: 0.010096
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:29] [V] [TRT] Tactic: 7191893591576074000 Time: 0.007628
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:29] [V] [TRT] Tactic: 7438984192263206338 Time: 0.009572
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:29] [V] [TRT] Tactic: 7504901284678552178 Time: 0.01308
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:29] [V] [TRT] Tactic: 8096257414008860171 Time: 0.009752
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:29] [V] [TRT] Tactic: 9143438935315839085 Time: 0.008272
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:29] [V] [TRT] Tactic: -9165697322068360861 Time: 0.01966
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:29] [V] [TRT] Tactic: -9118785798277698619 Time: 0.014956
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:29] [V] [TRT] Tactic: -8263994888336646547 Time: 0.013076
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:29] [V] [TRT] Tactic: -8205948405243401049 Time: 0.007912
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7992068592656168418 Time: 0.009964
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7842775553137511386 Time: 0.013328
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:29] [V] [TRT] Tactic: -7683887278997527517 Time: 0.008692
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:29] [V] [TRT] Tactic: -6400348606759295499 Time: 0.014536
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5980889159865208399 Time: 0.020888
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5766140806760372989 Time: 0.016064
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5709079507616090666 Time: 0.0126
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:29] [V] [TRT] Tactic: -5698636014239116282 Time: 0.01878
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:29] [V] [TRT] Tactic: -4933563390723451692 Time: 0.008404
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:29] [V] [TRT] Tactic: -4516822589357530549 Time: 0.01676
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3413217501222406256 Time: 0.019324
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3238475748440751107 Time: 0.00958
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3182884991006484042 Time: 0.013056
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:29] [V] [TRT] Tactic: -3173468756112541306 Time: 0.007624
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:29] [V] [TRT] Tactic: -2917455979290586480 Time: 0.021156
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:29] [V] [TRT] Tactic: -2571022005763160364 Time: 0.016208
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:29] [V] [TRT] Tactic: -2083778562631872334 Time: 0.010076
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1546787387293556842 Time: 0.012548
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1498626619443284096 Time: 0.010648
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1283580231568512025 Time: 0.00794
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:29] [V] [TRT] Tactic: -1173968681844185579 Time: 0.007804
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:29] [V] [TRT] Tactic: -762222380308749469 Time: 0.008636
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:29] [V] [TRT] Tactic: -556794153877490941 Time: 0.00842
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:29] [V] [TRT] Tactic: -516725800067794372 Time: 0.019328
[12/28/2021-10:07:29] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:29] [V] [TRT] Tactic: -428104331444385564 Time: 0.016664
[12/28/2021-10:07:29] [V] [TRT] Fastest Tactic: 3284282970967328046 Time: 0.007544
[12/28/2021-10:07:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3284282970967328046
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,64,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(4096,1,512,64) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(1024,1:4,128,16) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Float(128,64:32,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(1024,64:4,8,1) -> Int8(128,64:32,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,64,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(4096,1,512,64) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Float(1024,1:4,128,16) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning Reformat:Int8(128,64:32,8,1) -> Int8(1024,64:4,8,1) ***************
[12/28/2021-10:07:29] [V] [TRT] *************** Autotuning format combination: Float(4096,64,8,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_46 (CudaDepthwiseConvolution)
[12/28/2021-10:07:29] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_46 (FusedConvActConvolution)
[12/28/2021-10:07:29] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_46 (CudnnConvolution)
[12/28/2021-10:07:29] [V] [TRT] Tactic: 0 Time: 0.0115
[12/28/2021-10:07:29] [V] [TRT] Tactic: 1 Time: 0.011512
[12/28/2021-10:07:29] [V] [TRT] Tactic: 2 Time: 0.032428
[12/28/2021-10:07:29] [V] [TRT] Tactic: 56 Time: 0.011544
[12/28/2021-10:07:29] [V] [TRT] Tactic: 57 Time: 0.011448
[12/28/2021-10:07:29] [V] [TRT] Tactic: 58 Time: 0.032456
[12/28/2021-10:07:29] [V] [TRT] Tactic: 112 Time: 0.011476
[12/28/2021-10:07:29] [V] [TRT] Tactic: 113 Time: 0.011452
[12/28/2021-10:07:29] [V] [TRT] Tactic: 114 Time: 0.03238
[12/28/2021-10:07:29] [V] [TRT] Fastest Tactic: 57 Time: 0.011448
[12/28/2021-10:07:29] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:29] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4549827808004681195 Time: 0.01544
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5779835512569528575 Time: 0.018456
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 9151672657204310840
[12/28/2021-10:07:30] [V] [TRT] Tactic: 9151672657204310840 Time: 0.028748
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7491730084094677098 Time: 0.01684
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6313876406580483184 Time: 0.017204
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6273689210331812572 Time: 0.018248
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4337126844824617177 Time: 0.014984
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1123676555321336786 Time: 0.018428
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:30] [V] [TRT] Tactic: -701551393537224327 Time: 0.015756
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -4337126844824617177 Time: 0.014984
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 57
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Float(4096,1,512,64) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudnnConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1663866669559596164 Time: 0.01678
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2860655430572478466 Time: 0.013104
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4474630279712975759 Time: 0.01094
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4479823862704990365 Time: 0.010972
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4696204239951173149 Time: 0.01308
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5778138195697110003 Time: 0.017168
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:30] [V] [TRT] Tactic: 8918020581761223752 Time: 0.016428
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5905193483742532701 Time: 0.012472
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4035591156787122265 Time: 0.010848
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:30] [V] [TRT] Tactic: -2809379259463049391 Time: 0.017224
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1985235291706575900 Time: 0.016404
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:30] [V] [TRT] Tactic: -504296718212024303 Time: 0.016548
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.010848
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Float(1024,1:4,128,16) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudnnConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 1373022415249282411
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1373022415249282411 Time: 0.013548
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1663866669559596164 Time: 0.017248
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2860655430572478466 Time: 0.013396
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4474630279712975759 Time: 0.01122
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4479823862704990365 Time: 0.01122
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4696204239951173149 Time: 0.013172
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5778138195697110003 Time: 0.017248
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:30] [V] [TRT] Tactic: 7342025736444949634 Time: 0.013708
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:30] [V] [TRT] Tactic: 8918020581761223752 Time: 0.016528
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5905193483742532701 Time: 0.012668
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5457304872213719461 Time: 0.014032
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4035591156787122265 Time: 0.01096
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:30] [V] [TRT] Tactic: -2809379259463049391 Time: 0.01736
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1985235291706575900 Time: 0.01646
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:30] [V] [TRT] Tactic: -504296718212024303 Time: 0.016552
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.01096
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudaDepthwiseConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1508480131241957639 Time: 0.013524
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 2141154648944475104
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2141154648944475104 Time: 0.013444
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:30] [V] [TRT] Tactic: 3239257003214966313 Time: 0.01354
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5592640619112287921 Time: 0.0113
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:30] [V] [TRT] Tactic: 7621465827583909090 Time: 0.011572
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -6580271968881459581
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6580271968881459581 Time: 0.011388
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5576936487443445631 Time: 0.011756
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -4443833619060044580
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4443833619060044580 Time: 0.011156
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:30] [V] [TRT] Tactic: -2297737319934264721 Time: 0.01166
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1425085658556684465 Time: 0.011512
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:30] [V] [TRT] Tactic: -108011214168778087 Time: 0.01154
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -42427192380281294
[12/28/2021-10:07:30] [V] [TRT] Tactic: -42427192380281294 Time: 0.01146
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -4443833619060044580 Time: 0.011156
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4443833619060044580
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudaDepthwiseConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (FusedConvActConvolution)
[12/28/2021-10:07:30] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:30] [V] [TRT] Tactic: 175853789719975416 Time: 0.00954
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2171150287007712632 Time: 0.009424
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2234457234705232274 Time: 0.0094
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5834048089706882838 Time: 0.009212
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 6299962968199310600
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6299962968199310600 Time: 0.010784
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 6341572697076960911
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6341572697076960911 Time: 0.00922
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:30] [V] [TRT] Tactic: -8626990807754934295 Time: 0.009444
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -8498217049614706532
[12/28/2021-10:07:30] [V] [TRT] Tactic: -8498217049614706532 Time: 0.00908
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7303593854972602201 Time: 0.009136
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6585664687867083638 Time: 0.010928
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3326139578711341011
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3326139578711341011 Time: 0.009364
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:30] [V] [TRT] Tactic: -683636008127039856 Time: 0.01082
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -8498217049614706532 Time: 0.00908
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8498217049614706532
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Int8(1024,64:4,8,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1100922622480907544 Time: 0.009272
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 2855900226702061782
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2855900226702061782 Time: 0.010692
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:30] [V] [TRT] Tactic: 3606311198834416176 Time: 0.009384
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4325765560739862899 Time: 0.010732
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 8803458114157674373
[12/28/2021-10:07:30] [V] [TRT] Tactic: 8803458114157674373 Time: 0.009096
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -6934773036503365000
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6934773036503365000 Time: 0.009128
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -4431642509665791294
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4431642509665791294 Time: 0.008944
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:30] [V] [TRT] Tactic: -4255737803793506479 Time: 0.010916
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3958182351168863467 Time: 0.008936
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3111968753064955248 Time: 0.0094
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1492575840277333548 Time: 0.009464
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:30] [V] [TRT] Tactic: -868495160148524802 Time: 0.0091
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.008936
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2623576043214044314 Time: 0.006452
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 2818014835119698671
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2818014835119698671 Time: 0.007972
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 3721599319722771137
[12/28/2021-10:07:30] [V] [TRT] Tactic: 3721599319722771137 Time: 0.006204
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 4178917718361232468
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4178917718361232468 Time: 0.009292
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4551754795416974366 Time: 0.00664
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:30] [V] [TRT] Tactic: 4925112190271421402 Time: 0.006392
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:30] [V] [TRT] Tactic: 5012796702462679112 Time: 0.012036
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6556170942941957134 Time: 0.009316
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6618077155362058131
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6618077155362058131 Time: 0.0061
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6852868042694587230 Time: 0.006956
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 6969462133921577484
[12/28/2021-10:07:30] [V] [TRT] Tactic: 6969462133921577484 Time: 0.010952
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:30] [V] [TRT] Tactic: 8399092794516815300 Time: 0.011232
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:30] [V] [TRT] Tactic: -9132922677633967263 Time: 0.008024
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -8912999970161746151
[12/28/2021-10:07:30] [V] [TRT] Tactic: -8912999970161746151 Time: 0.007852
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -8893439100868426414
[12/28/2021-10:07:30] [V] [TRT] Tactic: -8893439100868426414 Time: 0.010372
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7988637803896331454 Time: 0.009128
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -7904635102498369361
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7904635102498369361 Time: 0.01168
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7606074703023778034 Time: 0.00922
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7413564913826321357 Time: 0.010364
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:30] [V] [TRT] Tactic: -7282232519526877434 Time: 0.01182
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -6406011580107094428
[12/28/2021-10:07:30] [V] [TRT] Tactic: -6406011580107094428 Time: 0.00676
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5603587790314027122 Time: 0.009356
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -5416590980288859834
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5416590980288859834 Time: 0.009272
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5334776871777565833 Time: 0.0145
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:30] [V] [TRT] Tactic: -5157868397078537095 Time: 0.010036
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3665201838779845683
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3665201838779845683 Time: 0.014284
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -3644377136375731441
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3644377136375731441 Time: 0.009232
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3502495740607894730
[12/28/2021-10:07:30] [V] [TRT] Tactic: -3502495740607894730 Time: 0.006292
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -2342404147487779225
[12/28/2021-10:07:30] [V] [TRT] Tactic: -2342404147487779225 Time: 0.009968
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1782593837177056527 Time: 0.0082
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:30] [V] [TRT] Tactic: -1610768292520086910 Time: 0.009292
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:30] [V] [TRT] Tactic: -621838502160440068 Time: 0.009324
[12/28/2021-10:07:30] [V] [TRT] Fastest Tactic: 6618077155362058131 Time: 0.0061
[12/28/2021-10:07:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6618077155362058131
[12/28/2021-10:07:30] [V] [TRT] *************** Autotuning format combination: Int8(128,64:32,8,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudaGroupConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CudaDepthwiseConvolution)
[12/28/2021-10:07:30] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (FusedConvActConvolution)
[12/28/2021-10:07:30] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:30] [V] [TRT] --------------- Timing Runner: Conv_46 (CaskConvolution)
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:30] [V] [TRT] Tactic: 177040020707947851 Time: 0.006648
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 434957160407688216
[12/28/2021-10:07:30] [V] [TRT] Tactic: 434957160407688216 Time: 0.010624
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 805889586762897346
[12/28/2021-10:07:30] [V] [TRT] Tactic: 805889586762897346 Time: 0.00942
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:30] [V] [TRT] Tactic: 1550399266192842845 Time: 0.006476
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2325023763229477890 Time: 0.008112
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 2346437292116182513
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2346437292116182513 Time: 0.008828
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 2522133112320625287
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2522133112320625287 Time: 0.009048
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:30] [V] [TRT] Tactic: 2985940154541537814 Time: 0.0088
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 3538565962642681625
[12/28/2021-10:07:30] [V] [TRT] Tactic: 3538565962642681625 Time: 0.00648
[12/28/2021-10:07:30] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:31] [V] [TRT] Tactic: 3899284354987683408 Time: 0.01094
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4042202769383439184 Time: 0.007028
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4259547356717612415 Time: 0.007484
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4717285412741024953 Time: 0.009272
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4734519122557206480 Time: 0.00942
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5121596860264626879 Time: 0.009376
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 5126565865931538390
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5126565865931538390 Time: 0.009196
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5158259316594207439 Time: 0.007112
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 5375256703210220108
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5375256703210220108 Time: 0.006952
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6433368103202497147
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6433368103202497147 Time: 0.007888
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6434020722187266170 Time: 0.009612
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6441948709525127755
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6441948709525127755 Time: 0.006136
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6457435868048963632
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6457435868048963632 Time: 0.006892
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6781129591847482048 Time: 0.007208
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6925201228918187099
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6925201228918187099 Time: 0.007736
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:31] [V] [TRT] Tactic: 7504901284678552178 Time: 0.007792
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7731430299029542276
[12/28/2021-10:07:31] [V] [TRT] Tactic: 7731430299029542276 Time: 0.007804
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7738495016763012180
[12/28/2021-10:07:31] [V] [TRT] Tactic: 7738495016763012180 Time: 0.009088
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 8234775147403903473
[12/28/2021-10:07:31] [V] [TRT] Tactic: 8234775147403903473 Time: 0.009596
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:31] [V] [TRT] Tactic: -9165697322068360861 Time: 0.009708
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:31] [V] [TRT] Tactic: -9118785798277698619 Time: 0.009184
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -8556775352640313933
[12/28/2021-10:07:31] [V] [TRT] Tactic: -8556775352640313933 Time: 0.00782
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:31] [V] [TRT] Tactic: -8263994888336646547 Time: 0.007868
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:31] [V] [TRT] Tactic: -8205948405243401049 Time: 0.006464
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:31] [V] [TRT] Tactic: -7842775553137511386 Time: 0.008324
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:31] [V] [TRT] Tactic: -7683887278997527517 Time: 0.00662
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -6527178416855951297
[12/28/2021-10:07:31] [V] [TRT] Tactic: -6527178416855951297 Time: 0.006388
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -6510232214299595844
[12/28/2021-10:07:31] [V] [TRT] Tactic: -6510232214299595844 Time: 0.006336
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:31] [V] [TRT] Tactic: -6400348606759295499 Time: 0.008772
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:31] [V] [TRT] Tactic: -5980889159865208399 Time: 0.010852
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:31] [V] [TRT] Tactic: -5766140806760372989 Time: 0.009124
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -5170003087447722174
[12/28/2021-10:07:31] [V] [TRT] Tactic: -5170003087447722174 Time: 0.006288
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -4849712423393454704
[12/28/2021-10:07:31] [V] [TRT] Tactic: -4849712423393454704 Time: 0.006932
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:31] [V] [TRT] Tactic: -4516822589357530549 Time: 0.00914
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -3613322253849278738
[12/28/2021-10:07:31] [V] [TRT] Tactic: -3613322253849278738 Time: 0.006164
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -3577322188448771475
[12/28/2021-10:07:31] [V] [TRT] Tactic: -3577322188448771475 Time: 0.007212
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -2754311112012636251
[12/28/2021-10:07:31] [V] [TRT] Tactic: -2754311112012636251 Time: 0.007368
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: -2315453944962430928
[12/28/2021-10:07:31] [V] [TRT] Tactic: -2315453944962430928 Time: 0.00916
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:31] [V] [TRT] Tactic: -2083778562631872334 Time: 0.007256
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -1499578657823798783
[12/28/2021-10:07:31] [V] [TRT] Tactic: -1499578657823798783 Time: 0.006376
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:31] [V] [TRT] Tactic: -1498626619443284096 Time: 0.00752
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:31] [V] [TRT] Tactic: -1283580231568512025 Time: 0.006388
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:31] [V] [TRT] Tactic: -1173968681844185579 Time: 0.006368
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:31] [V] [TRT] Tactic: -762222380308749469 Time: 0.006644
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -713022856474991236
[12/28/2021-10:07:31] [V] [TRT] Tactic: -713022856474991236 Time: 0.006224
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:31] [V] [TRT] Tactic: -556794153877490941 Time: 0.00674
[12/28/2021-10:07:31] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -375949437730908730
[12/28/2021-10:07:31] [V] [TRT] Tactic: -375949437730908730 Time: 0.006932
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 6441948709525127755 Time: 0.006136
[12/28/2021-10:07:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6441948709525127755
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008692
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00592
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00592
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008744
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005932
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005932
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007764
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00496
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00496
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007888
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00556
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00556
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009048
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00544
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00544
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007436
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005464
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005464
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008036
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.0057
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.0057
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007924
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005828
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005828
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009088
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005576
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005576
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007452
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00558
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00558
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.0089
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005672
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005672
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008908
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005752
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005752
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009208
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00548
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00548
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009236
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005524
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005524
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007344
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00552
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00552
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008472
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005768
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005768
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008344
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005832
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005832
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007672
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00498
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00498
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008948
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005948
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005948
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009064
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00596
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00596
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007624
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005032
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005032
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.00772
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005776
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005776
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007912
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.006012
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.006012
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.00902
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005948
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005948
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007576
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00472
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1 Time: 0.005124
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00472
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008696
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.00564
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.00564
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009252
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005704
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005704
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.007328
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005804
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005804
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.009204
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.006044
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.006044
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1002 Time: 0.008384
[12/28/2021-10:07:31] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.005872
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 0 Time: 0.005872
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] *************** Autotuning format combination: Float(2048,16,4,1), Float(2048,16,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudaDepthwiseConvolution)
[12/28/2021-10:07:31] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (FusedConvActConvolution)
[12/28/2021-10:07:31] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudnnConvolution)
[12/28/2021-10:07:31] [V] [TRT] Tactic: 0 Time: 0.05846
[12/28/2021-10:07:31] [V] [TRT] Tactic: 1 Time: 0.063208
[12/28/2021-10:07:31] [V] [TRT] Tactic: 2 Time: 0.08084
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6 Time: 0.040176
[12/28/2021-10:07:31] [V] [TRT] Tactic: 56 Time: 0.05848
[12/28/2021-10:07:31] [V] [TRT] Tactic: 57 Time: 0.06314
[12/28/2021-10:07:31] [V] [TRT] Tactic: 58 Time: 0.080704
[12/28/2021-10:07:31] [V] [TRT] Tactic: 60 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 61 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 62 Time: 0.040176
[12/28/2021-10:07:31] [V] [TRT] Tactic: 112 Time: 0.058524
[12/28/2021-10:07:31] [V] [TRT] Tactic: 113 Time: 0.351728
[12/28/2021-10:07:31] [V] [TRT] Tactic: 114 Time: 0.080704
[12/28/2021-10:07:31] [V] [TRT] Tactic: 116 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 117 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:31] [V] [TRT] Tactic: 118 Time: 0.040228
[12/28/2021-10:07:31] [V] [TRT] Fastest Tactic: 6 Time: 0.040176
[12/28/2021-10:07:31] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:31] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:31] [V] [TRT] Tactic: 4549827808004681195 Time: 0.117996
[12/28/2021-10:07:31] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:31] [V] [TRT] Tactic: 5779835512569528575 Time: 0.150868
[12/28/2021-10:07:31] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:31] [V] [TRT] Tactic: 6053873026024413720 Time: 0.183852
[12/28/2021-10:07:31] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6767548733843469815 Time: 0.137868
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:32] [V] [TRT] Tactic: -6313876406580483184 Time: 0.14062
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1123676555321336786 Time: 0.178824
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:32] [V] [TRT] Tactic: -701551393537224327 Time: 0.16172
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.117996
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 6
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Float(2048,1,512,128), Float(2048,1,512,128) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudnnConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2860655430572478466 Time: 0.090328
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4474630279712975759 Time: 0.0539
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4479823862704990365 Time: 0.051472
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4696204239951173149 Time: 0.095964
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5778138195697110003 Time: 0.151284
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:32] [V] [TRT] Tactic: 7155825427510256858 Time: 0.154876
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:32] [V] [TRT] Tactic: 8918020581761223752 Time: 0.151632
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:32] [V] [TRT] Tactic: -4756382386362004279 Time: 0.094512
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3855385237722507464 Time: 0.155628
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:32] [V] [TRT] Tactic: -2809379259463049391 Time: 0.15314
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:32] [V] [TRT] Tactic: -504296718212024303 Time: 0.145536
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.051472
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,128,32), Float(512,1:4,128,32) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudnnConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2860655430572478466 Time: 0.09052
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4474630279712975759 Time: 0.053872
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4479823862704990365 Time: 0.05154
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4696204239951173149 Time: 0.095892
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5778138195697110003 Time: 0.151188
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:32] [V] [TRT] Tactic: 7155825427510256858 Time: 0.15478
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:32] [V] [TRT] Tactic: 7342025736444949634 Time: 0.102676
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:32] [V] [TRT] Tactic: 8918020581761223752 Time: 0.1515
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7377458734869418330 Time: 0.101432
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5457304872213719461 Time: 0.102308
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:32] [V] [TRT] Tactic: -4756382386362004279 Time: 0.094584
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3855385237722507464 Time: 0.15556
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:32] [V] [TRT] Tactic: -2809379259463049391 Time: 0.153076
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:32] [V] [TRT] Tactic: -504296718212024303 Time: 0.145516
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.05154
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Float(2048,16,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudaDepthwiseConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1332468635798226953 Time: 0.062176
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1508480131241957639 Time: 0.0571
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1947019689364377201 Time: 0.04446
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3239257003214966313 Time: 0.061608
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5592640619112287921 Time: 0.036176
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:32] [V] [TRT] Tactic: 7621465827583909090 Time: 0.038084
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5576936487443445631 Time: 0.049368
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:32] [V] [TRT] Tactic: -2297737319934264721 Time: 0.056776
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1425085658556684465 Time: 0.05498
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:32] [V] [TRT] Tactic: -108011214168778087 Time: 0.043088
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.036176
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Int8(512,16:4,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudaDepthwiseConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (FusedConvActConvolution)
[12/28/2021-10:07:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:32] [V] [TRT] Tactic: 175853789719975416 Time: 0.05524
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2171150287007712632 Time: 0.053348
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2234457234705232274 Time: 0.03614
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5834048089706882838 Time: 0.047424
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:32] [V] [TRT] Tactic: -8626990807754934295 Time: 0.04156
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7303593854972602201 Time: 0.034632
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:32] [V] [TRT] Tactic: -6585664687867083638 Time: 0.059704
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3730012925709297561 Time: 0.042712
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:32] [V] [TRT] Tactic: -2277259417488004546 Time: 0.059876
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:32] [V] [TRT] Tactic: -683636008127039856 Time: 0.054836
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.034632
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Int8(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:32] [V] [TRT] Tactic: 984309058095623735 Time: 0.042868
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1100922622480907544 Time: 0.041472
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3238312825609165543 Time: 0.060124
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3606311198834416176 Time: 0.047536
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4325765560739862899 Time: 0.055264
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:32] [V] [TRT] Tactic: -4255737803793506479 Time: 0.059784
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3958182351168863467 Time: 0.034624
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3111968753064955248 Time: 0.0532
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1492575840277333548 Time: 0.055212
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:32] [V] [TRT] Tactic: -868495160148524802 Time: 0.036288
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.034624
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1), Float(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1011019097971850911 Time: 0.021496
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:32] [V] [TRT] Tactic: 1071114551801767124 Time: 0.013776
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2623576043214044314 Time: 0.009912
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3281631721811475881 Time: 0.011244
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4551754795416974366 Time: 0.010848
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4925112190271421402 Time: 0.009624
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5012796702462679112 Time: 0.037964
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5041593333398049019 Time: 0.00936
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5166018662410176512 Time: 0.036084
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6191867932654611882 Time: 0.020712
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6556170942941957134 Time: 0.025132
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6852868042694587230 Time: 0.01118
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:32] [V] [TRT] Tactic: 8399092794516815300 Time: 0.032876
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:32] [V] [TRT] Tactic: -9132922677633967263 Time: 0.01422
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7988637803896331454 Time: 0.022292
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7865001268126363229 Time: 0.03042
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7606074703023778034 Time: 0.022924
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7413564913826321357 Time: 0.021776
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7282232519526877434 Time: 0.036876
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5942379529065248478 Time: 0.014084
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5603587790314027122 Time: 0.028788
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5334776871777565833 Time: 0.036804
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5157868397078537095 Time: 0.02116
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:32] [V] [TRT] Tactic: -5100834417027499764 Time: 0.010316
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:32] [V] [TRT] Tactic: -3365360067423513506 Time: 0.00888
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:32] [V] [TRT] Tactic: -2194148180068068313 Time: 0.037604
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1782593837177056527 Time: 0.014524
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1610768292520086910 Time: 0.024076
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1573035963956198975 Time: 0.032376
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1558762241666006941 Time: 0.026336
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:32] [V] [TRT] Tactic: -1365353082499976145 Time: 0.02952
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:32] [V] [TRT] Tactic: -621838502160440068 Time: 0.029604
[12/28/2021-10:07:32] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.00888
[12/28/2021-10:07:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:32] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1), Int8(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudaGroupConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CudaDepthwiseConvolution)
[12/28/2021-10:07:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (FusedConvActConvolution)
[12/28/2021-10:07:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:32] [V] [TRT] --------------- Timing Runner: Conv_43 + Add_47 + Relu_50 (CaskConvolution)
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2325023763229477890 Time: 0.019664
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:32] [V] [TRT] Tactic: 2985940154541537814 Time: 0.024692
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3401614690060226673 Time: 0.009868
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:32] [V] [TRT] Tactic: 3899284354987683408 Time: 0.035284
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4042202769383439184 Time: 0.013448
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4182625619810185112 Time: 0.030048
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4717285412741024953 Time: 0.02378
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:32] [V] [TRT] Tactic: 4734519122557206480 Time: 0.031204
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:32] [V] [TRT] Tactic: 5136656982162849059 Time: 0.008636
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6004789655466615912 Time: 0.013712
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6146901278630392829 Time: 0.03084
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:32] [V] [TRT] Tactic: 6781129591847482048 Time: 0.013672
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:32] [V] [TRT] Tactic: 8096257414008860171 Time: 0.013384
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:32] [V] [TRT] Tactic: -9165697322068360861 Time: 0.031896
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:32] [V] [TRT] Tactic: -9118785798277698619 Time: 0.022404
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:32] [V] [TRT] Tactic: -8263994888336646547 Time: 0.019404
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:32] [V] [TRT] Tactic: -8205948405243401049 Time: 0.009492
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:32] [V] [TRT] Tactic: -7683887278997527517 Time: 0.010748
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:32] [V] [TRT] Tactic: -6400348606759295499 Time: 0.021984
[12/28/2021-10:07:32] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:33] [V] [TRT] Tactic: -5980889159865208399 Time: 0.03392
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:33] [V] [TRT] Tactic: -5766140806760372989 Time: 0.028344
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:33] [V] [TRT] Tactic: -4933563390723451692 Time: 0.010728
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:33] [V] [TRT] Tactic: -4516822589357530549 Time: 0.028876
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:33] [V] [TRT] Tactic: -3238475748440751107 Time: 0.012688
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:33] [V] [TRT] Tactic: -3182884991006484042 Time: 0.019504
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:33] [V] [TRT] Tactic: -3173468756112541306 Time: 0.009148
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:33] [V] [TRT] Tactic: -2917455979290586480 Time: 0.034928
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:33] [V] [TRT] Tactic: -2571022005763160364 Time: 0.025972
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:33] [V] [TRT] Tactic: -1546787387293556842 Time: 0.019128
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:33] [V] [TRT] Tactic: -1498626619443284096 Time: 0.014636
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:33] [V] [TRT] Tactic: -1283580231568512025 Time: 0.009372
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:33] [V] [TRT] Tactic: -762222380308749469 Time: 0.01064
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:33] [V] [TRT] Tactic: -516725800067794372 Time: 0.031156
[12/28/2021-10:07:33] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:33] [V] [TRT] Tactic: -428104331444385564 Time: 0.02906
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 5136656982162849059 Time: 0.008636
[12/28/2021-10:07:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5136656982162849059
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.0087
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.00584
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.00584
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008848
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005832
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005832
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008824
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005688
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005688
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007824
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.004856
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.004856
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007764
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005568
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005568
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00916
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005368
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005368
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007484
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005376
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005376
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.009184
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005688
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005688
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007988
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005672
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005672
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00788
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005704
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005704
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00922
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005552
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005552
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00736
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005508
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005508
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00734
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005744
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005744
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008932
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005776
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005776
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00884
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005736
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005736
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.009232
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.00556
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.00556
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.009264
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005488
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005488
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007472
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005604
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005604
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008444
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005792
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005792
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008328
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005708
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005708
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007624
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.004996
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.004996
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008848
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005904
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005904
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.0091
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005908
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005908
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.009208
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005828
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005828
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007532
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.00524
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.00524
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007628
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005684
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005684
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007972
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005888
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005888
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.008952
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005876
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005876
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.00838
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.005984
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.005984
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1002 Time: 0.007384
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.004684
[12/28/2021-10:07:33] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1 Time: 0.0052
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 0 Time: 0.004684
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning format combination: Float(2048,16,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudaDepthwiseConvolution)
[12/28/2021-10:07:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (FusedConvActConvolution)
[12/28/2021-10:07:33] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudnnConvolution)
[12/28/2021-10:07:33] [V] [TRT] Tactic: 0 Time: 0.055816
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1 Time: 0.058788
[12/28/2021-10:07:33] [V] [TRT] Tactic: 2 Time: 0.0789
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 5 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 6 Time: 0.037672
[12/28/2021-10:07:33] [V] [TRT] Tactic: 56 Time: 0.055788
[12/28/2021-10:07:33] [V] [TRT] Tactic: 57 Time: 0.061768
[12/28/2021-10:07:33] [V] [TRT] Tactic: 58 Time: 0.078164
[12/28/2021-10:07:33] [V] [TRT] Tactic: 60 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 61 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 62 Time: 0.037548
[12/28/2021-10:07:33] [V] [TRT] Tactic: 112 Time: 0.056028
[12/28/2021-10:07:33] [V] [TRT] Tactic: 113 Time: 0.34052
[12/28/2021-10:07:33] [V] [TRT] Tactic: 114 Time: 0.078108
[12/28/2021-10:07:33] [V] [TRT] Tactic: 116 skipped. Scratch requested: 47775744, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 117 skipped. Scratch requested: 106954752, available: 16777216
[12/28/2021-10:07:33] [V] [TRT] Tactic: 118 Time: 0.037564
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 62 Time: 0.037548
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4549827808004681195 Time: 0.116844
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:33] [V] [TRT] Tactic: 5779835512569528575 Time: 0.149732
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:33] [V] [TRT] Tactic: 6053873026024413720 Time: 0.182572
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:33] [V] [TRT] Tactic: 6767548733843469815 Time: 0.13702
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:33] [V] [TRT] Tactic: -6313876406580483184 Time: 0.139516
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:33] [V] [TRT] Tactic: -1123676555321336786 Time: 0.177712
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:33] [V] [TRT] Tactic: -701551393537224327 Time: 0.1607
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.116844
[12/28/2021-10:07:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 62
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning format combination: Float(2048,1,512,128) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudnnConvolution)
[12/28/2021-10:07:33] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:33] [V] [TRT] Tactic: 2860655430572478466 Time: 0.089468
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4474630279712975759 Time: 0.052896
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4479823862704990365 Time: 0.050584
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4696204239951173149 Time: 0.094952
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:33] [V] [TRT] Tactic: 5778138195697110003 Time: 0.151368
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:33] [V] [TRT] Tactic: 7155825427510256858 Time: 0.154
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:33] [V] [TRT] Tactic: 8918020581761223752 Time: 0.150684
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:33] [V] [TRT] Tactic: -4756382386362004279 Time: 0.093332
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:33] [V] [TRT] Tactic: -3855385237722507464 Time: 0.154812
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:33] [V] [TRT] Tactic: -2809379259463049391 Time: 0.152192
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:33] [V] [TRT] Tactic: -504296718212024303 Time: 0.1447
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.050584
[12/28/2021-10:07:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,128,32) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudnnConvolution)
[12/28/2021-10:07:33] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:33] [V] [TRT] Tactic: 2860655430572478466 Time: 0.089432
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4474630279712975759 Time: 0.052912
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4479823862704990365 Time: 0.050656
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:33] [V] [TRT] Tactic: 4696204239951173149 Time: 0.095024
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:33] [V] [TRT] Tactic: 5778138195697110003 Time: 0.15126
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:33] [V] [TRT] Tactic: 7155825427510256858 Time: 0.154004
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:33] [V] [TRT] Tactic: 7342025736444949634 Time: 0.1011
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:33] [V] [TRT] Tactic: 8918020581761223752 Time: 0.150884
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:33] [V] [TRT] Tactic: -7377458734869418330 Time: 0.099556
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:33] [V] [TRT] Tactic: -5457304872213719461 Time: 0.100556
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:33] [V] [TRT] Tactic: -4756382386362004279 Time: 0.093492
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:33] [V] [TRT] Tactic: -3855385237722507464 Time: 0.155068
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:33] [V] [TRT] Tactic: -2809379259463049391 Time: 0.152232
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:33] [V] [TRT] Tactic: -504296718212024303 Time: 0.14464
[12/28/2021-10:07:33] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.050656
[12/28/2021-10:07:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:33] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudaDepthwiseConvolution)
[12/28/2021-10:07:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:33] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1332468635798226953 Time: 0.060932
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1508480131241957639 Time: 0.055916
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:33] [V] [TRT] Tactic: 1947019689364377201 Time: 0.043992
[12/28/2021-10:07:33] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:33] [V] [TRT] Tactic: 3239257003214966313 Time: 0.060568
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5592640619112287921 Time: 0.035176
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:34] [V] [TRT] Tactic: 7621465827583909090 Time: 0.036932
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5576936487443445631 Time: 0.048204
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2297737319934264721 Time: 0.055904
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1425085658556684465 Time: 0.053964
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:34] [V] [TRT] Tactic: -108011214168778087 Time: 0.042004
[12/28/2021-10:07:34] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.035176
[12/28/2021-10:07:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:34] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudaDepthwiseConvolution)
[12/28/2021-10:07:34] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (FusedConvActConvolution)
[12/28/2021-10:07:34] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:34] [V] [TRT] Tactic: 175853789719975416 Time: 0.053836
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:34] [V] [TRT] Tactic: 2171150287007712632 Time: 0.051524
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:34] [V] [TRT] Tactic: 2234457234705232274 Time: 0.034572
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5834048089706882838 Time: 0.04592
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:34] [V] [TRT] Tactic: -8626990807754934295 Time: 0.039928
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7303593854972602201 Time: 0.033108
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:34] [V] [TRT] Tactic: -6585664687867083638 Time: 0.058016
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3730012925709297561 Time: 0.041128
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2277259417488004546 Time: 0.058316
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:34] [V] [TRT] Tactic: -683636008127039856 Time: 0.053196
[12/28/2021-10:07:34] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.033108
[12/28/2021-10:07:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:34] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:34] [V] [TRT] Tactic: 984309058095623735 Time: 0.040324
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:34] [V] [TRT] Tactic: 1100922622480907544 Time: 0.039828
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3238312825609165543 Time: 0.05832
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3606311198834416176 Time: 0.04572
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4325765560739862899 Time: 0.053228
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:34] [V] [TRT] Tactic: -4255737803793506479 Time: 0.057824
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3958182351168863467 Time: 0.032864
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3111968753064955248 Time: 0.051304
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1492575840277333548 Time: 0.05362
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:34] [V] [TRT] Tactic: -868495160148524802 Time: 0.034432
[12/28/2021-10:07:34] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.032864
[12/28/2021-10:07:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:34] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:34] [V] [TRT] Tactic: 1011019097971850911 Time: 0.02068
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:34] [V] [TRT] Tactic: 1071114551801767124 Time: 0.013364
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:34] [V] [TRT] Tactic: 2623576043214044314 Time: 0.009568
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3281631721811475881 Time: 0.010728
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4551754795416974366 Time: 0.01048
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4925112190271421402 Time: 0.009312
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5012796702462679112 Time: 0.034476
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5041593333398049019 Time: 0.009136
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5166018662410176512 Time: 0.03496
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6191867932654611882 Time: 0.020344
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6556170942941957134 Time: 0.02372
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6852868042694587230 Time: 0.010716
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:34] [V] [TRT] Tactic: 8399092794516815300 Time: 0.032376
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:34] [V] [TRT] Tactic: -9132922677633967263 Time: 0.013856
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7988637803896331454 Time: 0.020992
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7865001268126363229 Time: 0.0291
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7606074703023778034 Time: 0.02142
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7413564913826321357 Time: 0.021012
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7282232519526877434 Time: 0.033192
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5942379529065248478 Time: 0.013328
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5603587790314027122 Time: 0.027424
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5334776871777565833 Time: 0.035612
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5157868397078537095 Time: 0.020604
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5100834417027499764 Time: 0.00972
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3365360067423513506 Time: 0.008584
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2194148180068068313 Time: 0.034128
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1782593837177056527 Time: 0.013856
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1610768292520086910 Time: 0.022676
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1573035963956198975 Time: 0.031936
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1558762241666006941 Time: 0.02504
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1365353082499976145 Time: 0.028188
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:34] [V] [TRT] Tactic: -621838502160440068 Time: 0.02816
[12/28/2021-10:07:34] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.008584
[12/28/2021-10:07:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:34] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudaGroupConvolution)
[12/28/2021-10:07:34] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CudaDepthwiseConvolution)
[12/28/2021-10:07:34] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (FusedConvActConvolution)
[12/28/2021-10:07:34] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:34] [V] [TRT] --------------- Timing Runner: Conv_53 + Relu_56 (CaskConvolution)
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:34] [V] [TRT] Tactic: 177040020707947851 Time: 0.009988
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:34] [V] [TRT] Tactic: 1550399266192842845 Time: 0.009392
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:34] [V] [TRT] Tactic: 1572887561103143487 Time: 0.012676
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:34] [V] [TRT] Tactic: 2325023763229477890 Time: 0.01866
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:34] [V] [TRT] Tactic: 2985940154541537814 Time: 0.023492
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3284282970967328046 Time: 0.008556
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3401614690060226673 Time: 0.00954
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3512426920013359699 Time: 0.010184
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:34] [V] [TRT] Tactic: 3899284354987683408 Time: 0.033512
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4042202769383439184 Time: 0.012552
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4182625619810185112 Time: 0.028792
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4259547356717612415 Time: 0.013328
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4717285412741024953 Time: 0.022564
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:34] [V] [TRT] Tactic: 4734519122557206480 Time: 0.030132
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5121596860264626879 Time: 0.03018
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5136656982162849059 Time: 0.00856
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5158259316594207439 Time: 0.012828
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:34] [V] [TRT] Tactic: 5966973378912044513 Time: 0.01834
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6004789655466615912 Time: 0.012672
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6146901278630392829 Time: 0.029672
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6434020722187266170 Time: 0.03048
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:34] [V] [TRT] Tactic: 6781129591847482048 Time: 0.012912
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:34] [V] [TRT] Tactic: 7191893591576074000 Time: 0.009136
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:34] [V] [TRT] Tactic: 7438984192263206338 Time: 0.01248
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:34] [V] [TRT] Tactic: 7504901284678552178 Time: 0.018288
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:34] [V] [TRT] Tactic: 8096257414008860171 Time: 0.012376
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:34] [V] [TRT] Tactic: 9143438935315839085 Time: 0.00986
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:34] [V] [TRT] Tactic: -9165697322068360861 Time: 0.030292
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:34] [V] [TRT] Tactic: -9118785798277698619 Time: 0.021156
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:34] [V] [TRT] Tactic: -8263994888336646547 Time: 0.018276
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:34] [V] [TRT] Tactic: -8205948405243401049 Time: 0.009096
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7992068592656168418 Time: 0.012432
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7842775553137511386 Time: 0.018624
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:34] [V] [TRT] Tactic: -7683887278997527517 Time: 0.010284
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:34] [V] [TRT] Tactic: -6400348606759295499 Time: 0.021
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5980889159865208399 Time: 0.032272
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5766140806760372989 Time: 0.027384
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5709079507616090666 Time: 0.018152
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:34] [V] [TRT] Tactic: -5698636014239116282 Time: 0.029632
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:34] [V] [TRT] Tactic: -4933563390723451692 Time: 0.010304
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:34] [V] [TRT] Tactic: -4516822589357530549 Time: 0.027764
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3413217501222406256 Time: 0.029968
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3238475748440751107 Time: 0.012256
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3182884991006484042 Time: 0.018504
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:34] [V] [TRT] Tactic: -3173468756112541306 Time: 0.009016
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2917455979290586480 Time: 0.033112
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2571022005763160364 Time: 0.024824
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:34] [V] [TRT] Tactic: -2083778562631872334 Time: 0.01268
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:34] [V] [TRT] Tactic: -1546787387293556842 Time: 0.018068
[12/28/2021-10:07:34] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1498626619443284096 Time: 0.013784
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1283580231568512025 Time: 0.009184
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1173968681844185579 Time: 0.009056
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:35] [V] [TRT] Tactic: -762222380308749469 Time: 0.010228
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:35] [V] [TRT] Tactic: -556794153877490941 Time: 0.010188
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:35] [V] [TRT] Tactic: -516725800067794372 Time: 0.029816
[12/28/2021-10:07:35] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:35] [V] [TRT] Tactic: -428104331444385564 Time: 0.027936
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 3284282970967328046 Time: 0.008556
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3284282970967328046
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(2048,16,4,1), Float(2048,16,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(2048,1,512,128), Float(2048,1,512,128) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,128,32), Float(512,1:4,128,32) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Float(2048,16,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Int8(512,16:4,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1), Int8(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1), Float(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1), Int8(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(2048,16,4,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudaDepthwiseConvolution)
[12/28/2021-10:07:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (FusedConvActConvolution)
[12/28/2021-10:07:35] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudnnConvolution)
[12/28/2021-10:07:35] [V] [TRT] Tactic: 0 Time: 0.05796
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1 Time: 0.074816
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2 Time: 0.077436
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5 skipped. Scratch requested: 196083712, available: 16777216
[12/28/2021-10:07:35] [V] [TRT] Tactic: 56 Time: 0.057964
[12/28/2021-10:07:35] [V] [TRT] Tactic: 57 Time: 0.106296
[12/28/2021-10:07:35] [V] [TRT] Tactic: 58 Time: 0.077464
[12/28/2021-10:07:35] [V] [TRT] Tactic: 61 skipped. Scratch requested: 196083712, available: 16777216
[12/28/2021-10:07:35] [V] [TRT] Tactic: 112 Time: 0.057912
[12/28/2021-10:07:35] [V] [TRT] Tactic: 113 Time: 0.3429
[12/28/2021-10:07:35] [V] [TRT] Tactic: 114 Time: 0.077468
[12/28/2021-10:07:35] [V] [TRT] Tactic: 117 skipped. Scratch requested: 196083712, available: 16777216
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 112 Time: 0.057912
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4549827808004681195 Time: 0.117116
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5779835512569528575 Time: 0.154484
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:35] [V] [TRT] Tactic: 6053873026024413720 Time: 0.161348
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:35] [V] [TRT] Tactic: 6767548733843469815 Time: 0.122144
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:35] [V] [TRT] Tactic: -6313876406580483184 Time: 0.147632
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1123676555321336786 Time: 0.159536
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:35] [V] [TRT] Tactic: -701551393537224327 Time: 0.135116
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.117116
[12/28/2021-10:07:35] [V] [TRT] Setting workspace to 196083712enables more tactics for profiling
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(2048,1,512,128) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudnnConvolution)
[12/28/2021-10:07:35] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2860655430572478466 Time: 0.089052
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4474630279712975759 Time: 0.054152
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4479823862704990365 Time: 0.050536
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4696204239951173149 Time: 0.092648
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5778138195697110003 Time: 0.153744
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:35] [V] [TRT] Tactic: 7155825427510256858 Time: 0.148036
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:35] [V] [TRT] Tactic: 8918020581761223752 Time: 0.144628
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:35] [V] [TRT] Tactic: -4756382386362004279 Time: 0.09038
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3855385237722507464 Time: 0.160672
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:35] [V] [TRT] Tactic: -2809379259463049391 Time: 0.157668
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:35] [V] [TRT] Tactic: -504296718212024303 Time: 0.144212
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.050536
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,128,32) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudnnConvolution)
[12/28/2021-10:07:35] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2860655430572478466 Time: 0.089252
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4474630279712975759 Time: 0.054016
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4479823862704990365 Time: 0.05064
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4696204239951173149 Time: 0.09284
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5778138195697110003 Time: 0.1538
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:35] [V] [TRT] Tactic: 7155825427510256858 Time: 0.147908
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:35] [V] [TRT] Tactic: 7342025736444949634 Time: 0.101276
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:35] [V] [TRT] Tactic: 8918020581761223752 Time: 0.144688
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7377458734869418330 Time: 0.099628
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5457304872213719461 Time: 0.100576
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:35] [V] [TRT] Tactic: -4756382386362004279 Time: 0.090412
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3855385237722507464 Time: 0.16052
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:35] [V] [TRT] Tactic: -2809379259463049391 Time: 0.157556
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:35] [V] [TRT] Tactic: -504296718212024303 Time: 0.14422
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.05064
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudaDepthwiseConvolution)
[12/28/2021-10:07:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1332468635798226953 Time: 0.05988
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1508480131241957639 Time: 0.057748
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1947019689364377201 Time: 0.038472
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:35] [V] [TRT] Tactic: 3239257003214966313 Time: 0.059212
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5592640619112287921 Time: 0.035172
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:35] [V] [TRT] Tactic: 7621465827583909090 Time: 0.037048
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5576936487443445631 Time: 0.04166
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:35] [V] [TRT] Tactic: -2297737319934264721 Time: 0.049896
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1425085658556684465 Time: 0.043228
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:35] [V] [TRT] Tactic: -108011214168778087 Time: 0.044032
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.035172
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudaDepthwiseConvolution)
[12/28/2021-10:07:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (FusedConvActConvolution)
[12/28/2021-10:07:35] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:35] [V] [TRT] Tactic: 175853789719975416 Time: 0.047732
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2171150287007712632 Time: 0.041272
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2234457234705232274 Time: 0.034572
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5834048089706882838 Time: 0.03946
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:35] [V] [TRT] Tactic: -8626990807754934295 Time: 0.042092
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7303593854972602201 Time: 0.033196
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:35] [V] [TRT] Tactic: -6585664687867083638 Time: 0.054952
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3730012925709297561 Time: 0.036112
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:35] [V] [TRT] Tactic: -2277259417488004546 Time: 0.055548
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:35] [V] [TRT] Tactic: -683636008127039856 Time: 0.05368
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.033196
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:35] [V] [TRT] Tactic: 984309058095623735 Time: 0.0359
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1100922622480907544 Time: 0.041936
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:35] [V] [TRT] Tactic: 3238312825609165543 Time: 0.05508
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:35] [V] [TRT] Tactic: 3606311198834416176 Time: 0.038972
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4325765560739862899 Time: 0.053116
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:35] [V] [TRT] Tactic: -4255737803793506479 Time: 0.054436
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3958182351168863467 Time: 0.032952
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3111968753064955248 Time: 0.04098
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1492575840277333548 Time: 0.0476
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:35] [V] [TRT] Tactic: -868495160148524802 Time: 0.034488
[12/28/2021-10:07:35] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.032952
[12/28/2021-10:07:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:35] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:35] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1011019097971850911 Time: 0.019524
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:35] [V] [TRT] Tactic: 1071114551801767124 Time: 0.013188
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:35] [V] [TRT] Tactic: 2623576043214044314 Time: 0.009596
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:35] [V] [TRT] Tactic: 3281631721811475881 Time: 0.010524
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4551754795416974366 Time: 0.010432
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:35] [V] [TRT] Tactic: 4925112190271421402 Time: 0.009284
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5012796702462679112 Time: 0.032284
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5041593333398049019 Time: 0.00904
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:35] [V] [TRT] Tactic: 5166018662410176512 Time: 0.032192
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:35] [V] [TRT] Tactic: 6191867932654611882 Time: 0.02024
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:35] [V] [TRT] Tactic: 6556170942941957134 Time: 0.022088
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:35] [V] [TRT] Tactic: 6852868042694587230 Time: 0.010736
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:35] [V] [TRT] Tactic: 8399092794516815300 Time: 0.035304
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:35] [V] [TRT] Tactic: -9132922677633967263 Time: 0.013872
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7988637803896331454 Time: 0.02092
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7865001268126363229 Time: 0.024228
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7606074703023778034 Time: 0.02112
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7413564913826321357 Time: 0.020008
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:35] [V] [TRT] Tactic: -7282232519526877434 Time: 0.0317
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5942379529065248478 Time: 0.013408
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5603587790314027122 Time: 0.022656
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5334776871777565833 Time: 0.032788
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5157868397078537095 Time: 0.020464
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:35] [V] [TRT] Tactic: -5100834417027499764 Time: 0.009724
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:35] [V] [TRT] Tactic: -3365360067423513506 Time: 0.008544
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:35] [V] [TRT] Tactic: -2194148180068068313 Time: 0.031768
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:35] [V] [TRT] Tactic: -1782593837177056527 Time: 0.013828
[12/28/2021-10:07:35] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1610768292520086910 Time: 0.022348
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1573035963956198975 Time: 0.034936
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1558762241666006941 Time: 0.023848
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1365353082499976145 Time: 0.023452
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:36] [V] [TRT] Tactic: -621838502160440068 Time: 0.023412
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.008544
[12/28/2021-10:07:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudaGroupConvolution)
[12/28/2021-10:07:36] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CudaDepthwiseConvolution)
[12/28/2021-10:07:36] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (FusedConvActConvolution)
[12/28/2021-10:07:36] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_66 + Relu_69 (CaskConvolution)
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:36] [V] [TRT] Tactic: 177040020707947851 Time: 0.010096
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1550399266192842845 Time: 0.009348
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1572887561103143487 Time: 0.013096
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:36] [V] [TRT] Tactic: 2325023763229477890 Time: 0.01876
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:36] [V] [TRT] Tactic: 2985940154541537814 Time: 0.022068
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:36] [V] [TRT] Tactic: 3284282970967328046 Time: 0.008412
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:36] [V] [TRT] Tactic: 3401614690060226673 Time: 0.00954
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:36] [V] [TRT] Tactic: 3512426920013359699 Time: 0.010152
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:36] [V] [TRT] Tactic: 3899284354987683408 Time: 0.0329
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4042202769383439184 Time: 0.012516
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4182625619810185112 Time: 0.024232
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4259547356717612415 Time: 0.013544
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4717285412741024953 Time: 0.022524
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4734519122557206480 Time: 0.030252
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5121596860264626879 Time: 0.03008
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5136656982162849059 Time: 0.008396
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5158259316594207439 Time: 0.01258
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5966973378912044513 Time: 0.018476
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:36] [V] [TRT] Tactic: 6004789655466615912 Time: 0.013144
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:36] [V] [TRT] Tactic: 6146901278630392829 Time: 0.029672
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:36] [V] [TRT] Tactic: 6434020722187266170 Time: 0.030396
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:36] [V] [TRT] Tactic: 6781129591847482048 Time: 0.012712
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:36] [V] [TRT] Tactic: 7191893591576074000 Time: 0.008904
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:36] [V] [TRT] Tactic: 7438984192263206338 Time: 0.012348
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:36] [V] [TRT] Tactic: 7504901284678552178 Time: 0.018292
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:36] [V] [TRT] Tactic: 8096257414008860171 Time: 0.012596
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:36] [V] [TRT] Tactic: 9143438935315839085 Time: 0.009676
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:36] [V] [TRT] Tactic: -9165697322068360861 Time: 0.0304
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:36] [V] [TRT] Tactic: -9118785798277698619 Time: 0.021208
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:36] [V] [TRT] Tactic: -8263994888336646547 Time: 0.01832
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:36] [V] [TRT] Tactic: -8205948405243401049 Time: 0.00922
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:36] [V] [TRT] Tactic: -7992068592656168418 Time: 0.012504
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:36] [V] [TRT] Tactic: -7842775553137511386 Time: 0.018772
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:36] [V] [TRT] Tactic: -7683887278997527517 Time: 0.010252
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:36] [V] [TRT] Tactic: -6400348606759295499 Time: 0.020984
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5980889159865208399 Time: 0.032256
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5766140806760372989 Time: 0.02254
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5709079507616090666 Time: 0.018012
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5698636014239116282 Time: 0.029812
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:36] [V] [TRT] Tactic: -4933563390723451692 Time: 0.010244
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:36] [V] [TRT] Tactic: -4516822589357530549 Time: 0.023552
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:36] [V] [TRT] Tactic: -3413217501222406256 Time: 0.030012
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:36] [V] [TRT] Tactic: -3238475748440751107 Time: 0.012304
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:36] [V] [TRT] Tactic: -3182884991006484042 Time: 0.018428
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:36] [V] [TRT] Tactic: -3173468756112541306 Time: 0.008996
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[12/28/2021-10:07:36] [V] [TRT] Tactic: -2917455979290586480 Time: 0.032396
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[12/28/2021-10:07:36] [V] [TRT] Tactic: -2571022005763160364 Time: 0.023696
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:36] [V] [TRT] Tactic: -2083778562631872334 Time: 0.01276
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1546787387293556842 Time: 0.01802
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1498626619443284096 Time: 0.013816
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1283580231568512025 Time: 0.009124
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1173968681844185579 Time: 0.009044
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:36] [V] [TRT] Tactic: -762222380308749469 Time: 0.010152
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:36] [V] [TRT] Tactic: -556794153877490941 Time: 0.010272
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:36] [V] [TRT] Tactic: -516725800067794372 Time: 0.029908
[12/28/2021-10:07:36] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[12/28/2021-10:07:36] [V] [TRT] Tactic: -428104331444385564 Time: 0.023668
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: 5136656982162849059 Time: 0.008396
[12/28/2021-10:07:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5136656982162849059
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,16,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(2048,1,512,128) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(512,1:4,128,32) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Float(64,16:32,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(512,16:4,4,1) -> Int8(64,16:32,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,16,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(2048,1,512,128) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Float(512,1:4,128,32) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning Reformat:Int8(64,16:32,4,1) -> Int8(512,16:4,4,1) ***************
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning format combination: Float(2048,16,4,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CudaDepthwiseConvolution)
[12/28/2021-10:07:36] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (FusedConvActConvolution)
[12/28/2021-10:07:36] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CudnnConvolution)
[12/28/2021-10:07:36] [V] [TRT] Tactic: 0 Time: 0.01456
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1 Time: 0.01454
[12/28/2021-10:07:36] [V] [TRT] Tactic: 2 Time: 0.051424
[12/28/2021-10:07:36] [V] [TRT] Tactic: 56 Time: 0.014628
[12/28/2021-10:07:36] [V] [TRT] Tactic: 57 Time: 0.014576
[12/28/2021-10:07:36] [V] [TRT] Tactic: 58 Time: 0.051324
[12/28/2021-10:07:36] [V] [TRT] Tactic: 112 Time: 0.01454
[12/28/2021-10:07:36] [V] [TRT] Tactic: 113 Time: 0.01456
[12/28/2021-10:07:36] [V] [TRT] Tactic: 114 Time: 0.051344
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: 1 Time: 0.01454
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4549827808004681195 Time: 0.022052
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5779835512569528575 Time: 0.0312
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 9151672657204310840
[12/28/2021-10:07:36] [V] [TRT] Tactic: 9151672657204310840 Time: 0.037448
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[12/28/2021-10:07:36] [V] [TRT] Tactic: -7491730084094677098 Time: 0.025752
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:36] [V] [TRT] Tactic: -6313876406580483184 Time: 0.02668
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[12/28/2021-10:07:36] [V] [TRT] Tactic: -6273689210331812572 Time: 0.0307
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[12/28/2021-10:07:36] [V] [TRT] Tactic: -4337126844824617177 Time: 0.021264
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1123676555321336786 Time: 0.031052
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:36] [V] [TRT] Tactic: -701551393537224327 Time: 0.02244
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: -4337126844824617177 Time: 0.021264
[12/28/2021-10:07:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 1
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning format combination: Float(2048,1,512,128) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CudnnConvolution)
[12/28/2021-10:07:36] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1663866669559596164 Time: 0.025172
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:36] [V] [TRT] Tactic: 2860655430572478466 Time: 0.017688
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4474630279712975759 Time: 0.013512
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4479823862704990365 Time: 0.013396
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4696204239951173149 Time: 0.017596
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5778138195697110003 Time: 0.025476
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:36] [V] [TRT] Tactic: 8918020581761223752 Time: 0.02406
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5905193483742532701 Time: 0.016544
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:36] [V] [TRT] Tactic: -4035591156787122265 Time: 0.013192
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:36] [V] [TRT] Tactic: -2809379259463049391 Time: 0.025324
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1985235291706575900 Time: 0.02396
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:36] [V] [TRT] Tactic: -504296718212024303 Time: 0.024188
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.013192
[12/28/2021-10:07:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:36] [V] [TRT] *************** Autotuning format combination: Float(512,1:4,128,32) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CudnnConvolution)
[12/28/2021-10:07:36] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:36] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 1373022415249282411
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1373022415249282411 Time: 0.018448
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:36] [V] [TRT] Tactic: 1663866669559596164 Time: 0.025156
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:36] [V] [TRT] Tactic: 2860655430572478466 Time: 0.01766
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4474630279712975759 Time: 0.01362
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4479823862704990365 Time: 0.01336
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:36] [V] [TRT] Tactic: 4696204239951173149 Time: 0.01754
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:36] [V] [TRT] Tactic: 5778138195697110003 Time: 0.025472
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:36] [V] [TRT] Tactic: 7342025736444949634 Time: 0.018724
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:36] [V] [TRT] Tactic: 8918020581761223752 Time: 0.024032
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5905193483742532701 Time: 0.016604
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:36] [V] [TRT] Tactic: -5457304872213719461 Time: 0.019136
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:36] [V] [TRT] Tactic: -4035591156787122265 Time: 0.013216
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:36] [V] [TRT] Tactic: -2809379259463049391 Time: 0.025332
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:36] [V] [TRT] Tactic: -1985235291706575900 Time: 0.023964
[12/28/2021-10:07:36] [V] [TRT] Conv_75 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:36] [V] [TRT] Tactic: -504296718212024303 Time: 0.024004
[12/28/2021-10:07:36] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.013216
[12/28/2021-10:07:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:37] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CudaDepthwiseConvolution)
[12/28/2021-10:07:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:37] [V] [TRT] Tactic: 1508480131241957639 Time: 0.0185
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 2141154648944475104
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2141154648944475104 Time: 0.018348
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:37] [V] [TRT] Tactic: 3239257003214966313 Time: 0.018524
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5592640619112287921 Time: 0.012612
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:37] [V] [TRT] Tactic: 7621465827583909090 Time: 0.013392
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -6580271968881459581
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6580271968881459581 Time: 0.013728
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5576936487443445631 Time: 0.013452
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -4443833619060044580
[12/28/2021-10:07:37] [V] [TRT] Tactic: -4443833619060044580 Time: 0.012608
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:37] [V] [TRT] Tactic: -2297737319934264721 Time: 0.014176
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:37] [V] [TRT] Tactic: -1425085658556684465 Time: 0.013232
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:37] [V] [TRT] Tactic: -108011214168778087 Time: 0.013876
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -42427192380281294
[12/28/2021-10:07:37] [V] [TRT] Tactic: -42427192380281294 Time: 0.013152
[12/28/2021-10:07:37] [V] [TRT] Fastest Tactic: -4443833619060044580 Time: 0.012608
[12/28/2021-10:07:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4443833619060044580
[12/28/2021-10:07:37] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CudaDepthwiseConvolution)
[12/28/2021-10:07:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (FusedConvActConvolution)
[12/28/2021-10:07:37] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:37] [V] [TRT] Tactic: 175853789719975416 Time: 0.012208
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2171150287007712632 Time: 0.01122
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2234457234705232274 Time: 0.010972
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5834048089706882838 Time: 0.011244
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 6299962968199310600
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6299962968199310600 Time: 0.014204
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 6341572697076960911
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6341572697076960911 Time: 0.010624
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8626990807754934295 Time: 0.011768
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -8498217049614706532
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8498217049614706532 Time: 0.010788
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7303593854972602201 Time: 0.010764
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6585664687867083638 Time: 0.014292
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3326139578711341011
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3326139578711341011 Time: 0.011652
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:37] [V] [TRT] Tactic: -683636008127039856 Time: 0.014228
[12/28/2021-10:07:37] [V] [TRT] Fastest Tactic: 6341572697076960911 Time: 0.010624
[12/28/2021-10:07:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6341572697076960911
[12/28/2021-10:07:37] [V] [TRT] *************** Autotuning format combination: Int8(512,16:4,4,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:37] [V] [TRT] Tactic: 1100922622480907544 Time: 0.011588
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 2855900226702061782
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2855900226702061782 Time: 0.013644
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:37] [V] [TRT] Tactic: 3606311198834416176 Time: 0.010896
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4325765560739862899 Time: 0.01374
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 8803458114157674373
[12/28/2021-10:07:37] [V] [TRT] Tactic: 8803458114157674373 Time: 0.01062
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -6934773036503365000
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6934773036503365000 Time: 0.011436
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -4431642509665791294
[12/28/2021-10:07:37] [V] [TRT] Tactic: -4431642509665791294 Time: 0.010464
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:37] [V] [TRT] Tactic: -4255737803793506479 Time: 0.013788
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3958182351168863467 Time: 0.010492
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3111968753064955248 Time: 0.011052
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:37] [V] [TRT] Tactic: -1492575840277333548 Time: 0.011912
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:37] [V] [TRT] Tactic: -868495160148524802 Time: 0.010788
[12/28/2021-10:07:37] [V] [TRT] Fastest Tactic: -4431642509665791294 Time: 0.010464
[12/28/2021-10:07:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4431642509665791294
[12/28/2021-10:07:37] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2623576043214044314 Time: 0.006616
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 2818014835119698671
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2818014835119698671 Time: 0.0083
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 3721599319722771137
[12/28/2021-10:07:37] [V] [TRT] Tactic: 3721599319722771137 Time: 0.0065
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 4178917718361232468
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4178917718361232468 Time: 0.009452
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4551754795416974366 Time: 0.006776
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4925112190271421402 Time: 0.006348
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5012796702462679112 Time: 0.01144
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6556170942941957134 Time: 0.009692
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6618077155362058131
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6618077155362058131 Time: 0.006052
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6852868042694587230 Time: 0.00718
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 6969462133921577484
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6969462133921577484 Time: 0.015576
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:37] [V] [TRT] Tactic: 8399092794516815300 Time: 0.015712
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:37] [V] [TRT] Tactic: -9132922677633967263 Time: 0.008324
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -8912999970161746151
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8912999970161746151 Time: 0.008268
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -8893439100868426414
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8893439100868426414 Time: 0.009412
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7988637803896331454 Time: 0.00962
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -7904635102498369361
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7904635102498369361 Time: 0.01114
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7606074703023778034 Time: 0.009564
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7413564913826321357 Time: 0.009368
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7282232519526877434 Time: 0.011224
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -6406011580107094428
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6406011580107094428 Time: 0.00692
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5603587790314027122 Time: 0.009672
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -5416590980288859834
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5416590980288859834 Time: 0.009804
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5334776871777565833 Time: 0.01288
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5157868397078537095 Time: 0.010808
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3665201838779845683
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3665201838779845683 Time: 0.0126
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -3644377136375731441
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3644377136375731441 Time: 0.009692
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3502495740607894730
[12/28/2021-10:07:37] [V] [TRT] Tactic: -3502495740607894730 Time: 0.006276
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -2342404147487779225
[12/28/2021-10:07:37] [V] [TRT] Tactic: -2342404147487779225 Time: 0.010672
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:37] [V] [TRT] Tactic: -1782593837177056527 Time: 0.008412
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:37] [V] [TRT] Tactic: -1610768292520086910 Time: 0.00974
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:37] [V] [TRT] Tactic: -621838502160440068 Time: 0.009732
[12/28/2021-10:07:37] [V] [TRT] Fastest Tactic: 6618077155362058131 Time: 0.006052
[12/28/2021-10:07:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6618077155362058131
[12/28/2021-10:07:37] [V] [TRT] *************** Autotuning format combination: Int8(64,16:32,4,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CudaGroupConvolution)
[12/28/2021-10:07:37] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CudaDepthwiseConvolution)
[12/28/2021-10:07:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (FusedConvActConvolution)
[12/28/2021-10:07:37] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:37] [V] [TRT] --------------- Timing Runner: Conv_75 (CaskConvolution)
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:37] [V] [TRT] Tactic: 177040020707947851 Time: 0.006836
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 434957160407688216
[12/28/2021-10:07:37] [V] [TRT] Tactic: 434957160407688216 Time: 0.011684
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 805889586762897346
[12/28/2021-10:07:37] [V] [TRT] Tactic: 805889586762897346 Time: 0.010496
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:37] [V] [TRT] Tactic: 1550399266192842845 Time: 0.006504
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2325023763229477890 Time: 0.008592
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 2346437292116182513
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2346437292116182513 Time: 0.0096
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 2522133112320625287
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2522133112320625287 Time: 0.009656
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:37] [V] [TRT] Tactic: 2985940154541537814 Time: 0.009584
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 3538565962642681625
[12/28/2021-10:07:37] [V] [TRT] Tactic: 3538565962642681625 Time: 0.006516
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:37] [V] [TRT] Tactic: 3899284354987683408 Time: 0.011968
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4042202769383439184 Time: 0.007396
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4259547356717612415 Time: 0.007692
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4717285412741024953 Time: 0.009728
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:37] [V] [TRT] Tactic: 4734519122557206480 Time: 0.011016
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5121596860264626879 Time: 0.010876
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 5126565865931538390
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5126565865931538390 Time: 0.009588
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5158259316594207439 Time: 0.007452
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 5375256703210220108
[12/28/2021-10:07:37] [V] [TRT] Tactic: 5375256703210220108 Time: 0.007324
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6433368103202497147
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6433368103202497147 Time: 0.008388
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6434020722187266170 Time: 0.01086
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6441948709525127755
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6441948709525127755 Time: 0.006264
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6457435868048963632
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6457435868048963632 Time: 0.007248
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6781129591847482048 Time: 0.00758
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6925201228918187099
[12/28/2021-10:07:37] [V] [TRT] Tactic: 6925201228918187099 Time: 0.00846
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:37] [V] [TRT] Tactic: 7504901284678552178 Time: 0.008512
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7731430299029542276
[12/28/2021-10:07:37] [V] [TRT] Tactic: 7731430299029542276 Time: 0.008552
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7738495016763012180
[12/28/2021-10:07:37] [V] [TRT] Tactic: 7738495016763012180 Time: 0.010776
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 8234775147403903473
[12/28/2021-10:07:37] [V] [TRT] Tactic: 8234775147403903473 Time: 0.01052
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:37] [V] [TRT] Tactic: -9165697322068360861 Time: 0.010808
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:37] [V] [TRT] Tactic: -9118785798277698619 Time: 0.009508
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -8556775352640313933
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8556775352640313933 Time: 0.0084
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8263994888336646547 Time: 0.008644
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:37] [V] [TRT] Tactic: -8205948405243401049 Time: 0.006628
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7842775553137511386 Time: 0.008644
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:37] [V] [TRT] Tactic: -7683887278997527517 Time: 0.00682
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -6527178416855951297
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6527178416855951297 Time: 0.006548
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -6510232214299595844
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6510232214299595844 Time: 0.006452
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:37] [V] [TRT] Tactic: -6400348606759295499 Time: 0.00962
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5980889159865208399 Time: 0.011812
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5766140806760372989 Time: 0.009724
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -5170003087447722174
[12/28/2021-10:07:37] [V] [TRT] Tactic: -5170003087447722174 Time: 0.006436
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -4849712423393454704
[12/28/2021-10:07:37] [V] [TRT] Tactic: -4849712423393454704 Time: 0.007212
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:37] [V] [TRT] Tactic: -4516822589357530549 Time: 0.009912
[12/28/2021-10:07:37] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -3613322253849278738
[12/28/2021-10:07:38] [V] [TRT] Tactic: -3613322253849278738 Time: 0.006156
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -3577322188448771475
[12/28/2021-10:07:38] [V] [TRT] Tactic: -3577322188448771475 Time: 0.00746
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -2754311112012636251
[12/28/2021-10:07:38] [V] [TRT] Tactic: -2754311112012636251 Time: 0.007608
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: -2315453944962430928
[12/28/2021-10:07:38] [V] [TRT] Tactic: -2315453944962430928 Time: 0.010748
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:38] [V] [TRT] Tactic: -2083778562631872334 Time: 0.0075
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -1499578657823798783
[12/28/2021-10:07:38] [V] [TRT] Tactic: -1499578657823798783 Time: 0.006572
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:38] [V] [TRT] Tactic: -1498626619443284096 Time: 0.0078
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:38] [V] [TRT] Tactic: -1283580231568512025 Time: 0.006504
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:38] [V] [TRT] Tactic: -1173968681844185579 Time: 0.006424
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:38] [V] [TRT] Tactic: -762222380308749469 Time: 0.006728
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -713022856474991236
[12/28/2021-10:07:38] [V] [TRT] Tactic: -713022856474991236 Time: 0.006128
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:38] [V] [TRT] Tactic: -556794153877490941 Time: 0.006724
[12/28/2021-10:07:38] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -375949437730908730
[12/28/2021-10:07:38] [V] [TRT] Tactic: -375949437730908730 Time: 0.007296
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: -713022856474991236 Time: 0.006128
[12/28/2021-10:07:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -713022856474991236
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009664
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005224
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005224
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009664
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005444
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005444
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.00854
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.004948
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.004948
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.00866
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005424
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005424
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.010044
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005228
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005228
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008572
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005312
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005312
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008836
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005452
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005452
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008812
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005448
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005448
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.010064
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005284
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005284
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008612
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005252
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005252
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009724
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005456
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005456
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009644
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.00552
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.00552
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009968
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.00516
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.00516
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.010216
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005264
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005264
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008516
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005356
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005356
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009268
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005604
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005604
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009072
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.00556
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.00556
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.007188
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.004836
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.004836
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.00956
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.006144
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.006144
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.00986
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.006032
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.006032
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.007924
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.0051
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.0051
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.007204
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005448
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005448
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008876
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.006072
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.006072
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009856
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.006036
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.006036
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.007732
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.004844
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1 Time: 0.00522
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.004844
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009668
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005308
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005308
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.01028
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005232
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005232
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.008572
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005352
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005352
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.01
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005612
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005612
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1002 Time: 0.009236
[12/28/2021-10:07:38] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.005552
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 0 Time: 0.005552
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning format combination: Float(1024,4,2,1), Float(1024,4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudaDepthwiseConvolution)
[12/28/2021-10:07:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (FusedConvActConvolution)
[12/28/2021-10:07:38] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudnnConvolution)
[12/28/2021-10:07:38] [V] [TRT] Tactic: 0 Time: 0.10712
[12/28/2021-10:07:38] [V] [TRT] Tactic: 1 Time: 0.100196
[12/28/2021-10:07:38] [V] [TRT] Tactic: 2 Time: 0.110892
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 5 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 6 Time: 0.067144
[12/28/2021-10:07:38] [V] [TRT] Tactic: 56 Time: 0.107408
[12/28/2021-10:07:38] [V] [TRT] Tactic: 57 Time: 0.099436
[12/28/2021-10:07:38] [V] [TRT] Tactic: 58 Time: 0.11088
[12/28/2021-10:07:38] [V] [TRT] Tactic: 60 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 61 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 62 Time: 0.06702
[12/28/2021-10:07:38] [V] [TRT] Tactic: 112 Time: 0.107104
[12/28/2021-10:07:38] [V] [TRT] Tactic: 113 Time: 0.236444
[12/28/2021-10:07:38] [V] [TRT] Tactic: 114 Time: 0.111044
[12/28/2021-10:07:38] [V] [TRT] Tactic: 116 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 117 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:38] [V] [TRT] Tactic: 118 Time: 0.067144
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 62 Time: 0.06702
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4549827808004681195 Time: 0.227732
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:38] [V] [TRT] Tactic: 5779835512569528575 Time: 0.304528
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:38] [V] [TRT] Tactic: 6053873026024413720 Time: 0.318604
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:38] [V] [TRT] Tactic: 6767548733843469815 Time: 0.23454
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:38] [V] [TRT] Tactic: -6313876406580483184 Time: 0.271632
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:38] [V] [TRT] Tactic: -1123676555321336786 Time: 0.31184
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:38] [V] [TRT] Tactic: -701551393537224327 Time: 0.261764
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.227732
[12/28/2021-10:07:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 62
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning format combination: Float(1024,1,512,256), Float(1024,1,512,256) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudnnConvolution)
[12/28/2021-10:07:38] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:38] [V] [TRT] Tactic: 2860655430572478466 Time: 0.170336
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4474630279712975759 Time: 0.10034
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4479823862704990365 Time: 0.093096
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4696204239951173149 Time: 0.177224
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:38] [V] [TRT] Tactic: 5778138195697110003 Time: 0.29144
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:38] [V] [TRT] Tactic: 7155825427510256858 Time: 0.286568
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:38] [V] [TRT] Tactic: 8918020581761223752 Time: 0.28022
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:38] [V] [TRT] Tactic: -4756382386362004279 Time: 0.17242
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:38] [V] [TRT] Tactic: -3855385237722507464 Time: 0.30812
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:38] [V] [TRT] Tactic: -2809379259463049391 Time: 0.304112
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:38] [V] [TRT] Tactic: -504296718212024303 Time: 0.279832
[12/28/2021-10:07:38] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.093096
[12/28/2021-10:07:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:38] [V] [TRT] *************** Autotuning format combination: Float(256,1:4,128,64), Float(256,1:4,128,64) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudnnConvolution)
[12/28/2021-10:07:38] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:38] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:38] [V] [TRT] Tactic: 2860655430572478466 Time: 0.17014
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4474630279712975759 Time: 0.100204
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4479823862704990365 Time: 0.093028
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:38] [V] [TRT] Tactic: 4696204239951173149 Time: 0.177392
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:38] [V] [TRT] Tactic: 5778138195697110003 Time: 0.29142
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:38] [V] [TRT] Tactic: 7155825427510256858 Time: 0.2865
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:38] [V] [TRT] Tactic: 7342025736444949634 Time: 0.195504
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:38] [V] [TRT] Tactic: 8918020581761223752 Time: 0.280264
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:38] [V] [TRT] Tactic: -7377458734869418330 Time: 0.192304
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:38] [V] [TRT] Tactic: -5457304872213719461 Time: 0.19384
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:38] [V] [TRT] Tactic: -4756382386362004279 Time: 0.172552
[12/28/2021-10:07:38] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3855385237722507464 Time: 0.308216
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:39] [V] [TRT] Tactic: -2809379259463049391 Time: 0.30392
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:39] [V] [TRT] Tactic: -504296718212024303 Time: 0.279732
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.093028
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Float(1024,4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudaDepthwiseConvolution)
[12/28/2021-10:07:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1332468635798226953 Time: 0.1084
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1508480131241957639 Time: 0.10542
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1947019689364377201 Time: 0.067976
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:39] [V] [TRT] Tactic: 3239257003214966313 Time: 0.108156
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5592640619112287921 Time: 0.062308
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:39] [V] [TRT] Tactic: 7621465827583909090 Time: 0.065828
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5576936487443445631 Time: 0.074168
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:39] [V] [TRT] Tactic: -2297737319934264721 Time: 0.087916
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1425085658556684465 Time: 0.07776
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:39] [V] [TRT] Tactic: -108011214168778087 Time: 0.076096
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.062308
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Int8(256,4:4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudaDepthwiseConvolution)
[12/28/2021-10:07:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (FusedConvActConvolution)
[12/28/2021-10:07:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:39] [V] [TRT] Tactic: 175853789719975416 Time: 0.08642
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:39] [V] [TRT] Tactic: 2171150287007712632 Time: 0.07628
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:39] [V] [TRT] Tactic: 2234457234705232274 Time: 0.063788
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5834048089706882838 Time: 0.072168
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:39] [V] [TRT] Tactic: -8626990807754934295 Time: 0.07426
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7303593854972602201 Time: 0.060584
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:39] [V] [TRT] Tactic: -6585664687867083638 Time: 0.103452
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3730012925709297561 Time: 0.066012
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:39] [V] [TRT] Tactic: -2277259417488004546 Time: 0.104348
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:39] [V] [TRT] Tactic: -683636008127039856 Time: 0.10078
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.060584
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Int8(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:39] [V] [TRT] Tactic: 984309058095623735 Time: 0.065664
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1100922622480907544 Time: 0.074324
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:39] [V] [TRT] Tactic: 3238312825609165543 Time: 0.10368
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:39] [V] [TRT] Tactic: 3606311198834416176 Time: 0.07214
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:39] [V] [TRT] Tactic: 4325765560739862899 Time: 0.100016
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:39] [V] [TRT] Tactic: -4255737803793506479 Time: 0.102584
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3958182351168863467 Time: 0.060436
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3111968753064955248 Time: 0.076224
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1492575840277333548 Time: 0.08624
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:39] [V] [TRT] Tactic: -868495160148524802 Time: 0.063796
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.060436
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1), Float(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1011019097971850911 Time: 0.031636
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1071114551801767124 Time: 0.019216
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:39] [V] [TRT] Tactic: 2623576043214044314 Time: 0.012528
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:39] [V] [TRT] Tactic: 3281631721811475881 Time: 0.015
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:39] [V] [TRT] Tactic: 4551754795416974366 Time: 0.014172
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:39] [V] [TRT] Tactic: 4925112190271421402 Time: 0.012216
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5012796702462679112 Time: 0.057192
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5041593333398049019 Time: 0.012016
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5166018662410176512 Time: 0.054948
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6191867932654611882 Time: 0.03186
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6556170942941957134 Time: 0.037512
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6852868042694587230 Time: 0.0147
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:39] [V] [TRT] Tactic: 8399092794516815300 Time: 0.058296
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:39] [V] [TRT] Tactic: -9132922677633967263 Time: 0.019776
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7988637803896331454 Time: 0.034936
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7865001268126363229 Time: 0.0405
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7606074703023778034 Time: 0.034908
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7413564913826321357 Time: 0.031932
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7282232519526877434 Time: 0.056008
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5942379529065248478 Time: 0.019464
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5603587790314027122 Time: 0.037552
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5334776871777565833 Time: 0.055652
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5157868397078537095 Time: 0.032024
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:39] [V] [TRT] Tactic: -5100834417027499764 Time: 0.013224
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3365360067423513506 Time: 0.0109
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:39] [V] [TRT] Tactic: -2194148180068068313 Time: 0.056084
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1782593837177056527 Time: 0.0199
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1610768292520086910 Time: 0.037088
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1573035963956198975 Time: 0.05788
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1558762241666006941 Time: 0.040024
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1365353082499976145 Time: 0.03926
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:39] [V] [TRT] Tactic: -621838502160440068 Time: 0.03916
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.0109
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1), Int8(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudaGroupConvolution)
[12/28/2021-10:07:39] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CudaDepthwiseConvolution)
[12/28/2021-10:07:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (FusedConvActConvolution)
[12/28/2021-10:07:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Conv_72 + Add_76 + Relu_79 (CaskConvolution)
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:39] [V] [TRT] Tactic: 2325023763229477890 Time: 0.030524
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:39] [V] [TRT] Tactic: 3401614690060226673 Time: 0.013112
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:39] [V] [TRT] Tactic: 4042202769383439184 Time: 0.018556
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:39] [V] [TRT] Tactic: 4734519122557206480 Time: 0.053404
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:39] [V] [TRT] Tactic: 5136656982162849059 Time: 0.010748
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6004789655466615912 Time: 0.019524
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6146901278630392829 Time: 0.052956
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:39] [V] [TRT] Tactic: 6781129591847482048 Time: 0.019192
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:39] [V] [TRT] Tactic: 8096257414008860171 Time: 0.018712
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:39] [V] [TRT] Tactic: -9165697322068360861 Time: 0.0535
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:39] [V] [TRT] Tactic: -8263994888336646547 Time: 0.030472
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:39] [V] [TRT] Tactic: -8205948405243401049 Time: 0.01198
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:39] [V] [TRT] Tactic: -7683887278997527517 Time: 0.013904
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:39] [V] [TRT] Tactic: -4933563390723451692 Time: 0.014656
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3238475748440751107 Time: 0.018296
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3182884991006484042 Time: 0.030384
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:39] [V] [TRT] Tactic: -3173468756112541306 Time: 0.011832
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1546787387293556842 Time: 0.030144
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1498626619443284096 Time: 0.02096
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:39] [V] [TRT] Tactic: -1283580231568512025 Time: 0.011596
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:39] [V] [TRT] Tactic: -762222380308749469 Time: 0.014148
[12/28/2021-10:07:39] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:39] [V] [TRT] Tactic: -516725800067794372 Time: 0.052768
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 5136656982162849059 Time: 0.010748
[12/28/2021-10:07:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5136656982162849059
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.009684
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.005424
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.005424
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.009636
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.005376
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.005376
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.009744
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.00524
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.00524
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.008548
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.004884
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.004884
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.008532
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.00536
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.00536
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.010032
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.005184
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.005184
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 1002 Time: 0.008648
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:39] [V] [TRT] Tactic: 0 Time: 0.005156
[12/28/2021-10:07:39] [V] [TRT] Fastest Tactic: 0 Time: 0.005156
[12/28/2021-10:07:39] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.010332
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005288
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005288
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.008908
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005408
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005408
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.008748
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005516
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005516
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.010092
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005292
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005292
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.008528
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.00522
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.00522
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.008636
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.0053
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.0053
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009696
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005456
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005456
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009704
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005556
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005556
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.010064
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.00522
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.00522
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.010204
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005344
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005344
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.00858
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.00526
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.00526
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009212
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005556
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005556
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.00908
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.00558
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.00558
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.007112
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.004816
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.004816
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009668
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.006036
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.006036
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.00996
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.00612
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.00612
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.010052
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005616
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005616
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.00792
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005184
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005184
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.007236
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005428
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005428
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.008996
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.006044
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.006044
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009872
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.006108
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.006108
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.009288
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.005524
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.005524
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1002 Time: 0.00772
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.004816
[12/28/2021-10:07:40] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1 Time: 0.005244
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 0 Time: 0.004816
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning format combination: Float(1024,4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudaDepthwiseConvolution)
[12/28/2021-10:07:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (FusedConvActConvolution)
[12/28/2021-10:07:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudnnConvolution)
[12/28/2021-10:07:40] [V] [TRT] Tactic: 0 Time: 0.1046
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1 Time: 0.13572
[12/28/2021-10:07:40] [V] [TRT] Tactic: 2 Time: 0.108408
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 6 Time: 0.06404
[12/28/2021-10:07:40] [V] [TRT] Tactic: 56 Time: 0.104692
[12/28/2021-10:07:40] [V] [TRT] Tactic: 57 Time: 0.10722
[12/28/2021-10:07:40] [V] [TRT] Tactic: 58 Time: 0.108536
[12/28/2021-10:07:40] [V] [TRT] Tactic: 60 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 61 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 62 Time: 0.06392
[12/28/2021-10:07:40] [V] [TRT] Tactic: 112 Time: 0.104648
[12/28/2021-10:07:40] [V] [TRT] Tactic: 113 Time: 0.245808
[12/28/2021-10:07:40] [V] [TRT] Tactic: 114 Time: 0.108552
[12/28/2021-10:07:40] [V] [TRT] Tactic: 116 skipped. Scratch requested: 172228608, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 117 skipped. Scratch requested: 356515840, available: 16777216
[12/28/2021-10:07:40] [V] [TRT] Tactic: 118 Time: 0.063992
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 62 Time: 0.06392
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4549827808004681195 Time: 0.226708
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5779835512569528575 Time: 0.302184
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[12/28/2021-10:07:40] [V] [TRT] Tactic: 6053873026024413720 Time: 0.316296
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[12/28/2021-10:07:40] [V] [TRT] Tactic: 6767548733843469815 Time: 0.233748
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:40] [V] [TRT] Tactic: -6313876406580483184 Time: 0.270504
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:40] [V] [TRT] Tactic: -1123676555321336786 Time: 0.309028
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:40] [V] [TRT] Tactic: -701551393537224327 Time: 0.26044
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.226708
[12/28/2021-10:07:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 62
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning format combination: Float(1024,1,512,256) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudnnConvolution)
[12/28/2021-10:07:40] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:40] [V] [TRT] Tactic: 2860655430572478466 Time: 0.169456
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4474630279712975759 Time: 0.099348
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4479823862704990365 Time: 0.092224
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4696204239951173149 Time: 0.176484
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5778138195697110003 Time: 0.290456
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:40] [V] [TRT] Tactic: 7155825427510256858 Time: 0.285596
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:40] [V] [TRT] Tactic: 8918020581761223752 Time: 0.279308
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:40] [V] [TRT] Tactic: -4756382386362004279 Time: 0.171732
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:40] [V] [TRT] Tactic: -3855385237722507464 Time: 0.307496
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:40] [V] [TRT] Tactic: -2809379259463049391 Time: 0.303152
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:40] [V] [TRT] Tactic: -504296718212024303 Time: 0.279068
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.092224
[12/28/2021-10:07:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning format combination: Float(256,1:4,128,64) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudnnConvolution)
[12/28/2021-10:07:40] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:40] [V] [TRT] Tactic: 2860655430572478466 Time: 0.169448
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4474630279712975759 Time: 0.099448
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4479823862704990365 Time: 0.092132
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:40] [V] [TRT] Tactic: 4696204239951173149 Time: 0.176392
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5778138195697110003 Time: 0.29028
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[12/28/2021-10:07:40] [V] [TRT] Tactic: 7155825427510256858 Time: 0.28552
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:40] [V] [TRT] Tactic: 7342025736444949634 Time: 0.193916
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:40] [V] [TRT] Tactic: 8918020581761223752 Time: 0.279216
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:40] [V] [TRT] Tactic: -7377458734869418330 Time: 0.190808
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:40] [V] [TRT] Tactic: -5457304872213719461 Time: 0.192468
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[12/28/2021-10:07:40] [V] [TRT] Tactic: -4756382386362004279 Time: 0.171708
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[12/28/2021-10:07:40] [V] [TRT] Tactic: -3855385237722507464 Time: 0.307312
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:40] [V] [TRT] Tactic: -2809379259463049391 Time: 0.302936
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:40] [V] [TRT] Tactic: -504296718212024303 Time: 0.279016
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 4479823862704990365 Time: 0.092132
[12/28/2021-10:07:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4479823862704990365
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudaDepthwiseConvolution)
[12/28/2021-10:07:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1332468635798226953 Time: 0.1064
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1508480131241957639 Time: 0.102588
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[12/28/2021-10:07:40] [V] [TRT] Tactic: 1947019689364377201 Time: 0.066392
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:40] [V] [TRT] Tactic: 3239257003214966313 Time: 0.1055
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5592640619112287921 Time: 0.061004
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:40] [V] [TRT] Tactic: 7621465827583909090 Time: 0.064404
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:40] [V] [TRT] Tactic: -5576936487443445631 Time: 0.072876
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:40] [V] [TRT] Tactic: -2297737319934264721 Time: 0.086672
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:40] [V] [TRT] Tactic: -1425085658556684465 Time: 0.077036
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:40] [V] [TRT] Tactic: -108011214168778087 Time: 0.07486
[12/28/2021-10:07:40] [V] [TRT] Fastest Tactic: 5592640619112287921 Time: 0.061004
[12/28/2021-10:07:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5592640619112287921
[12/28/2021-10:07:40] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudaDepthwiseConvolution)
[12/28/2021-10:07:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (FusedConvActConvolution)
[12/28/2021-10:07:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:40] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:40] [V] [TRT] Tactic: 175853789719975416 Time: 0.084592
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:40] [V] [TRT] Tactic: 2171150287007712632 Time: 0.0742
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:40] [V] [TRT] Tactic: 2234457234705232274 Time: 0.062056
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:40] [V] [TRT] Tactic: 5834048089706882838 Time: 0.070612
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:40] [V] [TRT] Tactic: -8626990807754934295 Time: 0.072692
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:40] [V] [TRT] Tactic: -7303593854972602201 Time: 0.05896
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:40] [V] [TRT] Tactic: -6585664687867083638 Time: 0.101168
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[12/28/2021-10:07:40] [V] [TRT] Tactic: -3730012925709297561 Time: 0.064044
[12/28/2021-10:07:40] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[12/28/2021-10:07:41] [V] [TRT] Tactic: -2277259417488004546 Time: 0.102268
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:41] [V] [TRT] Tactic: -683636008127039856 Time: 0.09844
[12/28/2021-10:07:41] [V] [TRT] Fastest Tactic: -7303593854972602201 Time: 0.05896
[12/28/2021-10:07:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7303593854972602201
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[12/28/2021-10:07:41] [V] [TRT] Tactic: 984309058095623735 Time: 0.06378
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1100922622480907544 Time: 0.072516
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3238312825609165543 Time: 0.101656
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3606311198834416176 Time: 0.070336
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4325765560739862899 Time: 0.098024
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:41] [V] [TRT] Tactic: -4255737803793506479 Time: 0.100576
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3958182351168863467 Time: 0.058708
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3111968753064955248 Time: 0.074056
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1492575840277333548 Time: 0.084452
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:41] [V] [TRT] Tactic: -868495160148524802 Time: 0.06186
[12/28/2021-10:07:41] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.058708
[12/28/2021-10:07:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1011019097971850911 Time: 0.030756
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1071114551801767124 Time: 0.018784
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:41] [V] [TRT] Tactic: 2623576043214044314 Time: 0.012424
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3281631721811475881 Time: 0.014608
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4551754795416974366 Time: 0.01354
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4925112190271421402 Time: 0.011812
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5012796702462679112 Time: 0.055836
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5041593333398049019 Time: 0.01156
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5166018662410176512 Time: 0.054104
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6191867932654611882 Time: 0.031204
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6556170942941957134 Time: 0.035956
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6852868042694587230 Time: 0.01428
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:41] [V] [TRT] Tactic: 8399092794516815300 Time: 0.057276
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:41] [V] [TRT] Tactic: -9132922677633967263 Time: 0.019192
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7988637803896331454 Time: 0.033588
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7865001268126363229 Time: 0.03974
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7606074703023778034 Time: 0.033912
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7413564913826321357 Time: 0.031168
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7282232519526877434 Time: 0.054592
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5942379529065248478 Time: 0.018988
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5603587790314027122 Time: 0.036668
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5334776871777565833 Time: 0.054784
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5157868397078537095 Time: 0.031492
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5100834417027499764 Time: 0.012768
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3365360067423513506 Time: 0.010608
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[12/28/2021-10:07:41] [V] [TRT] Tactic: -2194148180068068313 Time: 0.054684
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1782593837177056527 Time: 0.019232
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1610768292520086910 Time: 0.03636
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1573035963956198975 Time: 0.056804
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1558762241666006941 Time: 0.038876
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1365353082499976145 Time: 0.038524
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:41] [V] [TRT] Tactic: -621838502160440068 Time: 0.038532
[12/28/2021-10:07:41] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.010608
[12/28/2021-10:07:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudaGroupConvolution)
[12/28/2021-10:07:41] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CudaDepthwiseConvolution)
[12/28/2021-10:07:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (FusedConvActConvolution)
[12/28/2021-10:07:41] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_82 + Relu_85 (CaskConvolution)
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:41] [V] [TRT] Tactic: 177040020707947851 Time: 0.01328
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1550399266192842845 Time: 0.011756
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1572887561103143487 Time: 0.01894
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:41] [V] [TRT] Tactic: 2325023763229477890 Time: 0.02976
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3284282970967328046 Time: 0.010564
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3401614690060226673 Time: 0.012732
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:41] [V] [TRT] Tactic: 3512426920013359699 Time: 0.01418
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4042202769383439184 Time: 0.018216
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4259547356717612415 Time: 0.019756
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:41] [V] [TRT] Tactic: 4734519122557206480 Time: 0.05202
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5121596860264626879 Time: 0.05226
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5136656982162849059 Time: 0.01068
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5158259316594207439 Time: 0.0181
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5966973378912044513 Time: 0.029516
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6004789655466615912 Time: 0.018872
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6146901278630392829 Time: 0.051604
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6434020722187266170 Time: 0.052372
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:41] [V] [TRT] Tactic: 6781129591847482048 Time: 0.018356
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:41] [V] [TRT] Tactic: 7191893591576074000 Time: 0.011672
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:41] [V] [TRT] Tactic: 7438984192263206338 Time: 0.017628
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:41] [V] [TRT] Tactic: 7504901284678552178 Time: 0.029396
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:41] [V] [TRT] Tactic: 8096257414008860171 Time: 0.017972
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:41] [V] [TRT] Tactic: 9143438935315839085 Time: 0.013144
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:41] [V] [TRT] Tactic: -9165697322068360861 Time: 0.052312
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:41] [V] [TRT] Tactic: -8263994888336646547 Time: 0.0294
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:41] [V] [TRT] Tactic: -8205948405243401049 Time: 0.011816
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7992068592656168418 Time: 0.018044
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7842775553137511386 Time: 0.029728
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:41] [V] [TRT] Tactic: -7683887278997527517 Time: 0.013532
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5709079507616090666 Time: 0.028984
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:41] [V] [TRT] Tactic: -5698636014239116282 Time: 0.051724
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:41] [V] [TRT] Tactic: -4933563390723451692 Time: 0.014228
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3413217501222406256 Time: 0.051816
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3238475748440751107 Time: 0.01764
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3182884991006484042 Time: 0.02956
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:41] [V] [TRT] Tactic: -3173468756112541306 Time: 0.011604
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:41] [V] [TRT] Tactic: -2083778562631872334 Time: 0.018208
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1546787387293556842 Time: 0.029024
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1498626619443284096 Time: 0.020192
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1283580231568512025 Time: 0.01144
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:41] [V] [TRT] Tactic: -1173968681844185579 Time: 0.011212
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:41] [V] [TRT] Tactic: -762222380308749469 Time: 0.013728
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:41] [V] [TRT] Tactic: -556794153877490941 Time: 0.013652
[12/28/2021-10:07:41] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:41] [V] [TRT] Tactic: -516725800067794372 Time: 0.051664
[12/28/2021-10:07:41] [V] [TRT] Fastest Tactic: 3284282970967328046 Time: 0.010564
[12/28/2021-10:07:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3284282970967328046
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Float(1024,4,2,1), Float(1024,4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Float(1024,1,512,256), Float(1024,1,512,256) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Float(256,1:4,128,64), Float(256,1:4,128,64) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Float(1024,4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Int8(256,4:4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1), Int8(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1), Float(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1), Int8(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:41] [V] [TRT] *************** Autotuning format combination: Float(1024,4,2,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudaDepthwiseConvolution)
[12/28/2021-10:07:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (FusedConvActConvolution)
[12/28/2021-10:07:41] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:41] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudnnConvolution)
[12/28/2021-10:07:41] [V] [TRT] Tactic: 0 Time: 0.113212
[12/28/2021-10:07:41] [V] [TRT] Tactic: 1 Time: 0.079232
[12/28/2021-10:07:41] [V] [TRT] Tactic: 2 Time: 0.109536
[12/28/2021-10:07:41] [V] [TRT] Tactic: 5 skipped. Scratch requested: 677380096, available: 16777216
[12/28/2021-10:07:41] [V] [TRT] Tactic: 56 Time: 0.113344
[12/28/2021-10:07:41] [V] [TRT] Tactic: 57 Time: 0.08926
[12/28/2021-10:07:41] [V] [TRT] Tactic: 58 Time: 0.109424
[12/28/2021-10:07:41] [V] [TRT] Tactic: 61 skipped. Scratch requested: 677380096, available: 16777216
[12/28/2021-10:07:41] [V] [TRT] Tactic: 112 Time: 0.113348
[12/28/2021-10:07:42] [V] [TRT] Tactic: 113 Time: 0.255232
[12/28/2021-10:07:42] [V] [TRT] Tactic: 114 Time: 0.109396
[12/28/2021-10:07:42] [V] [TRT] Tactic: 117 skipped. Scratch requested: 677380096, available: 16777216
[12/28/2021-10:07:42] [V] [TRT] Fastest Tactic: 1 Time: 0.079232
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] Setting workspace to 677380096enables more tactics for profiling
[12/28/2021-10:07:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 1
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Float(1024,1,512,256) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudnnConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Float(256,1:4,128,64) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudnnConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:42] [V] [TRT] Tactic: 7342025736444949634 Time: 0.193404
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7377458734869418330 Time: 0.19064
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5457304872213719461 Time: 0.192228
[12/28/2021-10:07:42] [V] [TRT] Fastest Tactic: -7377458734869418330 Time: 0.19064
[12/28/2021-10:07:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7377458734869418330
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudaDepthwiseConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudaDepthwiseConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (FusedConvActConvolution)
[12/28/2021-10:07:42] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:42] [V] [TRT] Tactic: 1011019097971850911 Time: 0.029912
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:42] [V] [TRT] Tactic: 1071114551801767124 Time: 0.017724
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:42] [V] [TRT] Tactic: 2623576043214044314 Time: 0.01218
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:42] [V] [TRT] Tactic: 3281631721811475881 Time: 0.014296
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4551754795416974366 Time: 0.013296
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4925112190271421402 Time: 0.011576
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5041593333398049019 Time: 0.01154
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5166018662410176512 Time: 0.051868
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6191867932654611882 Time: 0.029544
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6852868042694587230 Time: 0.014264
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:42] [V] [TRT] Tactic: 8399092794516815300 Time: 0.053068
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:42] [V] [TRT] Tactic: -9132922677633967263 Time: 0.018468
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7413564913826321357 Time: 0.031704
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5942379529065248478 Time: 0.018348
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5334776871777565833 Time: 0.052428
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5157868397078537095 Time: 0.029808
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5100834417027499764 Time: 0.012564
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:42] [V] [TRT] Tactic: -3365360067423513506 Time: 0.010476
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1782593837177056527 Time: 0.018584
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1573035963956198975 Time: 0.052548
[12/28/2021-10:07:42] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.010476
[12/28/2021-10:07:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudaGroupConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CudaDepthwiseConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (FusedConvActConvolution)
[12/28/2021-10:07:42] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_95 + Relu_98 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:42] [V] [TRT] Tactic: 177040020707947851 Time: 0.01316
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:42] [V] [TRT] Tactic: 1550399266192842845 Time: 0.01178
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:42] [V] [TRT] Tactic: 1572887561103143487 Time: 0.021136
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:42] [V] [TRT] Tactic: 2325023763229477890 Time: 0.03036
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:42] [V] [TRT] Tactic: 3284282970967328046 Time: 0.01042
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:42] [V] [TRT] Tactic: 3401614690060226673 Time: 0.01252
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:42] [V] [TRT] Tactic: 3512426920013359699 Time: 0.014544
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4042202769383439184 Time: 0.01808
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4259547356717612415 Time: 0.022312
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4734519122557206480 Time: 0.052072
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5121596860264626879 Time: 0.051884
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5136656982162849059 Time: 0.01046
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5158259316594207439 Time: 0.018224
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5966973378912044513 Time: 0.029768
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6004789655466615912 Time: 0.021416
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6146901278630392829 Time: 0.051604
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6434020722187266170 Time: 0.052404
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:42] [V] [TRT] Tactic: 6781129591847482048 Time: 0.018496
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:42] [V] [TRT] Tactic: 7191893591576074000 Time: 0.011556
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:42] [V] [TRT] Tactic: 7438984192263206338 Time: 0.017452
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:42] [V] [TRT] Tactic: 7504901284678552178 Time: 0.029148
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:42] [V] [TRT] Tactic: 8096257414008860171 Time: 0.018096
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:42] [V] [TRT] Tactic: 9143438935315839085 Time: 0.012464
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:42] [V] [TRT] Tactic: -9165697322068360861 Time: 0.052168
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:42] [V] [TRT] Tactic: -8263994888336646547 Time: 0.029124
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:42] [V] [TRT] Tactic: -8205948405243401049 Time: 0.011608
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7992068592656168418 Time: 0.018148
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7842775553137511386 Time: 0.030176
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7683887278997527517 Time: 0.013344
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5709079507616090666 Time: 0.029072
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:42] [V] [TRT] Tactic: -5698636014239116282 Time: 0.051176
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:42] [V] [TRT] Tactic: -4933563390723451692 Time: 0.014188
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:42] [V] [TRT] Tactic: -3413217501222406256 Time: 0.051544
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:42] [V] [TRT] Tactic: -3238475748440751107 Time: 0.017272
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:42] [V] [TRT] Tactic: -3182884991006484042 Time: 0.029756
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:42] [V] [TRT] Tactic: -3173468756112541306 Time: 0.011388
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:42] [V] [TRT] Tactic: -2083778562631872334 Time: 0.018124
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1546787387293556842 Time: 0.029004
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1498626619443284096 Time: 0.022152
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1283580231568512025 Time: 0.0111
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1173968681844185579 Time: 0.010948
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:42] [V] [TRT] Tactic: -762222380308749469 Time: 0.014
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:42] [V] [TRT] Tactic: -556794153877490941 Time: 0.013888
[12/28/2021-10:07:42] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:42] [V] [TRT] Tactic: -516725800067794372 Time: 0.05142
[12/28/2021-10:07:42] [V] [TRT] Fastest Tactic: 3284282970967328046 Time: 0.01042
[12/28/2021-10:07:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3284282970967328046
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(1024,1,512,256) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(256,1:4,128,64) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Float(32,4:32,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(256,4:4,2,1) -> Int8(32,4:32,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(1024,1,512,256) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Float(256,1:4,128,64) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning Reformat:Int8(32,4:32,2,1) -> Int8(256,4:4,2,1) ***************
[12/28/2021-10:07:42] [V] [TRT] *************** Autotuning format combination: Float(1024,4,2,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_104 (CudaDepthwiseConvolution)
[12/28/2021-10:07:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_104 (FusedConvActConvolution)
[12/28/2021-10:07:42] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_104 (CudnnConvolution)
[12/28/2021-10:07:42] [V] [TRT] Tactic: 0 Time: 0.047884
[12/28/2021-10:07:42] [V] [TRT] Tactic: 1 Time: 0.107648
[12/28/2021-10:07:42] [V] [TRT] Tactic: 2 Time: 0.11056
[12/28/2021-10:07:42] [V] [TRT] Tactic: 56 Time: 0.047736
[12/28/2021-10:07:42] [V] [TRT] Tactic: 57 Time: 0.110432
[12/28/2021-10:07:42] [V] [TRT] Tactic: 58 Time: 0.110644
[12/28/2021-10:07:42] [V] [TRT] Tactic: 112 Time: 0.047672
[12/28/2021-10:07:42] [V] [TRT] Tactic: 113 Time: 0.132224
[12/28/2021-10:07:42] [V] [TRT] Tactic: 114 Time: 0.110692
[12/28/2021-10:07:42] [V] [TRT] Fastest Tactic: 112 Time: 0.047672
[12/28/2021-10:07:42] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:42] [V] [TRT] Tactic: 4549827808004681195 Time: 0.033564
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:42] [V] [TRT] Tactic: 5779835512569528575 Time: 0.043784
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 9151672657204310840
[12/28/2021-10:07:42] [V] [TRT] Tactic: 9151672657204310840 Time: 0.045376
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[12/28/2021-10:07:42] [V] [TRT] Tactic: -7491730084094677098 Time: 0.040788
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:42] [V] [TRT] Tactic: -6313876406580483184 Time: 0.042572
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[12/28/2021-10:07:42] [V] [TRT] Tactic: -6273689210331812572 Time: 0.043172
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[12/28/2021-10:07:42] [V] [TRT] Tactic: -4337126844824617177 Time: 0.031468
[12/28/2021-10:07:42] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:42] [V] [TRT] Tactic: -1123676555321336786 Time: 0.043312
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:43] [V] [TRT] Tactic: -701551393537224327 Time: 0.034712
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: -4337126844824617177 Time: 0.031468
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4337126844824617177
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Float(1024,1,512,256) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudnnConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1663866669559596164 Time: 0.039304
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2860655430572478466 Time: 0.02626
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4474630279712975759 Time: 0.017908
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4479823862704990365 Time: 0.017672
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4696204239951173149 Time: 0.026012
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:43] [V] [TRT] Tactic: 5778138195697110003 Time: 0.039776
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:43] [V] [TRT] Tactic: 8918020581761223752 Time: 0.038256
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5905193483742532701 Time: 0.023972
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:43] [V] [TRT] Tactic: -4035591156787122265 Time: 0.017404
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:43] [V] [TRT] Tactic: -2809379259463049391 Time: 0.039368
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1985235291706575900 Time: 0.038172
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:43] [V] [TRT] Tactic: -504296718212024303 Time: 0.0385
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.017404
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Float(256,1:4,128,64) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudnnConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 1373022415249282411
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1373022415249282411 Time: 0.027892
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1663866669559596164 Time: 0.0394
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2860655430572478466 Time: 0.026472
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4474630279712975759 Time: 0.017916
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4479823862704990365 Time: 0.017836
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4696204239951173149 Time: 0.026084
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:43] [V] [TRT] Tactic: 5778138195697110003 Time: 0.039928
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:43] [V] [TRT] Tactic: 7342025736444949634 Time: 0.028156
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:43] [V] [TRT] Tactic: 8918020581761223752 Time: 0.03828
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5905193483742532701 Time: 0.024168
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5457304872213719461 Time: 0.028604
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:43] [V] [TRT] Tactic: -4035591156787122265 Time: 0.017588
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:43] [V] [TRT] Tactic: -2809379259463049391 Time: 0.03954
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1985235291706575900 Time: 0.038276
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:43] [V] [TRT] Tactic: -504296718212024303 Time: 0.038756
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.017588
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudaDepthwiseConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1508480131241957639 Time: 0.022128
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 2141154648944475104
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2141154648944475104 Time: 0.022004
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[12/28/2021-10:07:43] [V] [TRT] Tactic: 3239257003214966313 Time: 0.022124
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[12/28/2021-10:07:43] [V] [TRT] Tactic: 5592640619112287921 Time: 0.015296
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[12/28/2021-10:07:43] [V] [TRT] Tactic: 7621465827583909090 Time: 0.016176
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -6580271968881459581
[12/28/2021-10:07:43] [V] [TRT] Tactic: -6580271968881459581 Time: 0.01714
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5576936487443445631 Time: 0.01632
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -4443833619060044580
[12/28/2021-10:07:43] [V] [TRT] Tactic: -4443833619060044580 Time: 0.015228
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[12/28/2021-10:07:43] [V] [TRT] Tactic: -2297737319934264721 Time: 0.018124
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1425085658556684465 Time: 0.016024
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[12/28/2021-10:07:43] [V] [TRT] Tactic: -108011214168778087 Time: 0.017556
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -42427192380281294
[12/28/2021-10:07:43] [V] [TRT] Tactic: -42427192380281294 Time: 0.015424
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: -4443833619060044580 Time: 0.015228
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4443833619060044580
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudaDepthwiseConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (FusedConvActConvolution)
[12/28/2021-10:07:43] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[12/28/2021-10:07:43] [V] [TRT] Tactic: 175853789719975416 Time: 0.016072
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2171150287007712632 Time: 0.014192
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2234457234705232274 Time: 0.013724
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[12/28/2021-10:07:43] [V] [TRT] Tactic: 5834048089706882838 Time: 0.01398
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 6299962968199310600
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6299962968199310600 Time: 0.018436
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 6341572697076960911
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6341572697076960911 Time: 0.013056
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[12/28/2021-10:07:43] [V] [TRT] Tactic: -8626990807754934295 Time: 0.01548
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -8498217049614706532
[12/28/2021-10:07:43] [V] [TRT] Tactic: -8498217049614706532 Time: 0.013232
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7303593854972602201 Time: 0.013148
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[12/28/2021-10:07:43] [V] [TRT] Tactic: -6585664687867083638 Time: 0.018684
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3326139578711341011
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3326139578711341011 Time: 0.015332
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[12/28/2021-10:07:43] [V] [TRT] Tactic: -683636008127039856 Time: 0.01866
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: 6341572697076960911 Time: 0.013056
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6341572697076960911
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Int8(256,4:4,2,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1100922622480907544 Time: 0.015488
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 2855900226702061782
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2855900226702061782 Time: 0.018064
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[12/28/2021-10:07:43] [V] [TRT] Tactic: 3606311198834416176 Time: 0.013808
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4325765560739862899 Time: 0.018188
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 8803458114157674373
[12/28/2021-10:07:43] [V] [TRT] Tactic: 8803458114157674373 Time: 0.01294
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -6934773036503365000
[12/28/2021-10:07:43] [V] [TRT] Tactic: -6934773036503365000 Time: 0.01502
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -4431642509665791294
[12/28/2021-10:07:43] [V] [TRT] Tactic: -4431642509665791294 Time: 0.012864
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[12/28/2021-10:07:43] [V] [TRT] Tactic: -4255737803793506479 Time: 0.018044
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3958182351168863467 Time: 0.01286
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3111968753064955248 Time: 0.013856
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1492575840277333548 Time: 0.0158
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[12/28/2021-10:07:43] [V] [TRT] Tactic: -868495160148524802 Time: 0.01336
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: -3958182351168863467 Time: 0.01286
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3958182351168863467
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2623576043214044314 Time: 0.006736
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 2818014835119698671
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2818014835119698671 Time: 0.007632
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 3721599319722771137
[12/28/2021-10:07:43] [V] [TRT] Tactic: 3721599319722771137 Time: 0.006684
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 4178917718361232468
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4178917718361232468 Time: 0.010676
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4551754795416974366 Time: 0.00704
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4925112190271421402 Time: 0.006492
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[12/28/2021-10:07:43] [V] [TRT] Tactic: 5012796702462679112 Time: 0.01358
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6556170942941957134 Time: 0.010616
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6618077155362058131
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6618077155362058131 Time: 0.006132
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6852868042694587230 Time: 0.007032
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 6969462133921577484
[12/28/2021-10:07:43] [V] [TRT] Tactic: 6969462133921577484 Time: 0.013172
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:43] [V] [TRT] Tactic: 8399092794516815300 Time: 0.013496
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:43] [V] [TRT] Tactic: -9132922677633967263 Time: 0.007964
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -8912999970161746151
[12/28/2021-10:07:43] [V] [TRT] Tactic: -8912999970161746151 Time: 0.007612
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -8893439100868426414
[12/28/2021-10:07:43] [V] [TRT] Tactic: -8893439100868426414 Time: 0.00932
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7988637803896331454 Time: 0.01052
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -7904635102498369361
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7904635102498369361 Time: 0.013372
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7606074703023778034 Time: 0.010568
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7413564913826321357 Time: 0.010112
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[12/28/2021-10:07:43] [V] [TRT] Tactic: -7282232519526877434 Time: 0.013336
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -6406011580107094428
[12/28/2021-10:07:43] [V] [TRT] Tactic: -6406011580107094428 Time: 0.00664
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5603587790314027122 Time: 0.010924
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -5416590980288859834
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5416590980288859834 Time: 0.010628
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5334776871777565833 Time: 0.013108
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:43] [V] [TRT] Tactic: -5157868397078537095 Time: 0.009856
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3665201838779845683
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3665201838779845683 Time: 0.012372
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -3644377136375731441
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3644377136375731441 Time: 0.010708
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -3502495740607894730
[12/28/2021-10:07:43] [V] [TRT] Tactic: -3502495740607894730 Time: 0.006328
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -2342404147487779225
[12/28/2021-10:07:43] [V] [TRT] Tactic: -2342404147487779225 Time: 0.00928
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1782593837177056527 Time: 0.007936
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[12/28/2021-10:07:43] [V] [TRT] Tactic: -1610768292520086910 Time: 0.0109
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[12/28/2021-10:07:43] [V] [TRT] Tactic: -621838502160440068 Time: 0.010996
[12/28/2021-10:07:43] [V] [TRT] Fastest Tactic: 6618077155362058131 Time: 0.006132
[12/28/2021-10:07:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6618077155362058131
[12/28/2021-10:07:43] [V] [TRT] *************** Autotuning format combination: Int8(32,4:32,2,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudaGroupConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CudaDepthwiseConvolution)
[12/28/2021-10:07:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (FusedConvActConvolution)
[12/28/2021-10:07:43] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:43] [V] [TRT] --------------- Timing Runner: Conv_104 (CaskConvolution)
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:43] [V] [TRT] Tactic: 177040020707947851 Time: 0.006948
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 434957160407688216
[12/28/2021-10:07:43] [V] [TRT] Tactic: 434957160407688216 Time: 0.013956
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 805889586762897346
[12/28/2021-10:07:43] [V] [TRT] Tactic: 805889586762897346 Time: 0.012464
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:43] [V] [TRT] Tactic: 1550399266192842845 Time: 0.006596
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2325023763229477890 Time: 0.010188
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 2346437292116182513
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2346437292116182513 Time: 0.010644
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: 2522133112320625287
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2522133112320625287 Time: 0.010676
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[12/28/2021-10:07:43] [V] [TRT] Tactic: 2985940154541537814 Time: 0.010772
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 3538565962642681625
[12/28/2021-10:07:43] [V] [TRT] Tactic: 3538565962642681625 Time: 0.0065
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[12/28/2021-10:07:43] [V] [TRT] Tactic: 3899284354987683408 Time: 0.014112
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4042202769383439184 Time: 0.007672
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:43] [V] [TRT] Tactic: 4259547356717612415 Time: 0.0086
[12/28/2021-10:07:43] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[12/28/2021-10:07:44] [V] [TRT] Tactic: 4717285412741024953 Time: 0.011052
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:44] [V] [TRT] Tactic: 4734519122557206480 Time: 0.012896
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:44] [V] [TRT] Tactic: 5121596860264626879 Time: 0.0127
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 5126565865931538390
[12/28/2021-10:07:44] [V] [TRT] Tactic: 5126565865931538390 Time: 0.010812
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:44] [V] [TRT] Tactic: 5158259316594207439 Time: 0.00774
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 5375256703210220108
[12/28/2021-10:07:44] [V] [TRT] Tactic: 5375256703210220108 Time: 0.007624
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 6433368103202497147
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6433368103202497147 Time: 0.009256
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6434020722187266170 Time: 0.013236
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6441948709525127755
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6441948709525127755 Time: 0.006232
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6457435868048963632
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6457435868048963632 Time: 0.0075
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6781129591847482048 Time: 0.008012
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6925201228918187099
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6925201228918187099 Time: 0.009272
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:44] [V] [TRT] Tactic: 7504901284678552178 Time: 0.009452
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7731430299029542276
[12/28/2021-10:07:44] [V] [TRT] Tactic: 7731430299029542276 Time: 0.009132
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: 7738495016763012180
[12/28/2021-10:07:44] [V] [TRT] Tactic: 7738495016763012180 Time: 0.012624
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 8234775147403903473
[12/28/2021-10:07:44] [V] [TRT] Tactic: 8234775147403903473 Time: 0.012416
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:44] [V] [TRT] Tactic: -9165697322068360861 Time: 0.013232
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[12/28/2021-10:07:44] [V] [TRT] Tactic: -9118785798277698619 Time: 0.010676
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -8556775352640313933
[12/28/2021-10:07:44] [V] [TRT] Tactic: -8556775352640313933 Time: 0.009288
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:44] [V] [TRT] Tactic: -8263994888336646547 Time: 0.009504
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:44] [V] [TRT] Tactic: -8205948405243401049 Time: 0.006424
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:44] [V] [TRT] Tactic: -7842775553137511386 Time: 0.010068
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:44] [V] [TRT] Tactic: -7683887278997527517 Time: 0.006928
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -6527178416855951297
[12/28/2021-10:07:44] [V] [TRT] Tactic: -6527178416855951297 Time: 0.006568
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -6510232214299595844
[12/28/2021-10:07:44] [V] [TRT] Tactic: -6510232214299595844 Time: 0.0066
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[12/28/2021-10:07:44] [V] [TRT] Tactic: -6400348606759295499 Time: 0.010644
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[12/28/2021-10:07:44] [V] [TRT] Tactic: -5980889159865208399 Time: 0.014044
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[12/28/2021-10:07:44] [V] [TRT] Tactic: -5766140806760372989 Time: 0.011072
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -5170003087447722174
[12/28/2021-10:07:44] [V] [TRT] Tactic: -5170003087447722174 Time: 0.006348
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -4849712423393454704
[12/28/2021-10:07:44] [V] [TRT] Tactic: -4849712423393454704 Time: 0.00744
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[12/28/2021-10:07:44] [V] [TRT] Tactic: -4516822589357530549 Time: 0.011216
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -3613322253849278738
[12/28/2021-10:07:44] [V] [TRT] Tactic: -3613322253849278738 Time: 0.005996
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -3577322188448771475
[12/28/2021-10:07:44] [V] [TRT] Tactic: -3577322188448771475 Time: 0.0077
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: -2754311112012636251
[12/28/2021-10:07:44] [V] [TRT] Tactic: -2754311112012636251 Time: 0.007776
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: -2315453944962430928
[12/28/2021-10:07:44] [V] [TRT] Tactic: -2315453944962430928 Time: 0.012496
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:44] [V] [TRT] Tactic: -2083778562631872334 Time: 0.007984
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -1499578657823798783
[12/28/2021-10:07:44] [V] [TRT] Tactic: -1499578657823798783 Time: 0.006632
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:44] [V] [TRT] Tactic: -1498626619443284096 Time: 0.00858
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:44] [V] [TRT] Tactic: -1283580231568512025 Time: 0.006404
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:44] [V] [TRT] Tactic: -1173968681844185579 Time: 0.00646
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:44] [V] [TRT] Tactic: -762222380308749469 Time: 0.006956
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -713022856474991236
[12/28/2021-10:07:44] [V] [TRT] Tactic: -713022856474991236 Time: 0.005904
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:44] [V] [TRT] Tactic: -556794153877490941 Time: 0.006904
[12/28/2021-10:07:44] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: -375949437730908730
[12/28/2021-10:07:44] [V] [TRT] Tactic: -375949437730908730 Time: 0.007584
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: -713022856474991236 Time: 0.005904
[12/28/2021-10:07:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -713022856474991236
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.005544
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004732
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004732
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.006528
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004716
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004716
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010516
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004652
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004652
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010416
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005108
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005108
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.005528
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004592
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004592
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.006556
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004752
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004752
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010432
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010336
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005072
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005072
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.009956
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004712
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004712
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.0065
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004768
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004768
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011544
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005076
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005076
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011644
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005064
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005064
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.0129
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004792
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004792
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.006556
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004748
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004748
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.0065
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004836
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004836
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011312
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005092
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005092
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011084
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005088
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005088
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010956
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004556
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004556
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.008144
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005092
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005092
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.008084
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.008436
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.00452
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.00452
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.010496
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005064
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005064
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.008132
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.00506
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.00506
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.00822
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005076
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005076
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.00838
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004588
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1 Time: 0.00454
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 1 Time: 0.00454
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.012516
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004704
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004704
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.01258
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004748
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004748
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.00994
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.004768
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.004768
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011404
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.00514
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1002 Time: 0.011176
[12/28/2021-10:07:44] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.005036
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 0 Time: 0.005036
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1), Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudaDepthwiseConvolution)
[12/28/2021-10:07:44] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (FusedConvActConvolution)
[12/28/2021-10:07:44] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudnnConvolution)
[12/28/2021-10:07:44] [V] [TRT] Tactic: 0 Time: 0.640648
[12/28/2021-10:07:44] [V] [TRT] Tactic: 1 Time: 0.20566
[12/28/2021-10:07:44] [V] [TRT] Tactic: 2 Time: 0.293784
[12/28/2021-10:07:44] [V] [TRT] Tactic: 4 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 5 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 6 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 56 Time: 0.64048
[12/28/2021-10:07:44] [V] [TRT] Tactic: 57 Time: 0.2286
[12/28/2021-10:07:44] [V] [TRT] Tactic: 58 Time: 0.294332
[12/28/2021-10:07:44] [V] [TRT] Tactic: 60 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 61 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 62 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 112 Time: 0.640912
[12/28/2021-10:07:44] [V] [TRT] Tactic: 113 Time: 0.879372
[12/28/2021-10:07:44] [V] [TRT] Tactic: 114 Time: 0.293884
[12/28/2021-10:07:44] [V] [TRT] Tactic: 116 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 117 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Tactic: 118 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:44] [V] [TRT] Fastest Tactic: 1 Time: 0.20566
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:44] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] Setting workspace to 26216448enables more tactics for profiling
[12/28/2021-10:07:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 1
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512), Float(512,1,512,512) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudnnConvolution)
[12/28/2021-10:07:44] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:44] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128), Float(128,1:4,128,128) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudnnConvolution)
[12/28/2021-10:07:44] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:44] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:44] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:44] [V] [TRT] Tactic: 7342025736444949634 Time: 0.37938
[12/28/2021-10:07:44] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:44] [V] [TRT] Tactic: -7377458734869418330 Time: 0.373368
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:45] [V] [TRT] Tactic: -5457304872213719461 Time: 0.376496
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: -7377458734869418330 Time: 0.373368
[12/28/2021-10:07:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7377458734869418330
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudaDepthwiseConvolution)
[12/28/2021-10:07:45] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:45] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Int8(128,1:4,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudaDepthwiseConvolution)
[12/28/2021-10:07:45] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (FusedConvActConvolution)
[12/28/2021-10:07:45] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:45] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Int8(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:45] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1), Float(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1011019097971850911 Time: 0.052572
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1071114551801767124 Time: 0.029224
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:45] [V] [TRT] Tactic: 2623576043214044314 Time: 0.01788
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:45] [V] [TRT] Tactic: 3281631721811475881 Time: 0.022304
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:45] [V] [TRT] Tactic: 4551754795416974366 Time: 0.020024
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:45] [V] [TRT] Tactic: 4925112190271421402 Time: 0.016728
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:45] [V] [TRT] Tactic: 5041593333398049019 Time: 0.01682
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:45] [V] [TRT] Tactic: 5166018662410176512 Time: 0.096596
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:45] [V] [TRT] Tactic: 6191867932654611882 Time: 0.052216
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:45] [V] [TRT] Tactic: 6852868042694587230 Time: 0.020632
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:45] [V] [TRT] Tactic: 8399092794516815300 Time: 0.097112
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:45] [V] [TRT] Tactic: -9132922677633967263 Time: 0.029948
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:45] [V] [TRT] Tactic: -7413564913826321357 Time: 0.053944
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:45] [V] [TRT] Tactic: -5942379529065248478 Time: 0.0294
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:45] [V] [TRT] Tactic: -5334776871777565833 Time: 0.0974
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:45] [V] [TRT] Tactic: -5157868397078537095 Time: 0.052364
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:45] [V] [TRT] Tactic: -5100834417027499764 Time: 0.018788
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:45] [V] [TRT] Tactic: -3365360067423513506 Time: 0.014748
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:45] [V] [TRT] Tactic: -1782593837177056527 Time: 0.029696
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:45] [V] [TRT] Tactic: -1573035963956198975 Time: 0.096432
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.014748
[12/28/2021-10:07:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1), Int8(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudaGroupConvolution)
[12/28/2021-10:07:45] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CudaDepthwiseConvolution)
[12/28/2021-10:07:45] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (FusedConvActConvolution)
[12/28/2021-10:07:45] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Conv_101 + Add_105 + Relu_108 (CaskConvolution)
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:45] [V] [TRT] Tactic: 2325023763229477890 Time: 0.05286
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:45] [V] [TRT] Tactic: 3401614690060226673 Time: 0.018696
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:45] [V] [TRT] Tactic: 4042202769383439184 Time: 0.029224
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:45] [V] [TRT] Tactic: 4734519122557206480 Time: 0.09632
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:45] [V] [TRT] Tactic: 5136656982162849059 Time: 0.014576
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:45] [V] [TRT] Tactic: 6004789655466615912 Time: 0.031916
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:45] [V] [TRT] Tactic: 6146901278630392829 Time: 0.09596
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:45] [V] [TRT] Tactic: 6781129591847482048 Time: 0.02944
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:45] [V] [TRT] Tactic: 8096257414008860171 Time: 0.02922
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:45] [V] [TRT] Tactic: -9165697322068360861 Time: 0.097244
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:45] [V] [TRT] Tactic: -8263994888336646547 Time: 0.051772
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:45] [V] [TRT] Tactic: -8205948405243401049 Time: 0.01684
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:45] [V] [TRT] Tactic: -7683887278997527517 Time: 0.019988
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:45] [V] [TRT] Tactic: -4933563390723451692 Time: 0.022016
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:45] [V] [TRT] Tactic: -3238475748440751107 Time: 0.028744
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:45] [V] [TRT] Tactic: -3182884991006484042 Time: 0.052476
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:45] [V] [TRT] Tactic: -3173468756112541306 Time: 0.016656
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:45] [V] [TRT] Tactic: -1546787387293556842 Time: 0.051556
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:45] [V] [TRT] Tactic: -1498626619443284096 Time: 0.03426
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:45] [V] [TRT] Tactic: -1283580231568512025 Time: 0.016036
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:45] [V] [TRT] Tactic: -762222380308749469 Time: 0.020452
[12/28/2021-10:07:45] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:45] [V] [TRT] Tactic: -516725800067794372 Time: 0.09618
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 5136656982162849059 Time: 0.014576
[12/28/2021-10:07:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5136656982162849059
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.005568
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004648
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004648
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.00658
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004792
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004792
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.012604
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.00476
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.00476
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.01064
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004624
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004624
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.0104
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.00524
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.00524
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.005704
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.00474
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.00474
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.006664
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004932
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004932
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.01272
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005112
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005112
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.01074
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005188
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005188
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.01046
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005212
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005212
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.010096
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004888
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004888
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.006628
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004788
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004788
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.009952
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004776
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004776
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.01158
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005064
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005064
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.011592
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005164
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005164
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.012744
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004788
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004788
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.006572
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.00482
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.00482
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.006512
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.004724
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.004724
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 1002 Time: 0.011264
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:45] [V] [TRT] Tactic: 0 Time: 0.005112
[12/28/2021-10:07:45] [V] [TRT] Fastest Tactic: 0 Time: 0.005112
[12/28/2021-10:07:45] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:45] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.011064
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.005068
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.005068
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.010868
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.00458
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.00458
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.008208
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.005216
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.005216
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.008256
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.005104
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.005104
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.0114
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.00516
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.00516
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.008512
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.004512
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.004512
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.010516
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.005072
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.005072
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.00806
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.00504
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.00504
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.008204
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.005116
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.005116
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.011072
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.00508
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.00508
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1002 Time: 0.00842
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.004492
[12/28/2021-10:07:46] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1 Time: 0.00452
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 0 Time: 0.004492
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudaDepthwiseConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (FusedConvActConvolution)
[12/28/2021-10:07:46] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudnnConvolution)
[12/28/2021-10:07:46] [V] [TRT] Tactic: 0 Time: 0.63834
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1 Time: 0.222936
[12/28/2021-10:07:46] [V] [TRT] Tactic: 2 Time: 0.291012
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 56 Time: 0.638456
[12/28/2021-10:07:46] [V] [TRT] Tactic: 57 Time: 0.239644
[12/28/2021-10:07:46] [V] [TRT] Tactic: 58 Time: 0.291204
[12/28/2021-10:07:46] [V] [TRT] Tactic: 60 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 61 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 62 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 112 Time: 0.638172
[12/28/2021-10:07:46] [V] [TRT] Tactic: 113 Time: 0.796488
[12/28/2021-10:07:46] [V] [TRT] Tactic: 114 Time: 0.291152
[12/28/2021-10:07:46] [V] [TRT] Tactic: 116 skipped. Scratch requested: 651165696, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 117 skipped. Scratch requested: 1283457024, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Tactic: 118 skipped. Scratch requested: 26216448, available: 16777216
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: 1 Time: 0.222936
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] Setting workspace to 26216448enables more tactics for profiling
[12/28/2021-10:07:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 1
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudnnConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudnnConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[12/28/2021-10:07:46] [V] [TRT] Tactic: 7342025736444949634 Time: 0.378312
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[12/28/2021-10:07:46] [V] [TRT] Tactic: -7377458734869418330 Time: 0.37226
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[12/28/2021-10:07:46] [V] [TRT] Tactic: -5457304872213719461 Time: 0.37544
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: -7377458734869418330 Time: 0.37226
[12/28/2021-10:07:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7377458734869418330
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudaDepthwiseConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudaDepthwiseConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (FusedConvActConvolution)
[12/28/2021-10:07:46] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1011019097971850911 Time: 0.05144
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1071114551801767124 Time: 0.028532
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[12/28/2021-10:07:46] [V] [TRT] Tactic: 2623576043214044314 Time: 0.017596
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[12/28/2021-10:07:46] [V] [TRT] Tactic: 3281631721811475881 Time: 0.0219
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4551754795416974366 Time: 0.019788
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4925112190271421402 Time: 0.016416
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5041593333398049019 Time: 0.016388
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5166018662410176512 Time: 0.095052
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6191867932654611882 Time: 0.051592
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6852868042694587230 Time: 0.020296
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[12/28/2021-10:07:46] [V] [TRT] Tactic: 8399092794516815300 Time: 0.096524
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[12/28/2021-10:07:46] [V] [TRT] Tactic: -9132922677633967263 Time: 0.029184
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[12/28/2021-10:07:46] [V] [TRT] Tactic: -7413564913826321357 Time: 0.052876
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[12/28/2021-10:07:46] [V] [TRT] Tactic: -5942379529065248478 Time: 0.02864
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[12/28/2021-10:07:46] [V] [TRT] Tactic: -5334776871777565833 Time: 0.095844
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[12/28/2021-10:07:46] [V] [TRT] Tactic: -5157868397078537095 Time: 0.051436
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[12/28/2021-10:07:46] [V] [TRT] Tactic: -5100834417027499764 Time: 0.018444
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[12/28/2021-10:07:46] [V] [TRT] Tactic: -3365360067423513506 Time: 0.014452
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[12/28/2021-10:07:46] [V] [TRT] Tactic: -1782593837177056527 Time: 0.02898
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[12/28/2021-10:07:46] [V] [TRT] Tactic: -1573035963956198975 Time: 0.095872
[12/28/2021-10:07:46] [V] [TRT] Fastest Tactic: -3365360067423513506 Time: 0.014452
[12/28/2021-10:07:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3365360067423513506
[12/28/2021-10:07:46] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudaGroupConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CudaDepthwiseConvolution)
[12/28/2021-10:07:46] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (FusedConvActConvolution)
[12/28/2021-10:07:46] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:46] [V] [TRT] --------------- Timing Runner: Conv_111 + Relu_114 (CaskConvolution)
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[12/28/2021-10:07:46] [V] [TRT] Tactic: 177040020707947851 Time: 0.019524
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1550399266192842845 Time: 0.016552
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[12/28/2021-10:07:46] [V] [TRT] Tactic: 1572887561103143487 Time: 0.031148
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[12/28/2021-10:07:46] [V] [TRT] Tactic: 2325023763229477890 Time: 0.051992
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:46] [V] [TRT] Tactic: 3284282970967328046 Time: 0.014316
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:46] [V] [TRT] Tactic: 3401614690060226673 Time: 0.018408
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[12/28/2021-10:07:46] [V] [TRT] Tactic: 3512426920013359699 Time: 0.021736
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4042202769383439184 Time: 0.028768
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4259547356717612415 Time: 0.032768
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[12/28/2021-10:07:46] [V] [TRT] Tactic: 4734519122557206480 Time: 0.095248
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5121596860264626879 Time: 0.095296
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5136656982162849059 Time: 0.014408
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5158259316594207439 Time: 0.028804
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[12/28/2021-10:07:46] [V] [TRT] Tactic: 5966973378912044513 Time: 0.050888
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6004789655466615912 Time: 0.031096
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6146901278630392829 Time: 0.095016
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6434020722187266170 Time: 0.09598
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[12/28/2021-10:07:46] [V] [TRT] Tactic: 6781129591847482048 Time: 0.028984
[12/28/2021-10:07:46] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[12/28/2021-10:07:46] [V] [TRT] Tactic: 7191893591576074000 Time: 0.0165
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[12/28/2021-10:07:47] [V] [TRT] Tactic: 7438984192263206338 Time: 0.028248
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[12/28/2021-10:07:47] [V] [TRT] Tactic: 7504901284678552178 Time: 0.051004
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[12/28/2021-10:07:47] [V] [TRT] Tactic: 8096257414008860171 Time: 0.028724
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[12/28/2021-10:07:47] [V] [TRT] Tactic: 9143438935315839085 Time: 0.018852
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[12/28/2021-10:07:47] [V] [TRT] Tactic: -9165697322068360861 Time: 0.095936
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[12/28/2021-10:07:47] [V] [TRT] Tactic: -8263994888336646547 Time: 0.05096
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[12/28/2021-10:07:47] [V] [TRT] Tactic: -8205948405243401049 Time: 0.016472
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[12/28/2021-10:07:47] [V] [TRT] Tactic: -7992068592656168418 Time: 0.02874
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[12/28/2021-10:07:47] [V] [TRT] Tactic: -7842775553137511386 Time: 0.051952
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[12/28/2021-10:07:47] [V] [TRT] Tactic: -7683887278997527517 Time: 0.019644
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[12/28/2021-10:07:47] [V] [TRT] Tactic: -5709079507616090666 Time: 0.050984
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[12/28/2021-10:07:47] [V] [TRT] Tactic: -5698636014239116282 Time: 0.094896
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[12/28/2021-10:07:47] [V] [TRT] Tactic: -4933563390723451692 Time: 0.021796
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[12/28/2021-10:07:47] [V] [TRT] Tactic: -3413217501222406256 Time: 0.095124
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[12/28/2021-10:07:47] [V] [TRT] Tactic: -3238475748440751107 Time: 0.028252
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[12/28/2021-10:07:47] [V] [TRT] Tactic: -3182884991006484042 Time: 0.05152
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:47] [V] [TRT] Tactic: -3173468756112541306 Time: 0.016468
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[12/28/2021-10:07:47] [V] [TRT] Tactic: -2083778562631872334 Time: 0.029044
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1546787387293556842 Time: 0.05114
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1498626619443284096 Time: 0.033376
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1283580231568512025 Time: 0.015864
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1173968681844185579 Time: 0.01552
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[12/28/2021-10:07:47] [V] [TRT] Tactic: -762222380308749469 Time: 0.020112
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[12/28/2021-10:07:47] [V] [TRT] Tactic: -556794153877490941 Time: 0.020056
[12/28/2021-10:07:47] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[12/28/2021-10:07:47] [V] [TRT] Tactic: -516725800067794372 Time: 0.095024
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 3284282970967328046 Time: 0.014316
[12/28/2021-10:07:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3284282970967328046
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1), Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512), Float(512,1,512,512) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CudnnConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128), Float(128,1:4,128,128) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CudaDepthwiseConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Int8(128,1:4,1,1) -> Int8(128,1:4,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CudaDepthwiseConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (FusedConvActConvolution)
[12/28/2021-10:07:47] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Int8(128,1:4,1,1), Int8(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Conv_117 + Add_118 + Relu_121 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1), Float(16,1:32,1,1) -> Float(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Int8(16,1:32,1,1), Int8(16,1:32,1,1) -> Int8(16,1:32,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(128,1:4,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Int8(16,1:32,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_122 (TiledPooling)
[12/28/2021-10:07:47] [V] [TRT] Tactic: 7209217 Time: 0.015916
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 7209217 Time: 0.015916
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_122 (CudnnPooling)
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1 Time: 0.005288
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: -1 Time: 0.005288
[12/28/2021-10:07:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: -1
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(512,1,1,1) -> Float(10,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CudaDepthwiseConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (FusedConvActConvolution)
[12/28/2021-10:07:47] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CudnnConvolution)
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 0 Time: 0.07022
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1 Time: 0.096324
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 2 Time: 0.099644
[12/28/2021-10:07:47] [V] [TRT] Tactic: 4 skipped. Scratch requested: 49565696, available: 16777216
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 5 Time: 0.055448
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 56 Time: 0.070224
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 57 Time: 0.11336
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 58 Time: 0.099764
[12/28/2021-10:07:47] [V] [TRT] Tactic: 60 skipped. Scratch requested: 49565696, available: 16777216
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 61 Time: 0.055172
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 112 Time: 0.070412
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 113 Time: 0.264752
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 114 Time: 0.099788
[12/28/2021-10:07:47] [V] [TRT] Tactic: 116 skipped. Scratch requested: 49565696, available: 16777216
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 117 Time: 0.055364
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 61 Time: 0.055172
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CublasConvolution)
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 0 Time: 0.01246
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1 Time: 0.016572
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 2 Time: 0.013736
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 3 Time: 0.014268
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 0 Time: 0.01246
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 4549827808004681195 Time: 0.056936
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 5779835512569528575 Time: 0.072444
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 9151672657204310840
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 9151672657204310840 Time: 0.066224
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -7491730084094677098 Time: 0.064404
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: -6622064180404051845
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -6622064180404051845 Time: 0.061884
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -6313876406580483184 Time: 0.065328
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -6273689210331812572 Time: 0.071356
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -4337126844824617177 Time: 0.052804
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1123676555321336786 Time: 0.07134
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -701551393537224327 Time: 0.06188
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: -4337126844824617177 Time: 0.052804
[12/28/2021-10:07:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(512,1,512,512) -> Float(10,1,10,10) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CudnnConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CublasConvolution)
[12/28/2021-10:07:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(128,1:4,128,128) -> Float(3,1:4,3,3) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CudnnConvolution)
[12/28/2021-10:07:47] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CublasConvolution)
[12/28/2021-10:07:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Gemm_126 (CaskConvolution)
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 1373022415249282411
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1373022415249282411 Time: 0.047628
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1663866669559596164 Time: 0.0697
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 2860655430572478466 Time: 0.043816
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 4474630279712975759 Time: 0.026956
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 4479823862704990365 Time: 0.026724
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 4696204239951173149 Time: 0.043208
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 5778138195697110003 Time: 0.070432
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: 8918020581761223752 Time: 0.066804
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: -7067026478815706014
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -7067026478815706014 Time: 0.0473
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -5905193483742532701 Time: 0.03926
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -4035591156787122265 Time: 0.026156
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -2809379259463049391 Time: 0.069712
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -1985235291706575900 Time: 0.067284
[12/28/2021-10:07:47] [V] [TRT] Gemm_126 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:47] [V] [TRT] Tactic: -504296718212024303 Time: 0.068204
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: -4035591156787122265 Time: 0.026156
[12/28/2021-10:07:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4035591156787122265
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(10,1,10,10) -> Float(10,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1002 Time: 0.00608
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:47] [V] [TRT] Tactic: 0 Time: 0.004484
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 0 Time: 0.004484
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning Reformat:Float(3,1:4,3,3) -> Float(10,1,1,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:47] [V] [TRT] Tactic: 1002 Time: 0.006216
[12/28/2021-10:07:47] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[12/28/2021-10:07:47] [V] [TRT] Tactic: 0 Time: 0.00466
[12/28/2021-10:07:47] [V] [TRT] Fastest Tactic: 0 Time: 0.00466
[12/28/2021-10:07:47] [V] [TRT] *************** Autotuning format combination: Float(10,1,1,1) -> Float(10,1) ***************
[12/28/2021-10:07:47] [V] [TRT] --------------- Timing Runner: (Unnamed Layer* 50) [Shuffle] (Shuffle)
[12/28/2021-10:07:48] [V] [TRT] Tactic: 0 Time: 0.003968
[12/28/2021-10:07:48] [V] [TRT] Tactic: 1 Time: 0.008688
[12/28/2021-10:07:48] [V] [TRT] Fastest Tactic: 0 Time: 0.003968
[12/28/2021-10:07:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0
[12/28/2021-10:07:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to Conv_2 + Relu_5 + MaxPool_8 (actual_input_1) from Float(3072,1024,32,1) to Int8(3072,1024,32,1)
[12/28/2021-10:07:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to GlobalAveragePool_122 (264) from Int8(16,1:32,1,1) to Float(512,1,1,1)
[12/28/2021-10:07:48] [V] [TRT] Formats and tactics selection completed in 23.8981 seconds.
[12/28/2021-10:07:48] [V] [TRT] After reformat layers: 25 layers
[12/28/2021-10:07:48] [V] [TRT] Block size 16777216
[12/28/2021-10:07:48] [V] [TRT] Block size 131072
[12/28/2021-10:07:48] [V] [TRT] Block size 131072
[12/28/2021-10:07:48] [V] [TRT] Block size 131072
[12/28/2021-10:07:48] [V] [TRT] Total Activation Memory: 17170432
[12/28/2021-10:07:48] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[12/28/2021-10:07:48] [V] [TRT] Conv_11 + Relu_14 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:48] [V] [TRT] Conv_17 + Add_18 + Relu_21 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:48] [V] [TRT] Conv_24 + Relu_27 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[12/28/2021-10:07:48] [V] [TRT] Conv_30 + Add_31 + Relu_34 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[12/28/2021-10:07:48] [V] [TRT] Conv_37 + Relu_40 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:48] [V] [TRT] Conv_46 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 6441948709525127755
[12/28/2021-10:07:48] [V] [TRT] Conv_43 + Add_47 + Relu_50 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_53 + Relu_56 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:48] [V] [TRT] Conv_59 + Add_60 + Relu_63 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_66 + Relu_69 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_75 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -713022856474991236
[12/28/2021-10:07:48] [V] [TRT] Conv_72 + Add_76 + Relu_79 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_82 + Relu_85 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:48] [V] [TRT] Conv_88 + Add_89 + Relu_92 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_95 + Relu_98 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:48] [V] [TRT] Conv_104 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_epifadd Tactic: -713022856474991236
[12/28/2021-10:07:48] [V] [TRT] Conv_101 + Add_105 + Relu_108 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Conv_111 + Relu_114 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[12/28/2021-10:07:48] [V] [TRT] Conv_117 + Add_118 + Relu_121 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[12/28/2021-10:07:48] [V] [TRT] Setting a default quantization params because quantization data is missing for Gemm_126
[12/28/2021-10:07:48] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to Conv_2 + Relu_5 + MaxPool_8 HostPersistent: 0 DevicePersistent: 0
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_2 + Relu_5 + MaxPool_8 HostPersistent: 0 DevicePersistent: 0
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_11 + Relu_14 HostPersistent: 2976 DevicePersistent: 37888
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_17 + Add_18 + Relu_21 HostPersistent: 2976 DevicePersistent: 37888
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_24 + Relu_27 HostPersistent: 2976 DevicePersistent: 37888
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_30 + Add_31 + Relu_34 HostPersistent: 2976 DevicePersistent: 37888
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_37 + Relu_40 HostPersistent: 2976 DevicePersistent: 75264
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_46 HostPersistent: 2976 DevicePersistent: 9728
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_43 + Add_47 + Relu_50 HostPersistent: 2976 DevicePersistent: 148992
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_53 + Relu_56 HostPersistent: 2976 DevicePersistent: 148992
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_59 + Add_60 + Relu_63 HostPersistent: 2976 DevicePersistent: 148992
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_66 + Relu_69 HostPersistent: 2976 DevicePersistent: 297984
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_75 HostPersistent: 2976 DevicePersistent: 35840
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_72 + Add_76 + Relu_79 HostPersistent: 2976 DevicePersistent: 592896
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_82 + Relu_85 HostPersistent: 2976 DevicePersistent: 592896
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_88 + Add_89 + Relu_92 HostPersistent: 2976 DevicePersistent: 592896
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_95 + Relu_98 HostPersistent: 2976 DevicePersistent: 1185792
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_104 HostPersistent: 2976 DevicePersistent: 137216
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_101 + Add_105 + Relu_108 HostPersistent: 2976 DevicePersistent: 2365440
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_111 + Relu_114 HostPersistent: 2976 DevicePersistent: 2365440
[12/28/2021-10:07:48] [V] [TRT] Layer: Conv_117 + Add_118 + Relu_121 HostPersistent: 2976 DevicePersistent: 2365440
[12/28/2021-10:07:48] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to GlobalAveragePool_122 HostPersistent: 0 DevicePersistent: 0
[12/28/2021-10:07:48] [V] [TRT] Layer: GlobalAveragePool_122 HostPersistent: 48 DevicePersistent: 0
[12/28/2021-10:07:48] [V] [TRT] Layer: Gemm_126 HostPersistent: 340 DevicePersistent: 0
[12/28/2021-10:07:48] [I] [TRT] Total Host Persistent Memory: 56944
[12/28/2021-10:07:48] [I] [TRT] Total Device Persistent Memory: 11215360
[12/28/2021-10:07:48] [I] [TRT] Total Scratch Memory: 6144
[12/28/2021-10:07:48] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 13 MiB, GPU 4 MiB
[12/28/2021-10:07:48] [V] [TRT] Using cublasLt a tactic source
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2078, GPU 4033 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Using cuDNN as a tactic source
[12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2079, GPU 4041 (MiB)
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2078, GPU 4025 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Engine generation completed in 24.9934 seconds.
[12/28/2021-10:07:48] [V] [TRT] Deleting timing cache: 346 entries, 739 hits
[12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2078, GPU 4007 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Engine Layer Information:
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_2 + Relu_5 + MaxPool_8, Tactic: 0, actual_input_1[Float(32,3,32,32)] -> Reformatted Input Tensor 0 to Conv_2 + Relu_5 + MaxPool_8[Int8(32,3,32,32)]
Layer(ConvActPool): Conv_2 + Relu_5 + MaxPool_8, Tactic: 1131, Reformatted Input Tensor 0 to Conv_2 + Relu_5 + MaxPool_8[Int8(32,3,32,32)] -> 132[Int8(32,64,8,8)]
Layer(CaskConvolution): Conv_11 + Relu_14, Tactic: 3401614690060226673, 132[Int8(32,64,8,8)] -> 139[Int8(32,64,8,8)]
Layer(CaskConvolution): Conv_17 + Add_18 + Relu_21, Tactic: -3173468756112541306, 139[Int8(32,64,8,8)], 132[Int8(32,64,8,8)] -> 147[Int8(32,64,8,8)]
Layer(CaskConvolution): Conv_24 + Relu_27, Tactic: 3401614690060226673, 147[Int8(32,64,8,8)] -> 154[Int8(32,64,8,8)]
Layer(CaskConvolution): Conv_30 + Add_31 + Relu_34, Tactic: -3173468756112541306, 154[Int8(32,64,8,8)], 147[Int8(32,64,8,8)] -> 162[Int8(32,64,8,8)]
Layer(CaskConvolution): Conv_37 + Relu_40, Tactic: 3284282970967328046, 162[Int8(32,64,8,8)] -> 169[Int8(32,128,4,4)]
Layer(CaskConvolution): Conv_46, Tactic: 6441948709525127755, 162[Int8(32,64,8,8)] -> 291[Int8(32,128,4,4)]
Layer(CaskConvolution): Conv_43 + Add_47 + Relu_50, Tactic: 5136656982162849059, 169[Int8(32,128,4,4)], 291[Int8(32,128,4,4)] -> 181[Int8(32,128,4,4)]
Layer(CaskConvolution): Conv_53 + Relu_56, Tactic: 3284282970967328046, 181[Int8(32,128,4,4)] -> 188[Int8(32,128,4,4)]
Layer(CaskConvolution): Conv_59 + Add_60 + Relu_63, Tactic: 5136656982162849059, 188[Int8(32,128,4,4)], 181[Int8(32,128,4,4)] -> 196[Int8(32,128,4,4)]
Layer(CaskConvolution): Conv_66 + Relu_69, Tactic: 5136656982162849059, 196[Int8(32,128,4,4)] -> 203[Int8(32,256,2,2)]
Layer(CaskConvolution): Conv_75, Tactic: -713022856474991236, 196[Int8(32,128,4,4)] -> 306[Int8(32,256,2,2)]
Layer(CaskConvolution): Conv_72 + Add_76 + Relu_79, Tactic: 5136656982162849059, 203[Int8(32,256,2,2)], 306[Int8(32,256,2,2)] -> 215[Int8(32,256,2,2)]
Layer(CaskConvolution): Conv_82 + Relu_85, Tactic: 3284282970967328046, 215[Int8(32,256,2,2)] -> 222[Int8(32,256,2,2)]
Layer(CaskConvolution): Conv_88 + Add_89 + Relu_92, Tactic: 5136656982162849059, 222[Int8(32,256,2,2)], 215[Int8(32,256,2,2)] -> 230[Int8(32,256,2,2)]
Layer(CaskConvolution): Conv_95 + Relu_98, Tactic: 3284282970967328046, 230[Int8(32,256,2,2)] -> 237[Int8(32,512,1,1)]
Layer(CaskConvolution): Conv_104, Tactic: -713022856474991236, 230[Int8(32,256,2,2)] -> 321[Int8(32,512,1,1)]
Layer(CaskConvolution): Conv_101 + Add_105 + Relu_108, Tactic: 5136656982162849059, 237[Int8(32,512,1,1)], 321[Int8(32,512,1,1)] -> 249[Int8(32,512,1,1)]
Layer(CaskConvolution): Conv_111 + Relu_114, Tactic: 3284282970967328046, 249[Int8(32,512,1,1)] -> 256[Int8(32,512,1,1)]
Layer(CaskConvolution): Conv_117 + Add_118 + Relu_121, Tactic: 5136656982162849059, 256[Int8(32,512,1,1)], 249[Int8(32,512,1,1)] -> 264[Int8(32,512,1,1)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to GlobalAveragePool_122, Tactic: 0, 264[Int8(32,512,1,1)] -> Reformatted Input Tensor 0 to GlobalAveragePool_122[Float(32,512,1,1)]
Layer(CudnnPooling): GlobalAveragePool_122, Tactic: -1, Reformatted Input Tensor 0 to GlobalAveragePool_122[Float(32,512,1,1)] -> 265[Float(32,512,1,1)]
Layer(CublasConvolution): Gemm_126, Tactic: 0, 265[Float(32,512,1,1)] -> (Unnamed Layer* 49) [Fully Connected]_output[Float(32,10,1,1)]
[12/28/2021-10:07:48] [I] [TRT] [MemUsageSnapshot] Builder end: CPU 2078 MiB, GPU 4007 MiB
[12/28/2021-10:07:48] [I] [TRT] Loaded engine size: 10 MB
[12/28/2021-10:07:48] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2087 MiB, GPU 3995 MiB
[12/28/2021-10:07:48] [V] [TRT] Using cublasLt a tactic source
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2087, GPU 4017 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Using cuDNN as a tactic source
[12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2087, GPU 4025 (MiB)
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2087, GPU 4007 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Deserialization required 31115 microseconds.
[12/28/2021-10:07:48] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine end: CPU 2087 MiB, GPU 4007 MiB
[12/28/2021-10:07:48] [I] Engine built in 25.7721 sec.
[12/28/2021-10:07:48] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation begin: CPU 2044 MiB, GPU 4007 MiB
[12/28/2021-10:07:48] [V] [TRT] Using cublasLt a tactic source
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2044, GPU 4017 (MiB)
[12/28/2021-10:07:48] [V] [TRT] Using cuDNN as a tactic source
[12/28/2021-10:07:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2044, GPU 4025 (MiB)
[12/28/2021-10:07:48] [12/28/2021-10:07:48] [V] [TRT] Total per-runner device memory is 11215360
[12/28/2021-10:07:48] [V] [TRT] Total per-runner host memory is 56944
[12/28/2021-10:07:48] [V] [TRT] Allocated activation device memory of size 393216
[12/28/2021-10:07:48] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation end: CPU 2044 MiB, GPU 4037 MiB
[12/28/2021-10:07:48] [I] Created input binding for actual_input_1 with dimensions 32x3x32x32
[12/28/2021-10:07:48] [I] Created output binding for output1 with dimensions 32x10
[12/28/2021-10:07:48] [I] Starting inference
[12/28/2021-10:07:51] [I] Warmup completed 999 queries over 200 ms
[12/28/2021-10:07:51] [I] Timing trace has 17424 queries over 3.00056 s
[12/28/2021-10:07:51] [I] 
[12/28/2021-10:07:51] [I] === Trace details ===
[12/28/2021-10:07:51] [I] Trace averages of 10 runs:
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155095 ms - Host latency: 0.196249 ms (end to end 0.279852 ms, enqueue 0.0865402 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155247 ms - Host latency: 0.197131 ms (end to end 0.279863 ms, enqueue 0.0858688 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154868 ms - Host latency: 0.196663 ms (end to end 0.277112 ms, enqueue 0.0820984 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154863 ms - Host latency: 0.196158 ms (end to end 0.279761 ms, enqueue 0.0780472 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154901 ms - Host latency: 0.195885 ms (end to end 0.278635 ms, enqueue 0.077829 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154701 ms - Host latency: 0.196088 ms (end to end 0.259721 ms, enqueue 0.0827667 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155037 ms - Host latency: 0.196519 ms (end to end 0.275258 ms, enqueue 0.0960861 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154778 ms - Host latency: 0.196222 ms (end to end 0.280901 ms, enqueue 0.0958664 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154593 ms - Host latency: 0.195648 ms (end to end 0.278389 ms, enqueue 0.0967438 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154765 ms - Host latency: 0.196149 ms (end to end 0.279517 ms, enqueue 0.096167 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.196016 ms (end to end 0.278427 ms, enqueue 0.0998611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155135 ms - Host latency: 0.196922 ms (end to end 0.284839 ms, enqueue 0.103108 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155339 ms - Host latency: 0.196892 ms (end to end 0.284015 ms, enqueue 0.100717 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155334 ms - Host latency: 0.196706 ms (end to end 0.281253 ms, enqueue 0.091243 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154941 ms - Host latency: 0.196419 ms (end to end 0.279382 ms, enqueue 0.0895462 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154543 ms - Host latency: 0.195753 ms (end to end 0.27747 ms, enqueue 0.0899887 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154666 ms - Host latency: 0.195654 ms (end to end 0.278773 ms, enqueue 0.0897888 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154453 ms - Host latency: 0.195444 ms (end to end 0.277281 ms, enqueue 0.0906876 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154861 ms - Host latency: 0.195641 ms (end to end 0.278831 ms, enqueue 0.0911392 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154459 ms - Host latency: 0.195799 ms (end to end 0.276785 ms, enqueue 0.0863754 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155302 ms - Host latency: 0.196703 ms (end to end 0.27856 ms, enqueue 0.0854599 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154356 ms - Host latency: 0.195462 ms (end to end 0.278757 ms, enqueue 0.0849854 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154861 ms - Host latency: 0.196152 ms (end to end 0.277444 ms, enqueue 0.0846619 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154671 ms - Host latency: 0.195863 ms (end to end 0.27731 ms, enqueue 0.0859146 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154262 ms - Host latency: 0.19541 ms (end to end 0.277977 ms, enqueue 0.0843979 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154625 ms - Host latency: 0.195595 ms (end to end 0.277818 ms, enqueue 0.0852203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15434 ms - Host latency: 0.195578 ms (end to end 0.278223 ms, enqueue 0.0852463 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15442 ms - Host latency: 0.195325 ms (end to end 0.278491 ms, enqueue 0.0850159 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15452 ms - Host latency: 0.195601 ms (end to end 0.279074 ms, enqueue 0.0849503 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154659 ms - Host latency: 0.195654 ms (end to end 0.276338 ms, enqueue 0.0845551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155003 ms - Host latency: 0.19623 ms (end to end 0.279611 ms, enqueue 0.0851349 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.19543 ms (end to end 0.27626 ms, enqueue 0.0853546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.195782 ms (end to end 0.278465 ms, enqueue 0.085228 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15475 ms - Host latency: 0.196028 ms (end to end 0.279979 ms, enqueue 0.0860275 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15433 ms - Host latency: 0.195288 ms (end to end 0.277963 ms, enqueue 0.0846741 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154727 ms - Host latency: 0.195966 ms (end to end 0.279861 ms, enqueue 0.0856049 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154868 ms - Host latency: 0.196332 ms (end to end 0.278897 ms, enqueue 0.0848236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154269 ms - Host latency: 0.19501 ms (end to end 0.278372 ms, enqueue 0.0844574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154358 ms - Host latency: 0.195538 ms (end to end 0.277551 ms, enqueue 0.0849182 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154633 ms - Host latency: 0.195746 ms (end to end 0.2797 ms, enqueue 0.0851837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154596 ms - Host latency: 0.1957 ms (end to end 0.278406 ms, enqueue 0.0850616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195496 ms (end to end 0.278271 ms, enqueue 0.0848267 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154532 ms - Host latency: 0.195743 ms (end to end 0.278448 ms, enqueue 0.0853088 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154172 ms - Host latency: 0.195258 ms (end to end 0.277924 ms, enqueue 0.0851013 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154193 ms - Host latency: 0.19498 ms (end to end 0.279126 ms, enqueue 0.0844635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154764 ms - Host latency: 0.196017 ms (end to end 0.278629 ms, enqueue 0.0845764 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154352 ms - Host latency: 0.195459 ms (end to end 0.278656 ms, enqueue 0.0842743 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154501 ms - Host latency: 0.195453 ms (end to end 0.279303 ms, enqueue 0.0785645 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.196106 ms (end to end 0.277756 ms, enqueue 0.0787628 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154013 ms - Host latency: 0.19516 ms (end to end 0.278976 ms, enqueue 0.0779816 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154538 ms - Host latency: 0.195679 ms (end to end 0.279022 ms, enqueue 0.0783356 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.195676 ms (end to end 0.278055 ms, enqueue 0.0792908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195477 ms (end to end 0.279117 ms, enqueue 0.0779968 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154776 ms - Host latency: 0.196021 ms (end to end 0.279678 ms, enqueue 0.0776398 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154352 ms - Host latency: 0.1953 ms (end to end 0.279462 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154556 ms - Host latency: 0.195358 ms (end to end 0.278369 ms, enqueue 0.0782837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154416 ms - Host latency: 0.195346 ms (end to end 0.27963 ms, enqueue 0.079834 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154581 ms - Host latency: 0.19574 ms (end to end 0.279303 ms, enqueue 0.0779358 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1543 ms - Host latency: 0.195468 ms (end to end 0.277344 ms, enqueue 0.0785522 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154568 ms - Host latency: 0.195566 ms (end to end 0.279782 ms, enqueue 0.0792969 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154483 ms - Host latency: 0.195679 ms (end to end 0.279797 ms, enqueue 0.0781555 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154398 ms - Host latency: 0.195499 ms (end to end 0.280466 ms, enqueue 0.0781158 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15499 ms - Host latency: 0.196329 ms (end to end 0.280594 ms, enqueue 0.0779694 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195331 ms (end to end 0.280023 ms, enqueue 0.078186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.196112 ms (end to end 0.280316 ms, enqueue 0.0786133 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154318 ms - Host latency: 0.195255 ms (end to end 0.279587 ms, enqueue 0.0773956 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154608 ms - Host latency: 0.195844 ms (end to end 0.279263 ms, enqueue 0.078833 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154605 ms - Host latency: 0.195773 ms (end to end 0.280075 ms, enqueue 0.0772552 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195285 ms (end to end 0.279517 ms, enqueue 0.0783691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154495 ms - Host latency: 0.195529 ms (end to end 0.279514 ms, enqueue 0.0783997 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154538 ms - Host latency: 0.195959 ms (end to end 0.279962 ms, enqueue 0.0771881 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154202 ms - Host latency: 0.19519 ms (end to end 0.279532 ms, enqueue 0.0782501 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.19559 ms (end to end 0.278369 ms, enqueue 0.0777618 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154611 ms - Host latency: 0.195438 ms (end to end 0.280505 ms, enqueue 0.0782501 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154715 ms - Host latency: 0.196289 ms (end to end 0.282077 ms, enqueue 0.0782593 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154984 ms - Host latency: 0.196506 ms (end to end 0.286444 ms, enqueue 0.0779633 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155212 ms - Host latency: 0.197006 ms (end to end 0.287051 ms, enqueue 0.0777313 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154327 ms - Host latency: 0.195801 ms (end to end 0.284738 ms, enqueue 0.0779999 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154739 ms - Host latency: 0.196387 ms (end to end 0.287128 ms, enqueue 0.0775146 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195789 ms (end to end 0.285974 ms, enqueue 0.0777954 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154922 ms - Host latency: 0.196323 ms (end to end 0.286148 ms, enqueue 0.0778473 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154971 ms - Host latency: 0.196732 ms (end to end 0.285724 ms, enqueue 0.0777283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154752 ms - Host latency: 0.196347 ms (end to end 0.285526 ms, enqueue 0.077948 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155087 ms - Host latency: 0.196695 ms (end to end 0.285864 ms, enqueue 0.0792816 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154568 ms - Host latency: 0.196469 ms (end to end 0.283923 ms, enqueue 0.0796509 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.196085 ms (end to end 0.28504 ms, enqueue 0.0793701 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15488 ms - Host latency: 0.196216 ms (end to end 0.284421 ms, enqueue 0.0791779 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154752 ms - Host latency: 0.196335 ms (end to end 0.28392 ms, enqueue 0.0798218 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154498 ms - Host latency: 0.195972 ms (end to end 0.284277 ms, enqueue 0.0794495 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195963 ms (end to end 0.283243 ms, enqueue 0.0798462 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15455 ms - Host latency: 0.196332 ms (end to end 0.283908 ms, enqueue 0.0788879 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.196143 ms (end to end 0.283316 ms, enqueue 0.0792816 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154779 ms - Host latency: 0.1966 ms (end to end 0.28382 ms, enqueue 0.0796967 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154825 ms - Host latency: 0.196417 ms (end to end 0.284589 ms, enqueue 0.0785339 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155038 ms - Host latency: 0.196393 ms (end to end 0.284973 ms, enqueue 0.0791595 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196356 ms (end to end 0.283411 ms, enqueue 0.0792908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155316 ms - Host latency: 0.197287 ms (end to end 0.282474 ms, enqueue 0.0788666 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196747 ms (end to end 0.285468 ms, enqueue 0.0788208 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154739 ms - Host latency: 0.196255 ms (end to end 0.285123 ms, enqueue 0.0789581 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154852 ms - Host latency: 0.196466 ms (end to end 0.284366 ms, enqueue 0.0794861 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154898 ms - Host latency: 0.19675 ms (end to end 0.284769 ms, enqueue 0.0789978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154422 ms - Host latency: 0.195731 ms (end to end 0.283322 ms, enqueue 0.079538 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154398 ms - Host latency: 0.195676 ms (end to end 0.282974 ms, enqueue 0.0792877 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154715 ms - Host latency: 0.196091 ms (end to end 0.284418 ms, enqueue 0.0793976 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154648 ms - Host latency: 0.196283 ms (end to end 0.28464 ms, enqueue 0.0793121 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.196005 ms (end to end 0.280554 ms, enqueue 0.0803833 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154489 ms - Host latency: 0.195673 ms (end to end 0.284167 ms, enqueue 0.0809021 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154285 ms - Host latency: 0.195685 ms (end to end 0.278607 ms, enqueue 0.0813538 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154703 ms - Host latency: 0.196155 ms (end to end 0.278482 ms, enqueue 0.0794128 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155377 ms - Host latency: 0.197293 ms (end to end 0.279483 ms, enqueue 0.0800629 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196396 ms (end to end 0.278726 ms, enqueue 0.0801361 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.195972 ms (end to end 0.280148 ms, enqueue 0.0783264 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154861 ms - Host latency: 0.196234 ms (end to end 0.279013 ms, enqueue 0.0798065 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154742 ms - Host latency: 0.195883 ms (end to end 0.278629 ms, enqueue 0.0782715 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195389 ms (end to end 0.278085 ms, enqueue 0.0779633 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.195947 ms (end to end 0.278333 ms, enqueue 0.0780548 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15466 ms - Host latency: 0.195764 ms (end to end 0.278937 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155051 ms - Host latency: 0.196542 ms (end to end 0.279022 ms, enqueue 0.0792358 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154694 ms - Host latency: 0.195892 ms (end to end 0.279379 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196088 ms (end to end 0.279639 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154413 ms - Host latency: 0.195795 ms (end to end 0.278137 ms, enqueue 0.0778778 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154752 ms - Host latency: 0.196115 ms (end to end 0.27803 ms, enqueue 0.0784241 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154926 ms - Host latency: 0.196301 ms (end to end 0.279181 ms, enqueue 0.0789063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195679 ms (end to end 0.27822 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.19545 ms (end to end 0.279449 ms, enqueue 0.0779846 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154462 ms - Host latency: 0.1957 ms (end to end 0.279135 ms, enqueue 0.0780884 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154343 ms - Host latency: 0.195505 ms (end to end 0.277502 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154623 ms - Host latency: 0.195502 ms (end to end 0.279202 ms, enqueue 0.0777161 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154691 ms - Host latency: 0.196277 ms (end to end 0.279276 ms, enqueue 0.0785706 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195364 ms (end to end 0.279535 ms, enqueue 0.077655 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154678 ms - Host latency: 0.195932 ms (end to end 0.277512 ms, enqueue 0.0782349 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.196341 ms (end to end 0.278104 ms, enqueue 0.0780121 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154752 ms - Host latency: 0.196127 ms (end to end 0.277887 ms, enqueue 0.0781525 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15488 ms - Host latency: 0.196375 ms (end to end 0.278806 ms, enqueue 0.0787476 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154745 ms - Host latency: 0.196045 ms (end to end 0.278738 ms, enqueue 0.0780212 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155011 ms - Host latency: 0.196362 ms (end to end 0.278452 ms, enqueue 0.0782562 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154974 ms - Host latency: 0.196362 ms (end to end 0.279453 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.196255 ms (end to end 0.277591 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154398 ms - Host latency: 0.195648 ms (end to end 0.279086 ms, enqueue 0.0779877 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154691 ms - Host latency: 0.195718 ms (end to end 0.279062 ms, enqueue 0.0785889 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195779 ms (end to end 0.277548 ms, enqueue 0.0786133 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15452 ms - Host latency: 0.195584 ms (end to end 0.279074 ms, enqueue 0.0779053 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.195395 ms (end to end 0.279501 ms, enqueue 0.078067 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154483 ms - Host latency: 0.195462 ms (end to end 0.279791 ms, enqueue 0.0787018 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195746 ms (end to end 0.277185 ms, enqueue 0.0778748 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195706 ms (end to end 0.278015 ms, enqueue 0.0778503 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154376 ms - Host latency: 0.19523 ms (end to end 0.27793 ms, enqueue 0.0773651 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154694 ms - Host latency: 0.195544 ms (end to end 0.279532 ms, enqueue 0.0776917 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154703 ms - Host latency: 0.196191 ms (end to end 0.279309 ms, enqueue 0.0788269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154138 ms - Host latency: 0.195117 ms (end to end 0.277777 ms, enqueue 0.0774231 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154495 ms - Host latency: 0.195557 ms (end to end 0.278946 ms, enqueue 0.0778412 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154486 ms - Host latency: 0.195645 ms (end to end 0.279034 ms, enqueue 0.0786285 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195511 ms (end to end 0.277402 ms, enqueue 0.077948 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154343 ms - Host latency: 0.195489 ms (end to end 0.277457 ms, enqueue 0.0782105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154581 ms - Host latency: 0.195493 ms (end to end 0.278567 ms, enqueue 0.0777191 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154645 ms - Host latency: 0.195938 ms (end to end 0.278592 ms, enqueue 0.0779022 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195538 ms (end to end 0.278485 ms, enqueue 0.0778351 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154681 ms - Host latency: 0.195908 ms (end to end 0.279211 ms, enqueue 0.0776886 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196378 ms (end to end 0.279257 ms, enqueue 0.0782837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154752 ms - Host latency: 0.195651 ms (end to end 0.278491 ms, enqueue 0.0779572 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154596 ms - Host latency: 0.195865 ms (end to end 0.27868 ms, enqueue 0.0781219 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154495 ms - Host latency: 0.195819 ms (end to end 0.279157 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15495 ms - Host latency: 0.19624 ms (end to end 0.27952 ms, enqueue 0.0776215 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154861 ms - Host latency: 0.195779 ms (end to end 0.27832 ms, enqueue 0.0784515 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154599 ms - Host latency: 0.195673 ms (end to end 0.279675 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154501 ms - Host latency: 0.195721 ms (end to end 0.280048 ms, enqueue 0.0776672 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15488 ms - Host latency: 0.196072 ms (end to end 0.278784 ms, enqueue 0.0781128 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154282 ms - Host latency: 0.195312 ms (end to end 0.281226 ms, enqueue 0.0783661 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154739 ms - Host latency: 0.195758 ms (end to end 0.280884 ms, enqueue 0.077713 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154092 ms - Host latency: 0.194977 ms (end to end 0.279266 ms, enqueue 0.0777557 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154434 ms - Host latency: 0.195483 ms (end to end 0.279092 ms, enqueue 0.0779388 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.195984 ms (end to end 0.279379 ms, enqueue 0.0783844 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154477 ms - Host latency: 0.195428 ms (end to end 0.280554 ms, enqueue 0.0780395 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.196701 ms (end to end 0.269809 ms, enqueue 0.0781128 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195694 ms (end to end 0.278574 ms, enqueue 0.0786041 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154047 ms - Host latency: 0.195111 ms (end to end 0.279398 ms, enqueue 0.07836 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154483 ms - Host latency: 0.195419 ms (end to end 0.279965 ms, enqueue 0.07883 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.195755 ms (end to end 0.27999 ms, enqueue 0.0776367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154559 ms - Host latency: 0.19559 ms (end to end 0.279599 ms, enqueue 0.0781311 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154648 ms - Host latency: 0.195663 ms (end to end 0.280441 ms, enqueue 0.0788483 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154584 ms - Host latency: 0.19559 ms (end to end 0.279431 ms, enqueue 0.0783447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154547 ms - Host latency: 0.195468 ms (end to end 0.279407 ms, enqueue 0.0781647 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154474 ms - Host latency: 0.195459 ms (end to end 0.280161 ms, enqueue 0.077478 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154828 ms - Host latency: 0.195911 ms (end to end 0.279028 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154648 ms - Host latency: 0.195721 ms (end to end 0.280475 ms, enqueue 0.0795166 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154483 ms - Host latency: 0.195609 ms (end to end 0.279507 ms, enqueue 0.0778961 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.177902 ms - Host latency: 0.221829 ms (end to end 0.293405 ms, enqueue 0.122028 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199402 ms - Host latency: 0.244763 ms (end to end 0.256625 ms, enqueue 0.229245 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197827 ms - Host latency: 0.243195 ms (end to end 0.255649 ms, enqueue 0.22897 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197458 ms - Host latency: 0.242593 ms (end to end 0.255261 ms, enqueue 0.228323 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154922 ms - Host latency: 0.196042 ms (end to end 0.207446 ms, enqueue 0.176495 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152673 ms - Host latency: 0.19408 ms (end to end 0.204919 ms, enqueue 0.174026 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152756 ms - Host latency: 0.193491 ms (end to end 0.203549 ms, enqueue 0.173242 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1526 ms - Host latency: 0.193109 ms (end to end 0.203558 ms, enqueue 0.175046 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153357 ms - Host latency: 0.194141 ms (end to end 0.209705 ms, enqueue 0.159192 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155414 ms - Host latency: 0.19646 ms (end to end 0.240753 ms, enqueue 0.13219 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154584 ms - Host latency: 0.195233 ms (end to end 0.263507 ms, enqueue 0.122937 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154846 ms - Host latency: 0.196063 ms (end to end 0.288544 ms, enqueue 0.123071 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.197711 ms (end to end 0.298309 ms, enqueue 0.123944 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155469 ms - Host latency: 0.196356 ms (end to end 0.294141 ms, enqueue 0.123938 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154114 ms - Host latency: 0.195288 ms (end to end 0.282831 ms, enqueue 0.123853 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154498 ms - Host latency: 0.195941 ms (end to end 0.287238 ms, enqueue 0.107819 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154608 ms - Host latency: 0.19566 ms (end to end 0.283484 ms, enqueue 0.101813 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.196057 ms (end to end 0.277948 ms, enqueue 0.102252 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195966 ms (end to end 0.284027 ms, enqueue 0.10166 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154919 ms - Host latency: 0.196033 ms (end to end 0.285187 ms, enqueue 0.102277 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.1961 ms (end to end 0.286548 ms, enqueue 0.101868 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195496 ms (end to end 0.283716 ms, enqueue 0.10188 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.195673 ms - Host latency: 0.240021 ms (end to end 0.275842 ms, enqueue 0.200537 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.210925 ms - Host latency: 0.256293 ms (end to end 0.266364 ms, enqueue 0.236725 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.209705 ms - Host latency: 0.254559 ms (end to end 0.263641 ms, enqueue 0.236749 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.206061 ms - Host latency: 0.250714 ms (end to end 0.259619 ms, enqueue 0.232568 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162073 ms - Host latency: 0.205133 ms (end to end 0.213855 ms, enqueue 0.192639 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162994 ms - Host latency: 0.206006 ms (end to end 0.213562 ms, enqueue 0.192273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.201111 ms - Host latency: 0.246747 ms (end to end 0.256104 ms, enqueue 0.228986 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196149 ms - Host latency: 0.24209 ms (end to end 0.254791 ms, enqueue 0.225238 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158643 ms - Host latency: 0.199762 ms (end to end 0.211041 ms, enqueue 0.135913 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158368 ms - Host latency: 0.199451 ms (end to end 0.21062 ms, enqueue 0.136169 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158582 ms - Host latency: 0.199707 ms (end to end 0.210999 ms, enqueue 0.136316 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157574 ms - Host latency: 0.198407 ms (end to end 0.209796 ms, enqueue 0.135229 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158606 ms - Host latency: 0.199841 ms (end to end 0.211804 ms, enqueue 0.136951 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15799 ms - Host latency: 0.199133 ms (end to end 0.210046 ms, enqueue 0.133221 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196222 ms (end to end 0.277411 ms, enqueue 0.110913 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195673 ms (end to end 0.281671 ms, enqueue 0.111908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.196057 ms (end to end 0.281012 ms, enqueue 0.110803 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154681 ms - Host latency: 0.195837 ms (end to end 0.2828 ms, enqueue 0.111609 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195575 ms (end to end 0.278931 ms, enqueue 0.110669 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154388 ms - Host latency: 0.195715 ms (end to end 0.278961 ms, enqueue 0.110803 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154572 ms - Host latency: 0.195874 ms (end to end 0.278296 ms, enqueue 0.0997192 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195111 ms (end to end 0.267047 ms, enqueue 0.089032 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195502 ms (end to end 0.275012 ms, enqueue 0.0880737 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.195276 ms (end to end 0.274036 ms, enqueue 0.0885925 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15387 ms - Host latency: 0.194751 ms (end to end 0.272827 ms, enqueue 0.0886047 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153937 ms - Host latency: 0.195319 ms (end to end 0.277057 ms, enqueue 0.0896362 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.19519 ms (end to end 0.273407 ms, enqueue 0.0854187 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154767 ms - Host latency: 0.195984 ms (end to end 0.27522 ms, enqueue 0.0779907 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154608 ms - Host latency: 0.195917 ms (end to end 0.274005 ms, enqueue 0.0777161 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15495 ms - Host latency: 0.196503 ms (end to end 0.276379 ms, enqueue 0.0771973 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154681 ms - Host latency: 0.196008 ms (end to end 0.277045 ms, enqueue 0.0787354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.195441 ms (end to end 0.275244 ms, enqueue 0.0779846 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195972 ms (end to end 0.274347 ms, enqueue 0.0775024 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195471 ms (end to end 0.276483 ms, enqueue 0.0776428 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195844 ms (end to end 0.27785 ms, enqueue 0.0773132 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154846 ms - Host latency: 0.196027 ms (end to end 0.277448 ms, enqueue 0.0782105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196265 ms (end to end 0.276648 ms, enqueue 0.0773865 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.196149 ms (end to end 0.276941 ms, enqueue 0.0779236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154803 ms - Host latency: 0.196033 ms (end to end 0.276685 ms, enqueue 0.0783264 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154993 ms - Host latency: 0.1966 ms (end to end 0.277826 ms, enqueue 0.0787903 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196387 ms (end to end 0.273962 ms, enqueue 0.0811096 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.196234 ms (end to end 0.273657 ms, enqueue 0.0815613 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154559 ms - Host latency: 0.195764 ms (end to end 0.278613 ms, enqueue 0.0823303 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195911 ms (end to end 0.277576 ms, enqueue 0.0788208 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195898 ms (end to end 0.278967 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196228 ms (end to end 0.278674 ms, enqueue 0.0785706 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155035 ms - Host latency: 0.196533 ms (end to end 0.269745 ms, enqueue 0.0787231 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.196179 ms (end to end 0.250751 ms, enqueue 0.0786804 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195056 ms (end to end 0.233807 ms, enqueue 0.0839294 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154584 ms - Host latency: 0.197052 ms (end to end 0.272589 ms, enqueue 0.091748 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154657 ms - Host latency: 0.196997 ms (end to end 0.274109 ms, enqueue 0.10188 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154559 ms - Host latency: 0.19599 ms (end to end 0.278516 ms, enqueue 0.0783081 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155157 ms - Host latency: 0.196436 ms (end to end 0.278479 ms, enqueue 0.0784912 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195575 ms (end to end 0.275708 ms, enqueue 0.078772 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196155 ms (end to end 0.27547 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.19566 ms (end to end 0.274927 ms, enqueue 0.078009 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.212823 ms - Host latency: 0.260516 ms (end to end 0.274011 ms, enqueue 0.243243 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198853 ms - Host latency: 0.24447 ms (end to end 0.256519 ms, enqueue 0.229681 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.205048 ms - Host latency: 0.251019 ms (end to end 0.261432 ms, enqueue 0.234015 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.173828 ms - Host latency: 0.21701 ms (end to end 0.226794 ms, enqueue 0.199976 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152692 ms - Host latency: 0.193201 ms (end to end 0.203552 ms, enqueue 0.174231 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152728 ms - Host latency: 0.192725 ms (end to end 0.204132 ms, enqueue 0.173492 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152722 ms - Host latency: 0.193457 ms (end to end 0.203754 ms, enqueue 0.174011 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152808 ms - Host latency: 0.193878 ms (end to end 0.205341 ms, enqueue 0.174219 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.179565 ms - Host latency: 0.225024 ms (end to end 0.234595 ms, enqueue 0.204584 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183258 ms - Host latency: 0.227972 ms (end to end 0.237469 ms, enqueue 0.212469 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183698 ms - Host latency: 0.228693 ms (end to end 0.238043 ms, enqueue 0.212671 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182007 ms - Host latency: 0.227332 ms (end to end 0.23811 ms, enqueue 0.211487 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.166931 ms - Host latency: 0.211011 ms (end to end 0.220905 ms, enqueue 0.158093 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164362 ms - Host latency: 0.20647 ms (end to end 0.216406 ms, enqueue 0.143939 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164392 ms - Host latency: 0.206537 ms (end to end 0.216077 ms, enqueue 0.143048 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162042 ms - Host latency: 0.205084 ms (end to end 0.215308 ms, enqueue 0.143976 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1633 ms - Host latency: 0.205188 ms (end to end 0.215521 ms, enqueue 0.142566 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162952 ms - Host latency: 0.204816 ms (end to end 0.214862 ms, enqueue 0.141968 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155182 ms - Host latency: 0.196771 ms (end to end 0.25976 ms, enqueue 0.107111 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195911 ms (end to end 0.277759 ms, enqueue 0.102509 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154181 ms - Host latency: 0.195447 ms (end to end 0.279877 ms, enqueue 0.102283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155231 ms - Host latency: 0.196149 ms (end to end 0.26864 ms, enqueue 0.101721 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153839 ms - Host latency: 0.195001 ms (end to end 0.28009 ms, enqueue 0.101685 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154138 ms - Host latency: 0.195215 ms (end to end 0.274982 ms, enqueue 0.103925 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154962 ms - Host latency: 0.195654 ms (end to end 0.283624 ms, enqueue 0.0948914 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154401 ms - Host latency: 0.195459 ms (end to end 0.271027 ms, enqueue 0.0852295 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195795 ms (end to end 0.272186 ms, enqueue 0.084613 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195227 ms (end to end 0.271204 ms, enqueue 0.0843689 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195813 ms (end to end 0.273877 ms, enqueue 0.0859802 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154401 ms - Host latency: 0.195551 ms (end to end 0.272498 ms, enqueue 0.0841003 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185352 ms - Host latency: 0.22984 ms (end to end 0.282874 ms, enqueue 0.155042 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199036 ms - Host latency: 0.24444 ms (end to end 0.255701 ms, enqueue 0.228589 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200806 ms - Host latency: 0.246429 ms (end to end 0.257636 ms, enqueue 0.230768 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197211 ms - Host latency: 0.242908 ms (end to end 0.254736 ms, enqueue 0.227844 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.184467 ms - Host latency: 0.229816 ms (end to end 0.241895 ms, enqueue 0.197662 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164313 ms - Host latency: 0.206439 ms (end to end 0.2216 ms, enqueue 0.150092 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164734 ms - Host latency: 0.207587 ms (end to end 0.221136 ms, enqueue 0.149915 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162842 ms - Host latency: 0.207947 ms (end to end 0.221393 ms, enqueue 0.15108 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165131 ms - Host latency: 0.208228 ms (end to end 0.221283 ms, enqueue 0.150287 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164557 ms - Host latency: 0.209229 ms (end to end 0.22149 ms, enqueue 0.150562 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155408 ms - Host latency: 0.197095 ms (end to end 0.255975 ms, enqueue 0.115094 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.19505 ms (end to end 0.280261 ms, enqueue 0.107806 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15423 ms - Host latency: 0.195612 ms (end to end 0.278998 ms, enqueue 0.108441 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154016 ms - Host latency: 0.19502 ms (end to end 0.277509 ms, enqueue 0.107953 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196069 ms (end to end 0.278778 ms, enqueue 0.107642 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155206 ms - Host latency: 0.196448 ms (end to end 0.269226 ms, enqueue 0.108643 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195996 ms (end to end 0.272534 ms, enqueue 0.0987122 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153589 ms - Host latency: 0.194702 ms (end to end 0.266327 ms, enqueue 0.0874695 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154089 ms - Host latency: 0.195435 ms (end to end 0.271375 ms, enqueue 0.0873108 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15379 ms - Host latency: 0.194983 ms (end to end 0.272064 ms, enqueue 0.0861633 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153876 ms - Host latency: 0.194794 ms (end to end 0.269159 ms, enqueue 0.0869141 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153589 ms - Host latency: 0.194623 ms (end to end 0.27121 ms, enqueue 0.0866455 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153729 ms - Host latency: 0.194983 ms (end to end 0.266089 ms, enqueue 0.0863953 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154895 ms - Host latency: 0.196204 ms (end to end 0.276123 ms, enqueue 0.0784607 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154779 ms - Host latency: 0.195959 ms (end to end 0.275824 ms, enqueue 0.0777588 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154657 ms - Host latency: 0.196149 ms (end to end 0.275214 ms, enqueue 0.0785339 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154584 ms - Host latency: 0.19563 ms (end to end 0.274908 ms, enqueue 0.0784668 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195911 ms (end to end 0.274762 ms, enqueue 0.0784119 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154767 ms - Host latency: 0.195905 ms (end to end 0.273816 ms, enqueue 0.0779907 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154486 ms - Host latency: 0.195471 ms (end to end 0.275116 ms, enqueue 0.077655 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154773 ms - Host latency: 0.195868 ms (end to end 0.278406 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154791 ms - Host latency: 0.195807 ms (end to end 0.2776 ms, enqueue 0.077771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196033 ms (end to end 0.278839 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.195764 ms (end to end 0.279114 ms, enqueue 0.0782166 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154962 ms - Host latency: 0.196417 ms (end to end 0.277979 ms, enqueue 0.0779663 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155084 ms - Host latency: 0.196436 ms (end to end 0.277063 ms, enqueue 0.0782349 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15473 ms - Host latency: 0.195898 ms (end to end 0.278687 ms, enqueue 0.0784546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195319 ms (end to end 0.278174 ms, enqueue 0.0780945 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195697 ms (end to end 0.279132 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195789 ms (end to end 0.279071 ms, enqueue 0.0778931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.1953 ms (end to end 0.279926 ms, enqueue 0.0785706 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195892 ms (end to end 0.279388 ms, enqueue 0.0783691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196143 ms (end to end 0.279926 ms, enqueue 0.0785217 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.207782 ms - Host latency: 0.254425 ms (end to end 0.279602 ms, enqueue 0.21264 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.210168 ms - Host latency: 0.25611 ms (end to end 0.293201 ms, enqueue 0.176135 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.196051 ms (end to end 0.27995 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195374 ms (end to end 0.279382 ms, enqueue 0.0781555 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154889 ms - Host latency: 0.196338 ms (end to end 0.279492 ms, enqueue 0.0964172 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196594 ms (end to end 0.281519 ms, enqueue 0.09953 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154706 ms - Host latency: 0.196094 ms (end to end 0.284021 ms, enqueue 0.100311 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154779 ms - Host latency: 0.196051 ms (end to end 0.280749 ms, enqueue 0.0992126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154498 ms - Host latency: 0.196069 ms (end to end 0.284167 ms, enqueue 0.0995605 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154645 ms - Host latency: 0.195905 ms (end to end 0.268353 ms, enqueue 0.102185 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156097 ms - Host latency: 0.198334 ms (end to end 0.286816 ms, enqueue 0.0983398 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154474 ms - Host latency: 0.19574 ms (end to end 0.27749 ms, enqueue 0.0856812 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.196161 ms (end to end 0.278387 ms, enqueue 0.084729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154462 ms - Host latency: 0.195544 ms (end to end 0.27807 ms, enqueue 0.0848083 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.202258 ms - Host latency: 0.248492 ms (end to end 0.274677 ms, enqueue 0.212109 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.202332 ms - Host latency: 0.248566 ms (end to end 0.259839 ms, enqueue 0.232056 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196722 ms - Host latency: 0.242261 ms (end to end 0.254718 ms, enqueue 0.228125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197961 ms - Host latency: 0.24516 ms (end to end 0.264539 ms, enqueue 0.209338 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.207739 ms - Host latency: 0.254163 ms (end to end 0.264307 ms, enqueue 0.235242 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182849 ms - Host latency: 0.227063 ms (end to end 0.23645 ms, enqueue 0.21156 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183948 ms - Host latency: 0.228473 ms (end to end 0.237451 ms, enqueue 0.212592 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181982 ms - Host latency: 0.226282 ms (end to end 0.23595 ms, enqueue 0.211176 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181195 ms - Host latency: 0.225659 ms (end to end 0.235474 ms, enqueue 0.210645 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182312 ms - Host latency: 0.225958 ms (end to end 0.236224 ms, enqueue 0.202045 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164117 ms - Host latency: 0.206378 ms (end to end 0.215332 ms, enqueue 0.142346 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163654 ms - Host latency: 0.205188 ms (end to end 0.215338 ms, enqueue 0.142609 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16369 ms - Host latency: 0.205707 ms (end to end 0.215076 ms, enqueue 0.142914 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163849 ms - Host latency: 0.206104 ms (end to end 0.216425 ms, enqueue 0.144165 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163318 ms - Host latency: 0.205255 ms (end to end 0.215143 ms, enqueue 0.142395 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16106 ms - Host latency: 0.203046 ms (end to end 0.224731 ms, enqueue 0.12934 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1539 ms - Host latency: 0.195282 ms (end to end 0.272809 ms, enqueue 0.102618 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195636 ms (end to end 0.273773 ms, enqueue 0.102393 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154718 ms - Host latency: 0.195728 ms (end to end 0.273547 ms, enqueue 0.103455 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154486 ms - Host latency: 0.195367 ms (end to end 0.275464 ms, enqueue 0.101483 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154071 ms - Host latency: 0.195136 ms (end to end 0.275494 ms, enqueue 0.10144 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154279 ms - Host latency: 0.195563 ms (end to end 0.275745 ms, enqueue 0.102106 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153778 ms - Host latency: 0.194482 ms (end to end 0.269403 ms, enqueue 0.0891724 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15412 ms - Host latency: 0.195398 ms (end to end 0.268835 ms, enqueue 0.0858337 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153967 ms - Host latency: 0.195221 ms (end to end 0.26947 ms, enqueue 0.0843994 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153839 ms - Host latency: 0.194775 ms (end to end 0.269348 ms, enqueue 0.0847046 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153839 ms - Host latency: 0.195044 ms (end to end 0.26861 ms, enqueue 0.0849182 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15354 ms - Host latency: 0.19502 ms (end to end 0.268231 ms, enqueue 0.0854553 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155121 ms - Host latency: 0.196088 ms (end to end 0.274109 ms, enqueue 0.0826904 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155023 ms - Host latency: 0.196246 ms (end to end 0.276501 ms, enqueue 0.078479 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155322 ms - Host latency: 0.196991 ms (end to end 0.277075 ms, enqueue 0.0775696 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154742 ms - Host latency: 0.196417 ms (end to end 0.274146 ms, enqueue 0.0775085 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154657 ms - Host latency: 0.195831 ms (end to end 0.276001 ms, enqueue 0.0773376 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154968 ms - Host latency: 0.196252 ms (end to end 0.27735 ms, enqueue 0.0773743 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154523 ms - Host latency: 0.195947 ms (end to end 0.274487 ms, enqueue 0.0784546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154608 ms - Host latency: 0.195489 ms (end to end 0.280292 ms, enqueue 0.077655 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154535 ms - Host latency: 0.195575 ms (end to end 0.280139 ms, enqueue 0.0785889 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195465 ms (end to end 0.281458 ms, enqueue 0.077655 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195764 ms (end to end 0.282098 ms, enqueue 0.0781921 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.195758 ms (end to end 0.280786 ms, enqueue 0.0783081 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195837 ms (end to end 0.283002 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196509 ms (end to end 0.283307 ms, enqueue 0.0796692 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15495 ms - Host latency: 0.196228 ms (end to end 0.281354 ms, enqueue 0.0808716 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.196173 ms (end to end 0.281079 ms, enqueue 0.0813965 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195361 ms (end to end 0.280621 ms, enqueue 0.0784546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154102 ms - Host latency: 0.19516 ms (end to end 0.280231 ms, enqueue 0.0791138 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154633 ms - Host latency: 0.19585 ms (end to end 0.280682 ms, enqueue 0.0788513 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15379 ms - Host latency: 0.194708 ms (end to end 0.278113 ms, enqueue 0.080072 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195874 ms (end to end 0.281061 ms, enqueue 0.0793823 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.193762 ms - Host latency: 0.239282 ms (end to end 0.28064 ms, enqueue 0.181354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199329 ms - Host latency: 0.244513 ms (end to end 0.254291 ms, enqueue 0.227374 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196539 ms - Host latency: 0.241577 ms (end to end 0.253168 ms, enqueue 0.226471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.173364 ms - Host latency: 0.217432 ms (end to end 0.224969 ms, enqueue 0.20141 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.168677 ms - Host latency: 0.213049 ms (end to end 0.219855 ms, enqueue 0.196429 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.169861 ms - Host latency: 0.213965 ms (end to end 0.221643 ms, enqueue 0.19776 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170801 ms - Host latency: 0.21474 ms (end to end 0.221454 ms, enqueue 0.198199 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163544 ms - Host latency: 0.207068 ms (end to end 0.21712 ms, enqueue 0.162823 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156287 ms - Host latency: 0.197119 ms (end to end 0.214539 ms, enqueue 0.132886 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156146 ms - Host latency: 0.196765 ms (end to end 0.213843 ms, enqueue 0.13186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156042 ms - Host latency: 0.197333 ms (end to end 0.215277 ms, enqueue 0.1315 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.174219 ms - Host latency: 0.218781 ms (end to end 0.232355 ms, enqueue 0.178613 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.167444 ms - Host latency: 0.212921 ms (end to end 0.221497 ms, enqueue 0.19585 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.186591 ms - Host latency: 0.233777 ms (end to end 0.242346 ms, enqueue 0.213361 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.191327 ms - Host latency: 0.238806 ms (end to end 0.246722 ms, enqueue 0.217633 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.189258 ms - Host latency: 0.236609 ms (end to end 0.244232 ms, enqueue 0.215228 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.190454 ms - Host latency: 0.238922 ms (end to end 0.247174 ms, enqueue 0.21452 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163031 ms - Host latency: 0.206641 ms (end to end 0.221179 ms, enqueue 0.153387 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163879 ms - Host latency: 0.207751 ms (end to end 0.220313 ms, enqueue 0.152161 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16402 ms - Host latency: 0.207257 ms (end to end 0.21994 ms, enqueue 0.151807 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162463 ms - Host latency: 0.208087 ms (end to end 0.220721 ms, enqueue 0.153394 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163977 ms - Host latency: 0.208081 ms (end to end 0.221106 ms, enqueue 0.152295 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159686 ms - Host latency: 0.204041 ms (end to end 0.227191 ms, enqueue 0.139221 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154962 ms - Host latency: 0.196765 ms (end to end 0.287433 ms, enqueue 0.115582 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165955 ms - Host latency: 0.208923 ms (end to end 0.293896 ms, enqueue 0.148016 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183276 ms - Host latency: 0.228125 ms (end to end 0.238257 ms, enqueue 0.212134 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.184711 ms - Host latency: 0.229449 ms (end to end 0.23988 ms, enqueue 0.214203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183221 ms - Host latency: 0.22774 ms (end to end 0.237134 ms, enqueue 0.211786 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182874 ms - Host latency: 0.227618 ms (end to end 0.23703 ms, enqueue 0.212134 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181201 ms - Host latency: 0.226343 ms (end to end 0.237604 ms, enqueue 0.210931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.180542 ms - Host latency: 0.225006 ms (end to end 0.234875 ms, enqueue 0.209662 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170844 ms - Host latency: 0.214246 ms (end to end 0.225452 ms, enqueue 0.170282 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164069 ms - Host latency: 0.205927 ms (end to end 0.215759 ms, enqueue 0.142291 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163257 ms - Host latency: 0.20625 ms (end to end 0.216467 ms, enqueue 0.143958 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162781 ms - Host latency: 0.204572 ms (end to end 0.215582 ms, enqueue 0.141992 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16189 ms - Host latency: 0.20451 ms (end to end 0.215625 ms, enqueue 0.145551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163293 ms - Host latency: 0.205237 ms (end to end 0.216339 ms, enqueue 0.142413 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15509 ms - Host latency: 0.196051 ms (end to end 0.263275 ms, enqueue 0.105408 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.196008 ms (end to end 0.285724 ms, enqueue 0.102325 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154974 ms - Host latency: 0.195782 ms (end to end 0.281519 ms, enqueue 0.102106 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154919 ms - Host latency: 0.195996 ms (end to end 0.284973 ms, enqueue 0.102667 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154572 ms - Host latency: 0.195532 ms (end to end 0.280908 ms, enqueue 0.102643 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195703 ms (end to end 0.27995 ms, enqueue 0.102167 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154437 ms - Host latency: 0.19552 ms (end to end 0.277881 ms, enqueue 0.0960938 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154102 ms - Host latency: 0.195471 ms (end to end 0.274011 ms, enqueue 0.0832092 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195605 ms (end to end 0.273663 ms, enqueue 0.0837463 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.195862 ms (end to end 0.273608 ms, enqueue 0.0823303 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154401 ms - Host latency: 0.195184 ms (end to end 0.275848 ms, enqueue 0.0831482 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154193 ms - Host latency: 0.195532 ms (end to end 0.275092 ms, enqueue 0.0830994 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154425 ms - Host latency: 0.195404 ms (end to end 0.274603 ms, enqueue 0.0839294 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196039 ms (end to end 0.276532 ms, enqueue 0.0784241 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195636 ms (end to end 0.277423 ms, enqueue 0.0778381 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155042 ms - Host latency: 0.19646 ms (end to end 0.276532 ms, enqueue 0.0782715 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.196167 ms (end to end 0.275562 ms, enqueue 0.0786926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196429 ms (end to end 0.278119 ms, enqueue 0.0782043 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154657 ms - Host latency: 0.195776 ms (end to end 0.277808 ms, enqueue 0.0778625 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154266 ms - Host latency: 0.195422 ms (end to end 0.273853 ms, enqueue 0.0787415 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196198 ms (end to end 0.272333 ms, enqueue 0.0784668 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154486 ms - Host latency: 0.195636 ms (end to end 0.277002 ms, enqueue 0.0806458 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.19585 ms (end to end 0.278986 ms, enqueue 0.0782837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154572 ms - Host latency: 0.195923 ms (end to end 0.277783 ms, enqueue 0.077948 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154358 ms - Host latency: 0.1953 ms (end to end 0.277039 ms, enqueue 0.0783203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.195801 ms (end to end 0.278369 ms, enqueue 0.0780029 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154584 ms - Host latency: 0.195715 ms (end to end 0.27876 ms, enqueue 0.0789734 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.195697 ms (end to end 0.278784 ms, enqueue 0.0780151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154474 ms - Host latency: 0.195355 ms (end to end 0.278241 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154779 ms - Host latency: 0.195715 ms (end to end 0.278259 ms, enqueue 0.0783264 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196088 ms (end to end 0.277649 ms, enqueue 0.0778625 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154608 ms - Host latency: 0.195764 ms (end to end 0.278241 ms, enqueue 0.0797729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154559 ms - Host latency: 0.196021 ms (end to end 0.278253 ms, enqueue 0.0778931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195856 ms (end to end 0.278955 ms, enqueue 0.0780457 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154547 ms - Host latency: 0.19585 ms (end to end 0.277246 ms, enqueue 0.0786194 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154938 ms - Host latency: 0.195966 ms (end to end 0.279199 ms, enqueue 0.0781616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195721 ms (end to end 0.278064 ms, enqueue 0.0789612 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195819 ms (end to end 0.278107 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.196033 ms (end to end 0.277057 ms, enqueue 0.0780884 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154987 ms - Host latency: 0.196051 ms (end to end 0.27901 ms, enqueue 0.0775024 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195374 ms (end to end 0.278131 ms, enqueue 0.0779724 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154633 ms - Host latency: 0.196075 ms (end to end 0.27749 ms, enqueue 0.0796692 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154669 ms - Host latency: 0.195746 ms (end to end 0.27785 ms, enqueue 0.0786194 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154895 ms - Host latency: 0.196313 ms (end to end 0.277747 ms, enqueue 0.0775513 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196143 ms (end to end 0.27887 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.19646 ms (end to end 0.27854 ms, enqueue 0.0783569 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195972 ms (end to end 0.279492 ms, enqueue 0.0780395 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.19574 ms (end to end 0.279785 ms, enqueue 0.0794556 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195508 ms (end to end 0.277417 ms, enqueue 0.0781982 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195996 ms (end to end 0.277307 ms, enqueue 0.0781494 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195886 ms (end to end 0.277173 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195874 ms (end to end 0.279285 ms, enqueue 0.0774658 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196289 ms (end to end 0.281311 ms, enqueue 0.0788696 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195825 ms (end to end 0.278284 ms, enqueue 0.0778564 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155066 ms - Host latency: 0.196448 ms (end to end 0.279639 ms, enqueue 0.0781372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.196021 ms (end to end 0.27821 ms, enqueue 0.0784302 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195874 ms (end to end 0.276721 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195618 ms (end to end 0.276526 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195837 ms (end to end 0.275403 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195801 ms (end to end 0.275854 ms, enqueue 0.0780029 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155066 ms - Host latency: 0.196423 ms (end to end 0.276782 ms, enqueue 0.0777466 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154175 ms - Host latency: 0.195422 ms (end to end 0.27572 ms, enqueue 0.0778198 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.20332 ms - Host latency: 0.249573 ms (end to end 0.291882 ms, enqueue 0.192676 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198462 ms - Host latency: 0.243896 ms (end to end 0.254639 ms, enqueue 0.228491 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196179 ms - Host latency: 0.241455 ms (end to end 0.252673 ms, enqueue 0.226367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19624 ms - Host latency: 0.241736 ms (end to end 0.253284 ms, enqueue 0.227283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196606 ms - Host latency: 0.242224 ms (end to end 0.253162 ms, enqueue 0.226855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.173352 ms - Host latency: 0.217517 ms (end to end 0.229822 ms, enqueue 0.175378 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164771 ms - Host latency: 0.207642 ms (end to end 0.220447 ms, enqueue 0.149048 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165576 ms - Host latency: 0.208569 ms (end to end 0.221069 ms, enqueue 0.14856 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164343 ms - Host latency: 0.208813 ms (end to end 0.220398 ms, enqueue 0.14967 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164404 ms - Host latency: 0.208154 ms (end to end 0.220691 ms, enqueue 0.148828 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164905 ms - Host latency: 0.20769 ms (end to end 0.219482 ms, enqueue 0.148511 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155457 ms - Host latency: 0.19707 ms (end to end 0.263904 ms, enqueue 0.100134 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195166 ms (end to end 0.270703 ms, enqueue 0.0881714 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154163 ms - Host latency: 0.195496 ms (end to end 0.273608 ms, enqueue 0.0891846 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.195422 ms (end to end 0.27124 ms, enqueue 0.0880371 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.195789 ms (end to end 0.270007 ms, enqueue 0.0861938 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.195728 ms (end to end 0.274634 ms, enqueue 0.0813477 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154285 ms - Host latency: 0.195471 ms (end to end 0.275256 ms, enqueue 0.0799805 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.19585 ms (end to end 0.275879 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195764 ms (end to end 0.270312 ms, enqueue 0.0776123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195886 ms (end to end 0.278284 ms, enqueue 0.0769653 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.195813 ms (end to end 0.27782 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195935 ms (end to end 0.277832 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195837 ms (end to end 0.279224 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.195312 ms (end to end 0.279944 ms, enqueue 0.0779053 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.19541 ms (end to end 0.277979 ms, enqueue 0.077002 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.195801 ms (end to end 0.272656 ms, enqueue 0.0904785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195972 ms (end to end 0.282214 ms, enqueue 0.0985596 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195618 ms (end to end 0.281433 ms, enqueue 0.0982422 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195874 ms (end to end 0.279578 ms, enqueue 0.0973022 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196509 ms (end to end 0.279883 ms, enqueue 0.0880859 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196289 ms (end to end 0.27832 ms, enqueue 0.0810059 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.19657 ms (end to end 0.278687 ms, enqueue 0.0809448 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154895 ms - Host latency: 0.195752 ms (end to end 0.278552 ms, enqueue 0.0812378 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196289 ms (end to end 0.279175 ms, enqueue 0.0823242 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154846 ms - Host latency: 0.196045 ms (end to end 0.27981 ms, enqueue 0.0812744 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.195789 ms (end to end 0.27854 ms, enqueue 0.0811401 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195715 ms (end to end 0.279907 ms, enqueue 0.0777222 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195239 ms (end to end 0.28031 ms, enqueue 0.0784912 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195972 ms (end to end 0.280469 ms, enqueue 0.0784424 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.195178 ms (end to end 0.278943 ms, enqueue 0.0773193 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.178662 ms - Host latency: 0.223669 ms (end to end 0.301477 ms, enqueue 0.111682 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.21582 ms - Host latency: 0.262317 ms (end to end 0.273669 ms, enqueue 0.244702 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.169165 ms - Host latency: 0.211206 ms (end to end 0.23363 ms, enqueue 0.156836 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195764 ms (end to end 0.243201 ms, enqueue 0.133154 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153882 ms - Host latency: 0.194812 ms (end to end 0.253345 ms, enqueue 0.133228 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15376 ms - Host latency: 0.195007 ms (end to end 0.249573 ms, enqueue 0.132703 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153711 ms - Host latency: 0.194885 ms (end to end 0.259314 ms, enqueue 0.131653 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.196179 ms (end to end 0.280347 ms, enqueue 0.119458 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195642 ms (end to end 0.288208 ms, enqueue 0.114868 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195776 ms (end to end 0.288709 ms, enqueue 0.116174 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196423 ms (end to end 0.287769 ms, enqueue 0.114734 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195361 ms (end to end 0.289294 ms, enqueue 0.114514 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154968 ms - Host latency: 0.196106 ms (end to end 0.287097 ms, enqueue 0.115405 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196387 ms (end to end 0.287207 ms, enqueue 0.109058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195691 ms (end to end 0.283142 ms, enqueue 0.0973145 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195508 ms (end to end 0.283521 ms, enqueue 0.0969238 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195837 ms (end to end 0.282947 ms, enqueue 0.0963013 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.195825 ms (end to end 0.28335 ms, enqueue 0.0967896 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.196057 ms (end to end 0.283301 ms, enqueue 0.0964966 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195557 ms (end to end 0.282947 ms, enqueue 0.0964111 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154114 ms - Host latency: 0.195166 ms (end to end 0.283154 ms, enqueue 0.0972168 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.195325 ms (end to end 0.282275 ms, enqueue 0.0971558 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.195996 ms (end to end 0.283081 ms, enqueue 0.0962524 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195764 ms (end to end 0.28374 ms, enqueue 0.096582 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195068 ms (end to end 0.283142 ms, enqueue 0.0965942 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.195862 ms (end to end 0.282666 ms, enqueue 0.0969727 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195422 ms (end to end 0.282336 ms, enqueue 0.0966797 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195752 ms (end to end 0.282751 ms, enqueue 0.0966797 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195642 ms (end to end 0.282544 ms, enqueue 0.0961792 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195264 ms (end to end 0.281897 ms, enqueue 0.0965454 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.19585 ms (end to end 0.28396 ms, enqueue 0.0969971 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195947 ms (end to end 0.283606 ms, enqueue 0.0962891 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195947 ms (end to end 0.281311 ms, enqueue 0.0968994 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.19552 ms (end to end 0.280933 ms, enqueue 0.0967041 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195911 ms (end to end 0.280969 ms, enqueue 0.0959839 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.173059 ms - Host latency: 0.217444 ms (end to end 0.264307 ms, enqueue 0.173791 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183325 ms - Host latency: 0.227893 ms (end to end 0.237659 ms, enqueue 0.212378 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182092 ms - Host latency: 0.226208 ms (end to end 0.23595 ms, enqueue 0.211108 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.187549 ms - Host latency: 0.232544 ms (end to end 0.242908 ms, enqueue 0.217004 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.18291 ms - Host latency: 0.227258 ms (end to end 0.237524 ms, enqueue 0.2125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185083 ms - Host latency: 0.229932 ms (end to end 0.239722 ms, enqueue 0.213611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185388 ms - Host latency: 0.230017 ms (end to end 0.239233 ms, enqueue 0.213831 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164038 ms - Host latency: 0.206702 ms (end to end 0.217871 ms, enqueue 0.144617 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163306 ms - Host latency: 0.205457 ms (end to end 0.215613 ms, enqueue 0.143176 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16344 ms - Host latency: 0.205432 ms (end to end 0.215662 ms, enqueue 0.142004 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163147 ms - Host latency: 0.204773 ms (end to end 0.214929 ms, enqueue 0.141992 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163171 ms - Host latency: 0.205933 ms (end to end 0.217102 ms, enqueue 0.144141 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161926 ms - Host latency: 0.20376 ms (end to end 0.214539 ms, enqueue 0.137317 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.196143 ms (end to end 0.27699 ms, enqueue 0.101978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.195898 ms (end to end 0.282373 ms, enqueue 0.102075 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155371 ms - Host latency: 0.196851 ms (end to end 0.284998 ms, enqueue 0.10199 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195667 ms (end to end 0.282532 ms, enqueue 0.101929 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155188 ms - Host latency: 0.196936 ms (end to end 0.283533 ms, enqueue 0.101294 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155188 ms - Host latency: 0.196375 ms (end to end 0.282507 ms, enqueue 0.10249 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.196008 ms (end to end 0.277673 ms, enqueue 0.0881348 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.195935 ms (end to end 0.277039 ms, enqueue 0.0824951 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154846 ms - Host latency: 0.196069 ms (end to end 0.275134 ms, enqueue 0.0825317 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195483 ms (end to end 0.274329 ms, enqueue 0.0820923 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.196021 ms (end to end 0.275586 ms, enqueue 0.082959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195349 ms (end to end 0.274829 ms, enqueue 0.0824463 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195667 ms (end to end 0.275403 ms, enqueue 0.0819824 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196057 ms (end to end 0.276147 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195959 ms (end to end 0.27533 ms, enqueue 0.0775146 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155139 ms - Host latency: 0.196521 ms (end to end 0.277295 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196082 ms (end to end 0.276013 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195715 ms (end to end 0.276538 ms, enqueue 0.0769287 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195801 ms (end to end 0.27854 ms, enqueue 0.077063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155298 ms - Host latency: 0.19718 ms (end to end 0.285791 ms, enqueue 0.0771362 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.19613 ms (end to end 0.283057 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155249 ms - Host latency: 0.196643 ms (end to end 0.284119 ms, enqueue 0.0773315 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.196411 ms (end to end 0.283325 ms, enqueue 0.0774292 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196069 ms (end to end 0.283472 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155188 ms - Host latency: 0.197021 ms (end to end 0.284229 ms, enqueue 0.0771851 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15575 ms - Host latency: 0.197021 ms (end to end 0.28407 ms, enqueue 0.0777588 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195984 ms (end to end 0.281287 ms, enqueue 0.0790771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.19552 ms (end to end 0.28125 ms, enqueue 0.0788208 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195715 ms (end to end 0.280566 ms, enqueue 0.0796509 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195422 ms (end to end 0.281458 ms, enqueue 0.0792358 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195508 ms (end to end 0.280237 ms, enqueue 0.0789063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154285 ms - Host latency: 0.195215 ms (end to end 0.280798 ms, enqueue 0.0794312 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196338 ms (end to end 0.279163 ms, enqueue 0.0789307 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195923 ms (end to end 0.278625 ms, enqueue 0.079248 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.195984 ms (end to end 0.279224 ms, enqueue 0.0790161 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195715 ms (end to end 0.279028 ms, enqueue 0.0789063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195862 ms (end to end 0.277722 ms, enqueue 0.0799683 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195496 ms (end to end 0.280286 ms, enqueue 0.0793091 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.196143 ms (end to end 0.277258 ms, enqueue 0.0782105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195898 ms (end to end 0.27887 ms, enqueue 0.077478 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195886 ms (end to end 0.278992 ms, enqueue 0.0775269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154211 ms - Host latency: 0.195154 ms (end to end 0.276917 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195862 ms (end to end 0.277844 ms, enqueue 0.0788818 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195178 ms (end to end 0.278552 ms, enqueue 0.0783447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154773 ms - Host latency: 0.195996 ms (end to end 0.274854 ms, enqueue 0.0782105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195776 ms (end to end 0.275134 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195862 ms (end to end 0.274207 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195776 ms (end to end 0.275513 ms, enqueue 0.0787231 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155139 ms - Host latency: 0.196777 ms (end to end 0.275085 ms, enqueue 0.0776245 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195947 ms (end to end 0.275439 ms, enqueue 0.0781372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155237 ms - Host latency: 0.196643 ms (end to end 0.269605 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154895 ms - Host latency: 0.196606 ms (end to end 0.274792 ms, enqueue 0.0775269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195398 ms (end to end 0.278162 ms, enqueue 0.0800903 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195593 ms (end to end 0.275684 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.19574 ms (end to end 0.279004 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154285 ms - Host latency: 0.195227 ms (end to end 0.279687 ms, enqueue 0.0775391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154163 ms - Host latency: 0.195178 ms (end to end 0.280054 ms, enqueue 0.0784912 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195483 ms (end to end 0.280066 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196143 ms (end to end 0.28031 ms, enqueue 0.0778198 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195911 ms (end to end 0.279504 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195813 ms (end to end 0.279492 ms, enqueue 0.0771362 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195557 ms (end to end 0.279358 ms, enqueue 0.0783081 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195361 ms (end to end 0.27937 ms, enqueue 0.0783813 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195435 ms (end to end 0.279614 ms, enqueue 0.0776489 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195374 ms (end to end 0.278833 ms, enqueue 0.0784546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154041 ms - Host latency: 0.195325 ms (end to end 0.279102 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195605 ms (end to end 0.278699 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195544 ms (end to end 0.277161 ms, enqueue 0.0784668 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.195288 ms (end to end 0.279639 ms, enqueue 0.0778931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.19834 ms (end to end 0.25719 ms, enqueue 0.11958 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156128 ms - Host latency: 0.206921 ms (end to end 0.216895 ms, enqueue 0.174182 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.169495 ms - Host latency: 0.213306 ms (end to end 0.221875 ms, enqueue 0.19906 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156421 ms - Host latency: 0.199438 ms (end to end 0.206067 ms, enqueue 0.184265 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.19751 ms (end to end 0.206262 ms, enqueue 0.184595 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.196802 ms (end to end 0.206042 ms, enqueue 0.184058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157471 ms - Host latency: 0.199231 ms (end to end 0.227844 ms, enqueue 0.137146 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154211 ms - Host latency: 0.1953 ms (end to end 0.273865 ms, enqueue 0.127112 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.19646 ms (end to end 0.292957 ms, enqueue 0.128088 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155249 ms - Host latency: 0.196277 ms (end to end 0.295483 ms, enqueue 0.127222 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196069 ms (end to end 0.293396 ms, enqueue 0.127295 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155042 ms - Host latency: 0.196204 ms (end to end 0.292029 ms, enqueue 0.128906 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155334 ms - Host latency: 0.196863 ms (end to end 0.289136 ms, enqueue 0.113855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195691 ms (end to end 0.275562 ms, enqueue 0.0954346 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195874 ms (end to end 0.281531 ms, enqueue 0.0954712 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195886 ms (end to end 0.275916 ms, enqueue 0.0954834 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.19613 ms (end to end 0.277515 ms, enqueue 0.0958862 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.196228 ms (end to end 0.281604 ms, enqueue 0.0956543 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.196716 ms (end to end 0.283643 ms, enqueue 0.0965332 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195593 ms (end to end 0.276721 ms, enqueue 0.077771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154211 ms - Host latency: 0.195325 ms (end to end 0.275488 ms, enqueue 0.078186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196143 ms (end to end 0.276917 ms, enqueue 0.0785767 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196777 ms (end to end 0.276001 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.196167 ms (end to end 0.274841 ms, enqueue 0.0788208 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196289 ms (end to end 0.276038 ms, enqueue 0.0785522 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195325 ms (end to end 0.275574 ms, enqueue 0.0783081 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.19585 ms (end to end 0.276392 ms, enqueue 0.0789063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.197156 ms (end to end 0.281067 ms, enqueue 0.0794312 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.195923 ms (end to end 0.277527 ms, enqueue 0.0775757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16073 ms - Host latency: 0.207385 ms (end to end 0.223608 ms, enqueue 0.158704 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164746 ms - Host latency: 0.208386 ms (end to end 0.220361 ms, enqueue 0.149597 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158057 ms - Host latency: 0.202295 ms (end to end 0.216528 ms, enqueue 0.139868 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15553 ms - Host latency: 0.196777 ms (end to end 0.215613 ms, enqueue 0.132288 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156189 ms - Host latency: 0.197058 ms (end to end 0.217529 ms, enqueue 0.132935 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155737 ms - Host latency: 0.196814 ms (end to end 0.216199 ms, enqueue 0.132288 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155505 ms - Host latency: 0.196606 ms (end to end 0.216931 ms, enqueue 0.132422 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156799 ms - Host latency: 0.197791 ms (end to end 0.215674 ms, enqueue 0.132922 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154968 ms - Host latency: 0.196069 ms (end to end 0.235706 ms, enqueue 0.117883 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195447 ms (end to end 0.280481 ms, enqueue 0.0991211 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196301 ms (end to end 0.281592 ms, enqueue 0.0983154 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195886 ms (end to end 0.27937 ms, enqueue 0.0988159 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196826 ms (end to end 0.284668 ms, enqueue 0.0988159 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196704 ms (end to end 0.284827 ms, enqueue 0.0991699 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.19613 ms (end to end 0.281604 ms, enqueue 0.0995605 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.19613 ms (end to end 0.277991 ms, enqueue 0.0801514 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.196082 ms (end to end 0.274292 ms, enqueue 0.07854 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196143 ms (end to end 0.273901 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195984 ms (end to end 0.275159 ms, enqueue 0.0786255 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196094 ms (end to end 0.275098 ms, enqueue 0.0791626 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195264 ms (end to end 0.271069 ms, enqueue 0.0786987 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195752 ms (end to end 0.270752 ms, enqueue 0.0805664 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.195459 ms (end to end 0.275427 ms, enqueue 0.0781982 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155408 ms - Host latency: 0.197131 ms (end to end 0.275415 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196057 ms (end to end 0.277515 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.196082 ms (end to end 0.275928 ms, enqueue 0.0782959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.1953 ms (end to end 0.277856 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195825 ms (end to end 0.276978 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195435 ms (end to end 0.278259 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154211 ms - Host latency: 0.195459 ms (end to end 0.277173 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195898 ms (end to end 0.277698 ms, enqueue 0.0785034 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.19574 ms (end to end 0.278992 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154944 ms - Host latency: 0.196167 ms (end to end 0.27832 ms, enqueue 0.0782959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.197192 ms (end to end 0.270923 ms, enqueue 0.0797119 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154175 ms - Host latency: 0.194995 ms (end to end 0.278687 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195679 ms (end to end 0.279626 ms, enqueue 0.0786621 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.196155 ms (end to end 0.279529 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195557 ms (end to end 0.275623 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.195129 ms (end to end 0.27533 ms, enqueue 0.0789673 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195142 ms (end to end 0.280713 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.196375 ms (end to end 0.280469 ms, enqueue 0.0779053 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154358 ms - Host latency: 0.195398 ms (end to end 0.282349 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196472 ms (end to end 0.28269 ms, enqueue 0.0780395 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154993 ms - Host latency: 0.196826 ms (end to end 0.282275 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195386 ms (end to end 0.282178 ms, enqueue 0.0777954 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.195776 ms (end to end 0.281519 ms, enqueue 0.0780151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195935 ms (end to end 0.280017 ms, enqueue 0.0781372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195703 ms (end to end 0.279334 ms, enqueue 0.0775879 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.195911 ms (end to end 0.280029 ms, enqueue 0.0783325 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195679 ms (end to end 0.279675 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195642 ms (end to end 0.279041 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195667 ms (end to end 0.27981 ms, enqueue 0.0775391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195728 ms (end to end 0.279749 ms, enqueue 0.0778931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195654 ms (end to end 0.254639 ms, enqueue 0.124658 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196216 ms (end to end 0.288037 ms, enqueue 0.123633 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155017 ms - Host latency: 0.196118 ms (end to end 0.291199 ms, enqueue 0.125757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196143 ms (end to end 0.289233 ms, enqueue 0.123657 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155322 ms - Host latency: 0.196643 ms (end to end 0.292041 ms, enqueue 0.123059 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196375 ms (end to end 0.29248 ms, enqueue 0.124316 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195398 ms (end to end 0.291541 ms, enqueue 0.123413 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196167 ms (end to end 0.285815 ms, enqueue 0.11593 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195923 ms (end to end 0.283142 ms, enqueue 0.101917 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.19624 ms (end to end 0.282678 ms, enqueue 0.103003 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196387 ms (end to end 0.284924 ms, enqueue 0.103015 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155212 ms - Host latency: 0.197217 ms (end to end 0.284802 ms, enqueue 0.10271 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.196045 ms (end to end 0.283484 ms, enqueue 0.10343 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155347 ms - Host latency: 0.197046 ms (end to end 0.286096 ms, enqueue 0.102563 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.196106 ms (end to end 0.28241 ms, enqueue 0.100574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196167 ms (end to end 0.282971 ms, enqueue 0.102039 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154944 ms - Host latency: 0.19707 ms (end to end 0.282861 ms, enqueue 0.101294 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.196362 ms (end to end 0.284375 ms, enqueue 0.101111 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.196362 ms (end to end 0.28324 ms, enqueue 0.101172 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195752 ms (end to end 0.282178 ms, enqueue 0.100806 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.19585 ms (end to end 0.281152 ms, enqueue 0.0872803 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.19613 ms (end to end 0.280688 ms, enqueue 0.0843994 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196582 ms (end to end 0.281445 ms, enqueue 0.0849609 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196094 ms (end to end 0.276135 ms, enqueue 0.0861694 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195874 ms (end to end 0.273462 ms, enqueue 0.0854736 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195947 ms (end to end 0.274121 ms, enqueue 0.0861328 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196057 ms (end to end 0.276538 ms, enqueue 0.0863525 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196216 ms (end to end 0.275171 ms, enqueue 0.0783813 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15542 ms - Host latency: 0.197766 ms (end to end 0.277625 ms, enqueue 0.0809448 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.19613 ms (end to end 0.27738 ms, enqueue 0.0781372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196143 ms (end to end 0.276514 ms, enqueue 0.0797729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155835 ms - Host latency: 0.19696 ms (end to end 0.277441 ms, enqueue 0.0778809 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.222253 ms - Host latency: 0.269629 ms (end to end 0.278943 ms, enqueue 0.247961 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198059 ms - Host latency: 0.243652 ms (end to end 0.254517 ms, enqueue 0.227417 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16012 ms - Host latency: 0.201953 ms (end to end 0.26012 ms, enqueue 0.100647 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195752 ms (end to end 0.274805 ms, enqueue 0.0800049 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195776 ms (end to end 0.276404 ms, enqueue 0.0791382 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195837 ms (end to end 0.278186 ms, enqueue 0.0877686 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195642 ms (end to end 0.282092 ms, enqueue 0.100977 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.19552 ms (end to end 0.280066 ms, enqueue 0.101331 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.196399 ms (end to end 0.282446 ms, enqueue 0.10072 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164734 ms - Host latency: 0.207874 ms (end to end 0.285095 ms, enqueue 0.133826 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19884 ms - Host latency: 0.243774 ms (end to end 0.250623 ms, enqueue 0.225098 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200208 ms - Host latency: 0.245337 ms (end to end 0.25415 ms, enqueue 0.22832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.201941 ms - Host latency: 0.246985 ms (end to end 0.2547 ms, enqueue 0.228394 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199097 ms - Host latency: 0.244336 ms (end to end 0.251318 ms, enqueue 0.225305 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197827 ms - Host latency: 0.24281 ms (end to end 0.250354 ms, enqueue 0.22074 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163757 ms - Host latency: 0.206226 ms (end to end 0.221802 ms, enqueue 0.151001 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163843 ms - Host latency: 0.206934 ms (end to end 0.220337 ms, enqueue 0.151025 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165112 ms - Host latency: 0.207617 ms (end to end 0.220935 ms, enqueue 0.15033 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165344 ms - Host latency: 0.208362 ms (end to end 0.222009 ms, enqueue 0.150134 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163489 ms - Host latency: 0.207397 ms (end to end 0.221033 ms, enqueue 0.150415 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162793 ms - Host latency: 0.207043 ms (end to end 0.219507 ms, enqueue 0.146826 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154175 ms - Host latency: 0.194958 ms (end to end 0.278638 ms, enqueue 0.106921 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156042 ms - Host latency: 0.198096 ms (end to end 0.290283 ms, enqueue 0.108191 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.191565 ms - Host latency: 0.237634 ms (end to end 0.27644 ms, enqueue 0.202905 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19762 ms - Host latency: 0.243286 ms (end to end 0.255042 ms, enqueue 0.22876 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19585 ms - Host latency: 0.241455 ms (end to end 0.252759 ms, enqueue 0.22644 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19845 ms - Host latency: 0.244275 ms (end to end 0.254602 ms, enqueue 0.228271 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197034 ms - Host latency: 0.242712 ms (end to end 0.253809 ms, enqueue 0.227234 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.195386 ms - Host latency: 0.240881 ms (end to end 0.252551 ms, enqueue 0.22655 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.180908 ms - Host latency: 0.224329 ms (end to end 0.236414 ms, enqueue 0.182007 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164697 ms - Host latency: 0.208435 ms (end to end 0.220398 ms, enqueue 0.148401 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164465 ms - Host latency: 0.208472 ms (end to end 0.220483 ms, enqueue 0.148926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165369 ms - Host latency: 0.208154 ms (end to end 0.220496 ms, enqueue 0.149072 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164954 ms - Host latency: 0.208301 ms (end to end 0.219592 ms, enqueue 0.148792 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165002 ms - Host latency: 0.208862 ms (end to end 0.220203 ms, enqueue 0.149084 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155469 ms - Host latency: 0.196362 ms (end to end 0.26665 ms, enqueue 0.111426 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.195764 ms (end to end 0.287 ms, enqueue 0.10769 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155383 ms - Host latency: 0.196582 ms (end to end 0.287634 ms, enqueue 0.107153 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155322 ms - Host latency: 0.19657 ms (end to end 0.290735 ms, enqueue 0.107837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155212 ms - Host latency: 0.196216 ms (end to end 0.287354 ms, enqueue 0.107483 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155017 ms - Host latency: 0.196301 ms (end to end 0.28772 ms, enqueue 0.107727 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154358 ms - Host latency: 0.195361 ms (end to end 0.283667 ms, enqueue 0.0957031 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195898 ms (end to end 0.279785 ms, enqueue 0.0850464 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154138 ms - Host latency: 0.195276 ms (end to end 0.280066 ms, enqueue 0.085144 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195264 ms (end to end 0.279382 ms, enqueue 0.0845337 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195386 ms (end to end 0.279883 ms, enqueue 0.0844482 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195471 ms (end to end 0.279431 ms, enqueue 0.0841553 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.195996 ms (end to end 0.279883 ms, enqueue 0.0826782 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195715 ms (end to end 0.28009 ms, enqueue 0.0786377 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195312 ms (end to end 0.279395 ms, enqueue 0.0775024 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195508 ms (end to end 0.279736 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195508 ms (end to end 0.27876 ms, enqueue 0.0777588 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195471 ms (end to end 0.27948 ms, enqueue 0.0772705 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195532 ms (end to end 0.279443 ms, enqueue 0.0772827 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195483 ms (end to end 0.280725 ms, enqueue 0.0775513 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195386 ms (end to end 0.2797 ms, enqueue 0.0771606 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195789 ms (end to end 0.277661 ms, enqueue 0.0774292 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195398 ms (end to end 0.279285 ms, enqueue 0.0782959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195752 ms (end to end 0.280334 ms, enqueue 0.0787476 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195581 ms (end to end 0.281226 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195752 ms (end to end 0.280872 ms, enqueue 0.0783325 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195605 ms (end to end 0.280786 ms, enqueue 0.0778198 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155017 ms - Host latency: 0.196533 ms (end to end 0.281885 ms, enqueue 0.0772949 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195825 ms (end to end 0.282727 ms, enqueue 0.0775269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.207959 ms - Host latency: 0.254773 ms (end to end 0.269336 ms, enqueue 0.239966 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199072 ms - Host latency: 0.244519 ms (end to end 0.257117 ms, enqueue 0.229675 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199292 ms - Host latency: 0.244897 ms (end to end 0.256287 ms, enqueue 0.22948 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197583 ms - Host latency: 0.242944 ms (end to end 0.254297 ms, enqueue 0.228052 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19823 ms - Host latency: 0.243518 ms (end to end 0.255249 ms, enqueue 0.228857 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.176526 ms - Host latency: 0.221045 ms (end to end 0.23335 ms, enqueue 0.180371 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163489 ms - Host latency: 0.208545 ms (end to end 0.220508 ms, enqueue 0.150818 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163928 ms - Host latency: 0.208081 ms (end to end 0.221155 ms, enqueue 0.149597 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163806 ms - Host latency: 0.207605 ms (end to end 0.221155 ms, enqueue 0.150745 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164734 ms - Host latency: 0.209302 ms (end to end 0.221741 ms, enqueue 0.150037 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162036 ms - Host latency: 0.207104 ms (end to end 0.219958 ms, enqueue 0.148059 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.1953 ms (end to end 0.26344 ms, enqueue 0.104614 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195715 ms (end to end 0.283765 ms, enqueue 0.106616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155627 ms - Host latency: 0.196973 ms (end to end 0.260901 ms, enqueue 0.125793 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155237 ms - Host latency: 0.196326 ms (end to end 0.276147 ms, enqueue 0.11521 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155615 ms - Host latency: 0.197009 ms (end to end 0.286499 ms, enqueue 0.116077 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.189722 ms - Host latency: 0.234583 ms (end to end 0.297607 ms, enqueue 0.17738 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.223767 ms - Host latency: 0.269995 ms (end to end 0.279639 ms, enqueue 0.250952 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.220398 ms - Host latency: 0.266406 ms (end to end 0.276587 ms, enqueue 0.247302 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185815 ms - Host latency: 0.230237 ms (end to end 0.239001 ms, enqueue 0.214001 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157886 ms - Host latency: 0.201123 ms (end to end 0.208398 ms, enqueue 0.18678 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152869 ms - Host latency: 0.193457 ms (end to end 0.204431 ms, enqueue 0.174817 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152686 ms - Host latency: 0.193176 ms (end to end 0.203503 ms, enqueue 0.175293 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152722 ms - Host latency: 0.193066 ms (end to end 0.20415 ms, enqueue 0.174841 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.19657 ms (end to end 0.231995 ms, enqueue 0.133362 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.196143 ms (end to end 0.283655 ms, enqueue 0.124756 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155188 ms - Host latency: 0.196606 ms (end to end 0.293359 ms, enqueue 0.12467 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.208875 ms - Host latency: 0.256299 ms (end to end 0.315063 ms, enqueue 0.211658 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.2521 ms - Host latency: 0.304626 ms (end to end 0.313428 ms, enqueue 0.274866 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.262793 ms - Host latency: 0.315503 ms (end to end 0.324268 ms, enqueue 0.284802 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.250415 ms - Host latency: 0.302551 ms (end to end 0.311365 ms, enqueue 0.272559 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.232397 ms - Host latency: 0.282922 ms (end to end 0.291211 ms, enqueue 0.254773 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.188501 ms - Host latency: 0.234241 ms (end to end 0.24707 ms, enqueue 0.216333 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185681 ms - Host latency: 0.230676 ms (end to end 0.241724 ms, enqueue 0.215552 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181714 ms - Host latency: 0.226355 ms (end to end 0.237537 ms, enqueue 0.212659 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182251 ms - Host latency: 0.22749 ms (end to end 0.239526 ms, enqueue 0.206824 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163293 ms - Host latency: 0.206567 ms (end to end 0.220117 ms, enqueue 0.148767 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163989 ms - Host latency: 0.208643 ms (end to end 0.220044 ms, enqueue 0.149292 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164502 ms - Host latency: 0.207556 ms (end to end 0.219348 ms, enqueue 0.147729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165259 ms - Host latency: 0.208154 ms (end to end 0.219763 ms, enqueue 0.14751 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164282 ms - Host latency: 0.207666 ms (end to end 0.219153 ms, enqueue 0.147607 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158838 ms - Host latency: 0.200854 ms (end to end 0.23595 ms, enqueue 0.126379 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.227856 ms - Host latency: 0.27522 ms (end to end 0.302515 ms, enqueue 0.239404 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.232397 ms - Host latency: 0.27887 ms (end to end 0.288965 ms, enqueue 0.259399 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.232178 ms - Host latency: 0.278418 ms (end to end 0.289453 ms, enqueue 0.25979 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.191895 ms - Host latency: 0.236084 ms (end to end 0.244067 ms, enqueue 0.219312 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.193347 ms - Host latency: 0.236987 ms (end to end 0.252747 ms, enqueue 0.209949 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181958 ms - Host latency: 0.225891 ms (end to end 0.233813 ms, enqueue 0.209766 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.179443 ms - Host latency: 0.223889 ms (end to end 0.232092 ms, enqueue 0.208069 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16615 ms - Host latency: 0.209827 ms (end to end 0.223206 ms, enqueue 0.155591 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155652 ms - Host latency: 0.197986 ms (end to end 0.282495 ms, enqueue 0.118652 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.19646 ms (end to end 0.283582 ms, enqueue 0.117297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155066 ms - Host latency: 0.196277 ms (end to end 0.284216 ms, enqueue 0.117676 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154138 ms - Host latency: 0.195752 ms (end to end 0.287146 ms, enqueue 0.11825 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196277 ms (end to end 0.285449 ms, enqueue 0.117456 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195605 ms (end to end 0.283948 ms, enqueue 0.117102 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196155 ms (end to end 0.28269 ms, enqueue 0.10011 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195459 ms (end to end 0.277649 ms, enqueue 0.0976685 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195789 ms (end to end 0.283044 ms, enqueue 0.0969727 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.196069 ms (end to end 0.277515 ms, enqueue 0.0973389 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.196118 ms (end to end 0.277673 ms, enqueue 0.0963013 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196204 ms (end to end 0.280139 ms, enqueue 0.0963501 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.196021 ms (end to end 0.277283 ms, enqueue 0.0942871 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.19574 ms (end to end 0.272144 ms, enqueue 0.0839111 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155188 ms - Host latency: 0.198071 ms (end to end 0.2729 ms, enqueue 0.0868164 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196423 ms (end to end 0.276331 ms, enqueue 0.0832642 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196228 ms (end to end 0.273462 ms, enqueue 0.0832275 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196851 ms (end to end 0.27572 ms, enqueue 0.0846191 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.196204 ms (end to end 0.272156 ms, enqueue 0.0825928 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154919 ms - Host latency: 0.196228 ms (end to end 0.275513 ms, enqueue 0.0836182 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196228 ms (end to end 0.27439 ms, enqueue 0.0837646 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195715 ms (end to end 0.278699 ms, enqueue 0.0830444 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195923 ms (end to end 0.274841 ms, enqueue 0.0830566 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195984 ms (end to end 0.276807 ms, enqueue 0.0820679 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155017 ms - Host latency: 0.196619 ms (end to end 0.276453 ms, enqueue 0.0824219 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195837 ms (end to end 0.277161 ms, enqueue 0.0831665 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154602 ms - Host latency: 0.195813 ms (end to end 0.276917 ms, enqueue 0.0835815 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.19574 ms (end to end 0.274756 ms, enqueue 0.0824341 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195435 ms (end to end 0.277222 ms, enqueue 0.0815674 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195752 ms (end to end 0.277454 ms, enqueue 0.0816406 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154358 ms - Host latency: 0.195508 ms (end to end 0.276343 ms, enqueue 0.0831787 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195349 ms (end to end 0.276563 ms, enqueue 0.081189 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195911 ms (end to end 0.27738 ms, enqueue 0.0815063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.196973 ms (end to end 0.277429 ms, enqueue 0.0812866 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15592 ms - Host latency: 0.197449 ms (end to end 0.27771 ms, enqueue 0.0799194 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196851 ms (end to end 0.278284 ms, enqueue 0.0814087 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195532 ms (end to end 0.276855 ms, enqueue 0.0824219 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.196106 ms (end to end 0.276758 ms, enqueue 0.0804321 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154187 ms - Host latency: 0.195459 ms (end to end 0.276184 ms, enqueue 0.080835 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195984 ms (end to end 0.277234 ms, enqueue 0.080188 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.196167 ms (end to end 0.2776 ms, enqueue 0.0825317 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195996 ms (end to end 0.277063 ms, enqueue 0.0794067 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.19646 ms (end to end 0.277148 ms, enqueue 0.0793945 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195605 ms (end to end 0.276355 ms, enqueue 0.0801636 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.19679 ms (end to end 0.27832 ms, enqueue 0.07948 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196021 ms (end to end 0.277344 ms, enqueue 0.0797729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155347 ms - Host latency: 0.196826 ms (end to end 0.277576 ms, enqueue 0.0801147 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195935 ms (end to end 0.279065 ms, enqueue 0.0800293 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195947 ms (end to end 0.2771 ms, enqueue 0.0796509 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.196216 ms (end to end 0.277258 ms, enqueue 0.0785278 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195728 ms (end to end 0.270911 ms, enqueue 0.078772 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195874 ms (end to end 0.277148 ms, enqueue 0.0785522 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1547 ms - Host latency: 0.196118 ms (end to end 0.277917 ms, enqueue 0.0787354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155212 ms - Host latency: 0.197278 ms (end to end 0.279419 ms, enqueue 0.0811157 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195667 ms (end to end 0.279272 ms, enqueue 0.0786255 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195447 ms (end to end 0.276807 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154944 ms - Host latency: 0.19668 ms (end to end 0.276294 ms, enqueue 0.0788452 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196533 ms (end to end 0.2776 ms, enqueue 0.0784668 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195459 ms (end to end 0.274756 ms, enqueue 0.0784302 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154895 ms - Host latency: 0.195898 ms (end to end 0.277002 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.196936 ms (end to end 0.278113 ms, enqueue 0.0794556 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195935 ms (end to end 0.277466 ms, enqueue 0.0781494 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.196057 ms (end to end 0.279163 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196008 ms (end to end 0.276831 ms, enqueue 0.0778809 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.195959 ms (end to end 0.277917 ms, enqueue 0.0775391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195935 ms (end to end 0.277515 ms, enqueue 0.0792358 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196155 ms (end to end 0.275867 ms, enqueue 0.0775757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154773 ms - Host latency: 0.196326 ms (end to end 0.275879 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196216 ms (end to end 0.277795 ms, enqueue 0.0785278 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15509 ms - Host latency: 0.196802 ms (end to end 0.277295 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.19563 ms (end to end 0.277075 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195679 ms (end to end 0.274438 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.196033 ms (end to end 0.277917 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195764 ms (end to end 0.276514 ms, enqueue 0.0781738 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195593 ms (end to end 0.277954 ms, enqueue 0.077771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195776 ms (end to end 0.278479 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.196021 ms (end to end 0.27793 ms, enqueue 0.0785767 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195947 ms (end to end 0.279468 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195752 ms (end to end 0.278894 ms, enqueue 0.078186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195154 ms (end to end 0.279041 ms, enqueue 0.0776367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195715 ms (end to end 0.279297 ms, enqueue 0.0773193 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195361 ms (end to end 0.277637 ms, enqueue 0.0779419 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195862 ms (end to end 0.279419 ms, enqueue 0.0775146 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195312 ms (end to end 0.27771 ms, enqueue 0.0782837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196106 ms (end to end 0.279358 ms, enqueue 0.0774902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154797 ms - Host latency: 0.195972 ms (end to end 0.278833 ms, enqueue 0.0787109 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195605 ms (end to end 0.278479 ms, enqueue 0.0787354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.196021 ms (end to end 0.279114 ms, enqueue 0.0791138 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195422 ms (end to end 0.278491 ms, enqueue 0.0775024 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195569 ms (end to end 0.278528 ms, enqueue 0.0776123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195923 ms (end to end 0.279187 ms, enqueue 0.078894 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195752 ms (end to end 0.279858 ms, enqueue 0.0791138 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195911 ms (end to end 0.279419 ms, enqueue 0.0781616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154749 ms - Host latency: 0.195911 ms (end to end 0.279163 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195374 ms (end to end 0.277832 ms, enqueue 0.0780151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195715 ms (end to end 0.276111 ms, enqueue 0.0787598 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195312 ms (end to end 0.278296 ms, enqueue 0.0784058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.195874 ms (end to end 0.278162 ms, enqueue 0.0784058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195923 ms (end to end 0.278735 ms, enqueue 0.0774536 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195679 ms (end to end 0.278638 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.1953 ms (end to end 0.278467 ms, enqueue 0.0810303 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195764 ms (end to end 0.279272 ms, enqueue 0.0784058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.196057 ms (end to end 0.279297 ms, enqueue 0.0786499 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.19563 ms (end to end 0.278882 ms, enqueue 0.077478 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154334 ms - Host latency: 0.195337 ms (end to end 0.279456 ms, enqueue 0.0792114 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195911 ms (end to end 0.279346 ms, enqueue 0.0781372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195691 ms (end to end 0.280029 ms, enqueue 0.0792114 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195288 ms (end to end 0.277917 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195911 ms (end to end 0.279053 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154211 ms - Host latency: 0.195325 ms (end to end 0.277832 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195911 ms (end to end 0.278748 ms, enqueue 0.0787598 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195532 ms (end to end 0.278027 ms, enqueue 0.0785645 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.19519 ms (end to end 0.279407 ms, enqueue 0.0784302 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195483 ms (end to end 0.278223 ms, enqueue 0.0776245 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195203 ms (end to end 0.278784 ms, enqueue 0.078418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.195935 ms (end to end 0.270459 ms, enqueue 0.0775513 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195508 ms (end to end 0.279248 ms, enqueue 0.0776489 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.19613 ms (end to end 0.278662 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195752 ms (end to end 0.280542 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154382 ms - Host latency: 0.195435 ms (end to end 0.278613 ms, enqueue 0.0782593 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195557 ms (end to end 0.278699 ms, enqueue 0.07854 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195471 ms (end to end 0.278796 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195898 ms (end to end 0.280151 ms, enqueue 0.078418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195557 ms (end to end 0.279724 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195825 ms (end to end 0.279321 ms, enqueue 0.0777954 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195581 ms (end to end 0.280103 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195264 ms (end to end 0.279309 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195703 ms (end to end 0.279163 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154553 ms - Host latency: 0.195557 ms (end to end 0.279578 ms, enqueue 0.0775757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195398 ms (end to end 0.280225 ms, enqueue 0.0781616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195508 ms (end to end 0.279456 ms, enqueue 0.0786011 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195569 ms (end to end 0.279712 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195361 ms (end to end 0.279285 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195312 ms (end to end 0.279089 ms, enqueue 0.0776123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195679 ms (end to end 0.280066 ms, enqueue 0.0783691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195105 ms (end to end 0.278821 ms, enqueue 0.077478 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154285 ms - Host latency: 0.195422 ms (end to end 0.279968 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195337 ms (end to end 0.280103 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195825 ms (end to end 0.279578 ms, enqueue 0.0783691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.195374 ms (end to end 0.279712 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.195764 ms (end to end 0.279883 ms, enqueue 0.078833 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195569 ms (end to end 0.279834 ms, enqueue 0.0776489 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195361 ms (end to end 0.279529 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195166 ms (end to end 0.278992 ms, enqueue 0.0780151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195862 ms (end to end 0.2797 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.195764 ms (end to end 0.27533 ms, enqueue 0.078418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195923 ms (end to end 0.275146 ms, enqueue 0.0781982 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196411 ms (end to end 0.277759 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155017 ms - Host latency: 0.196765 ms (end to end 0.276306 ms, enqueue 0.0778809 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.195886 ms (end to end 0.276343 ms, enqueue 0.0774902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195715 ms (end to end 0.278284 ms, enqueue 0.0775879 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195715 ms (end to end 0.279407 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154578 ms - Host latency: 0.195447 ms (end to end 0.27981 ms, enqueue 0.0784424 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195874 ms (end to end 0.280383 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153931 ms - Host latency: 0.195154 ms (end to end 0.279065 ms, enqueue 0.078833 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195386 ms (end to end 0.27843 ms, enqueue 0.077356 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195483 ms (end to end 0.279773 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195276 ms (end to end 0.278931 ms, enqueue 0.0781616 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195715 ms (end to end 0.279553 ms, enqueue 0.0779175 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154504 ms - Host latency: 0.195508 ms (end to end 0.279224 ms, enqueue 0.0778687 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195801 ms (end to end 0.27948 ms, enqueue 0.0783569 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195129 ms (end to end 0.2797 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195923 ms (end to end 0.279456 ms, enqueue 0.0775269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154407 ms - Host latency: 0.195728 ms (end to end 0.27998 ms, enqueue 0.07854 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195349 ms (end to end 0.279334 ms, enqueue 0.0795776 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154236 ms - Host latency: 0.195544 ms (end to end 0.277222 ms, enqueue 0.0789673 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154431 ms - Host latency: 0.1953 ms (end to end 0.279614 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154724 ms - Host latency: 0.195654 ms (end to end 0.27998 ms, enqueue 0.0781128 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195471 ms (end to end 0.27926 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15426 ms - Host latency: 0.195215 ms (end to end 0.279102 ms, enqueue 0.0772705 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195215 ms (end to end 0.279773 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.19574 ms (end to end 0.279541 ms, enqueue 0.0775757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195691 ms (end to end 0.279517 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195422 ms (end to end 0.28009 ms, enqueue 0.0793091 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.19574 ms (end to end 0.279285 ms, enqueue 0.078186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195483 ms (end to end 0.279272 ms, enqueue 0.0784058 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154309 ms - Host latency: 0.195349 ms (end to end 0.279419 ms, enqueue 0.0778564 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154102 ms - Host latency: 0.195142 ms (end to end 0.278894 ms, enqueue 0.0773315 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153943 ms - Host latency: 0.19491 ms (end to end 0.277063 ms, enqueue 0.0780151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195679 ms (end to end 0.279871 ms, enqueue 0.0777466 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195752 ms (end to end 0.280005 ms, enqueue 0.0782105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195312 ms (end to end 0.2771 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15448 ms - Host latency: 0.195264 ms (end to end 0.279626 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154651 ms - Host latency: 0.195605 ms (end to end 0.279443 ms, enqueue 0.0783203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.195911 ms (end to end 0.279187 ms, enqueue 0.0776978 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154822 ms - Host latency: 0.196106 ms (end to end 0.279334 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154675 ms - Host latency: 0.195862 ms (end to end 0.275012 ms, enqueue 0.0776733 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154626 ms - Host latency: 0.195947 ms (end to end 0.275415 ms, enqueue 0.0778198 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195996 ms (end to end 0.275781 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196069 ms (end to end 0.274231 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154871 ms - Host latency: 0.196545 ms (end to end 0.276697 ms, enqueue 0.0772217 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154456 ms - Host latency: 0.195508 ms (end to end 0.274805 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154529 ms - Host latency: 0.195691 ms (end to end 0.27655 ms, enqueue 0.0778931 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195544 ms (end to end 0.276978 ms, enqueue 0.0784546 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196252 ms (end to end 0.278455 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196045 ms (end to end 0.278027 ms, enqueue 0.0775757 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196497 ms (end to end 0.277124 ms, enqueue 0.078186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196082 ms (end to end 0.276709 ms, enqueue 0.078064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195813 ms (end to end 0.279016 ms, enqueue 0.0787109 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195728 ms (end to end 0.278223 ms, enqueue 0.0772827 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195923 ms (end to end 0.27749 ms, enqueue 0.0776367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195679 ms (end to end 0.279053 ms, enqueue 0.0771729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195557 ms (end to end 0.27937 ms, enqueue 0.0773437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195703 ms (end to end 0.278076 ms, enqueue 0.078418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195801 ms (end to end 0.277686 ms, enqueue 0.0787598 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1552 ms - Host latency: 0.196484 ms (end to end 0.278955 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195801 ms (end to end 0.278784 ms, enqueue 0.0774902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195898 ms (end to end 0.277905 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195557 ms (end to end 0.277686 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195581 ms (end to end 0.279883 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195972 ms (end to end 0.279321 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196094 ms (end to end 0.277881 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195483 ms (end to end 0.279443 ms, enqueue 0.0777588 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195874 ms (end to end 0.279565 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195483 ms (end to end 0.27998 ms, enqueue 0.0784668 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195679 ms (end to end 0.278271 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195703 ms (end to end 0.27915 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195703 ms (end to end 0.279712 ms, enqueue 0.0775391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196118 ms (end to end 0.278271 ms, enqueue 0.0788574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195923 ms (end to end 0.278687 ms, enqueue 0.0797607 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.19563 ms (end to end 0.279443 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195557 ms (end to end 0.277759 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195972 ms (end to end 0.278296 ms, enqueue 0.0784424 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195776 ms (end to end 0.279663 ms, enqueue 0.0781494 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195435 ms (end to end 0.279541 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.19563 ms (end to end 0.280444 ms, enqueue 0.0779053 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.19563 ms (end to end 0.278613 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195923 ms (end to end 0.276733 ms, enqueue 0.0773437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195874 ms (end to end 0.277515 ms, enqueue 0.0797119 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195752 ms (end to end 0.276978 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.196118 ms (end to end 0.277637 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195947 ms (end to end 0.27981 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.19563 ms (end to end 0.278418 ms, enqueue 0.0788574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195728 ms (end to end 0.278198 ms, enqueue 0.0773682 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195825 ms (end to end 0.27666 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195752 ms (end to end 0.27832 ms, enqueue 0.0778564 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195776 ms (end to end 0.27832 ms, enqueue 0.0775391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196802 ms (end to end 0.278662 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195996 ms (end to end 0.27854 ms, enqueue 0.0787598 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195972 ms (end to end 0.270728 ms, enqueue 0.0781494 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196387 ms (end to end 0.278467 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195557 ms (end to end 0.277319 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196021 ms (end to end 0.278125 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195166 ms (end to end 0.277441 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196216 ms (end to end 0.277344 ms, enqueue 0.0778564 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195679 ms (end to end 0.278418 ms, enqueue 0.0774902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196069 ms (end to end 0.27771 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.19585 ms (end to end 0.276489 ms, enqueue 0.0782959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196021 ms (end to end 0.27854 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.19563 ms (end to end 0.277832 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.194897 ms (end to end 0.277881 ms, enqueue 0.0778564 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195898 ms (end to end 0.278125 ms, enqueue 0.0772217 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195068 ms (end to end 0.278857 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195825 ms (end to end 0.277563 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195337 ms (end to end 0.278394 ms, enqueue 0.0773437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195801 ms (end to end 0.278491 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195703 ms (end to end 0.279077 ms, enqueue 0.077417 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.195825 ms (end to end 0.277661 ms, enqueue 0.0795898 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195581 ms (end to end 0.278687 ms, enqueue 0.0790283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195703 ms (end to end 0.278955 ms, enqueue 0.0787842 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195532 ms (end to end 0.278735 ms, enqueue 0.0792236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195825 ms (end to end 0.281079 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.18252 ms - Host latency: 0.227393 ms (end to end 0.292554 ms, enqueue 0.140918 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.221924 ms - Host latency: 0.26792 ms (end to end 0.277637 ms, enqueue 0.245435 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.220361 ms - Host latency: 0.26626 ms (end to end 0.27561 ms, enqueue 0.24563 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.220142 ms - Host latency: 0.266138 ms (end to end 0.274438 ms, enqueue 0.245068 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171094 ms - Host latency: 0.22124 ms (end to end 0.229834 ms, enqueue 0.187451 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153027 ms - Host latency: 0.206934 ms (end to end 0.216895 ms, enqueue 0.165454 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152637 ms - Host latency: 0.204395 ms (end to end 0.21499 ms, enqueue 0.167358 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152905 ms - Host latency: 0.206592 ms (end to end 0.216968 ms, enqueue 0.164795 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152686 ms - Host latency: 0.20542 ms (end to end 0.215063 ms, enqueue 0.165039 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.200635 ms (end to end 0.222998 ms, enqueue 0.143921 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196582 ms (end to end 0.28916 ms, enqueue 0.117017 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196313 ms (end to end 0.288354 ms, enqueue 0.117993 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155347 ms - Host latency: 0.196973 ms (end to end 0.28999 ms, enqueue 0.117578 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195801 ms (end to end 0.288477 ms, enqueue 0.11792 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196533 ms (end to end 0.290186 ms, enqueue 0.119116 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196338 ms (end to end 0.289893 ms, enqueue 0.114478 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154102 ms - Host latency: 0.195215 ms (end to end 0.280298 ms, enqueue 0.0878662 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195483 ms (end to end 0.281689 ms, enqueue 0.0872559 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195898 ms (end to end 0.2823 ms, enqueue 0.0873047 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195752 ms (end to end 0.282983 ms, enqueue 0.0875488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195825 ms (end to end 0.282813 ms, enqueue 0.0871582 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195679 ms (end to end 0.281909 ms, enqueue 0.0869629 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196289 ms (end to end 0.282056 ms, enqueue 0.083667 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195947 ms (end to end 0.27998 ms, enqueue 0.0817627 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.179492 ms - Host latency: 0.224634 ms (end to end 0.264331 ms, enqueue 0.184399 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183252 ms - Host latency: 0.227783 ms (end to end 0.238477 ms, enqueue 0.212695 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182227 ms - Host latency: 0.226514 ms (end to end 0.236792 ms, enqueue 0.211768 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171045 ms - Host latency: 0.216211 ms (end to end 0.223364 ms, enqueue 0.19895 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170068 ms - Host latency: 0.214282 ms (end to end 0.220728 ms, enqueue 0.197363 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170923 ms - Host latency: 0.214868 ms (end to end 0.222461 ms, enqueue 0.198755 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171924 ms - Host latency: 0.216064 ms (end to end 0.223584 ms, enqueue 0.199902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159009 ms - Host latency: 0.200073 ms (end to end 0.214111 ms, enqueue 0.139771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15686 ms - Host latency: 0.198779 ms (end to end 0.218433 ms, enqueue 0.133447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156787 ms - Host latency: 0.197632 ms (end to end 0.217212 ms, enqueue 0.133374 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156714 ms - Host latency: 0.197876 ms (end to end 0.216406 ms, enqueue 0.133154 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157324 ms - Host latency: 0.198437 ms (end to end 0.214746 ms, enqueue 0.13418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156372 ms - Host latency: 0.197339 ms (end to end 0.215283 ms, enqueue 0.132397 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155591 ms - Host latency: 0.196924 ms (end to end 0.242017 ms, enqueue 0.112427 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155054 ms - Host latency: 0.196655 ms (end to end 0.282031 ms, enqueue 0.0983887 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195825 ms (end to end 0.279541 ms, enqueue 0.0988281 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.197168 ms (end to end 0.281445 ms, enqueue 0.100293 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196973 ms (end to end 0.282617 ms, enqueue 0.0984131 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195728 ms (end to end 0.282715 ms, enqueue 0.0997803 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196289 ms (end to end 0.279712 ms, enqueue 0.095166 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195898 ms (end to end 0.274316 ms, enqueue 0.0788818 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195288 ms (end to end 0.273706 ms, enqueue 0.0792725 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196655 ms (end to end 0.277954 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195947 ms (end to end 0.277832 ms, enqueue 0.0785645 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.196045 ms (end to end 0.276147 ms, enqueue 0.0798584 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196338 ms (end to end 0.277368 ms, enqueue 0.079248 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.19668 ms (end to end 0.278906 ms, enqueue 0.0791504 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.224097 ms - Host latency: 0.272144 ms (end to end 0.309546 ms, enqueue 0.215186 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.337036 ms - Host latency: 0.391138 ms (end to end 0.403052 ms, enqueue 0.359277 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.312085 ms - Host latency: 0.364697 ms (end to end 0.376587 ms, enqueue 0.3354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.186133 ms - Host latency: 0.229443 ms (end to end 0.275342 ms, enqueue 0.157153 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196069 ms (end to end 0.281152 ms, enqueue 0.10105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196753 ms (end to end 0.284155 ms, enqueue 0.10105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196826 ms (end to end 0.282837 ms, enqueue 0.0990967 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196973 ms (end to end 0.285327 ms, enqueue 0.0997803 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196216 ms (end to end 0.283325 ms, enqueue 0.108765 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196948 ms (end to end 0.282837 ms, enqueue 0.113721 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.19563 ms (end to end 0.28667 ms, enqueue 0.112769 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196509 ms (end to end 0.284814 ms, enqueue 0.112842 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196069 ms (end to end 0.286011 ms, enqueue 0.11189 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195776 ms (end to end 0.284082 ms, enqueue 0.112622 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195776 ms (end to end 0.28418 ms, enqueue 0.108301 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155298 ms - Host latency: 0.197388 ms (end to end 0.278125 ms, enqueue 0.0917236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195801 ms (end to end 0.278467 ms, enqueue 0.0924805 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196338 ms (end to end 0.277148 ms, enqueue 0.0917236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.196069 ms (end to end 0.278125 ms, enqueue 0.0941895 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195654 ms (end to end 0.276538 ms, enqueue 0.0925537 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195947 ms (end to end 0.276953 ms, enqueue 0.092041 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195337 ms (end to end 0.275293 ms, enqueue 0.08125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154077 ms - Host latency: 0.195264 ms (end to end 0.272485 ms, enqueue 0.0790039 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195386 ms (end to end 0.271313 ms, enqueue 0.0798584 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195874 ms (end to end 0.26875 ms, enqueue 0.0798584 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195605 ms (end to end 0.274097 ms, enqueue 0.0793945 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153931 ms - Host latency: 0.194922 ms (end to end 0.271411 ms, enqueue 0.0794189 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195972 ms (end to end 0.274146 ms, enqueue 0.079248 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195654 ms (end to end 0.27688 ms, enqueue 0.0793213 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.196729 ms (end to end 0.279419 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196069 ms (end to end 0.277344 ms, enqueue 0.0786377 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195728 ms (end to end 0.277686 ms, enqueue 0.0786377 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196533 ms (end to end 0.278564 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196191 ms (end to end 0.278589 ms, enqueue 0.079126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196094 ms (end to end 0.278931 ms, enqueue 0.0793213 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196509 ms (end to end 0.278564 ms, enqueue 0.0788086 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.172876 ms - Host latency: 0.21687 ms (end to end 0.287036 ms, enqueue 0.118872 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200952 ms - Host latency: 0.246338 ms (end to end 0.256738 ms, enqueue 0.229663 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158521 ms - Host latency: 0.200635 ms (end to end 0.268774 ms, enqueue 0.0906006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.196045 ms (end to end 0.281274 ms, enqueue 0.0940918 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196118 ms (end to end 0.28457 ms, enqueue 0.100391 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155249 ms - Host latency: 0.196997 ms (end to end 0.284302 ms, enqueue 0.100488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196338 ms (end to end 0.285034 ms, enqueue 0.0997559 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196094 ms (end to end 0.283496 ms, enqueue 0.0999023 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196069 ms (end to end 0.281885 ms, enqueue 0.100464 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.19668 ms (end to end 0.284399 ms, enqueue 0.0962646 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195923 ms (end to end 0.278784 ms, enqueue 0.0851318 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195996 ms (end to end 0.278174 ms, enqueue 0.0848389 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196143 ms (end to end 0.278149 ms, enqueue 0.0856445 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195996 ms (end to end 0.278369 ms, enqueue 0.0862549 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195947 ms (end to end 0.279004 ms, enqueue 0.0855713 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196216 ms (end to end 0.280347 ms, enqueue 0.0851562 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.196484 ms (end to end 0.278174 ms, enqueue 0.0794922 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.19563 ms (end to end 0.277856 ms, enqueue 0.0785156 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.195898 ms (end to end 0.278589 ms, enqueue 0.079248 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195874 ms (end to end 0.278613 ms, enqueue 0.0783447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154028 ms - Host latency: 0.194971 ms (end to end 0.278467 ms, enqueue 0.0773193 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.19563 ms (end to end 0.278223 ms, enqueue 0.0771729 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195459 ms (end to end 0.277686 ms, enqueue 0.0783203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195679 ms (end to end 0.279932 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196289 ms (end to end 0.281299 ms, enqueue 0.0773437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195581 ms (end to end 0.280469 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195605 ms (end to end 0.279419 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196289 ms (end to end 0.27998 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195874 ms (end to end 0.28103 ms, enqueue 0.0786133 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195654 ms (end to end 0.280737 ms, enqueue 0.0784912 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195459 ms (end to end 0.27937 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195898 ms (end to end 0.280981 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195459 ms (end to end 0.279785 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195874 ms (end to end 0.279858 ms, enqueue 0.077124 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195581 ms (end to end 0.279468 ms, enqueue 0.0769531 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195752 ms (end to end 0.278369 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195288 ms (end to end 0.27981 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.19563 ms (end to end 0.279834 ms, enqueue 0.0776123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195532 ms (end to end 0.279907 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195459 ms (end to end 0.279004 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195874 ms (end to end 0.279346 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195361 ms (end to end 0.279224 ms, enqueue 0.0773682 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195898 ms (end to end 0.279663 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195361 ms (end to end 0.279883 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195532 ms (end to end 0.279468 ms, enqueue 0.0770996 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195532 ms (end to end 0.27981 ms, enqueue 0.0783203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195044 ms (end to end 0.279053 ms, enqueue 0.0776367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195679 ms (end to end 0.280347 ms, enqueue 0.0778809 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196021 ms (end to end 0.279419 ms, enqueue 0.078125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196069 ms (end to end 0.279687 ms, enqueue 0.0775879 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195264 ms (end to end 0.278613 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195654 ms (end to end 0.279736 ms, enqueue 0.0771973 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.19563 ms (end to end 0.279663 ms, enqueue 0.0823486 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195679 ms (end to end 0.279663 ms, enqueue 0.0783447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196265 ms (end to end 0.279443 ms, enqueue 0.0782715 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195459 ms (end to end 0.277686 ms, enqueue 0.0780029 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195801 ms (end to end 0.278809 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196045 ms (end to end 0.279175 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.196191 ms (end to end 0.280005 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195117 ms (end to end 0.278662 ms, enqueue 0.0773437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195288 ms (end to end 0.27832 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196118 ms (end to end 0.280151 ms, enqueue 0.0786377 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195483 ms (end to end 0.278955 ms, enqueue 0.077417 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195728 ms (end to end 0.279565 ms, enqueue 0.0781738 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195581 ms (end to end 0.277148 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195703 ms (end to end 0.278247 ms, enqueue 0.0774658 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.19541 ms (end to end 0.279321 ms, enqueue 0.0770752 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195239 ms (end to end 0.278735 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195923 ms (end to end 0.278589 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195654 ms (end to end 0.279834 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195825 ms (end to end 0.279224 ms, enqueue 0.0772949 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195874 ms (end to end 0.27876 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195166 ms (end to end 0.279321 ms, enqueue 0.0782227 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.19519 ms (end to end 0.278833 ms, enqueue 0.0776123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195776 ms (end to end 0.279858 ms, enqueue 0.0783691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195679 ms (end to end 0.277441 ms, enqueue 0.0776855 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195679 ms (end to end 0.278174 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195557 ms (end to end 0.279028 ms, enqueue 0.0776367 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.195361 ms (end to end 0.277051 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195898 ms (end to end 0.276855 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195435 ms (end to end 0.279102 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195801 ms (end to end 0.278564 ms, enqueue 0.0801758 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195361 ms (end to end 0.277441 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195679 ms (end to end 0.270044 ms, enqueue 0.0775635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195557 ms (end to end 0.288257 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.195898 ms (end to end 0.288867 ms, enqueue 0.0780273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196094 ms (end to end 0.287207 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196143 ms (end to end 0.288037 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195996 ms (end to end 0.288208 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.195972 ms (end to end 0.288208 ms, enqueue 0.0774902 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195679 ms (end to end 0.287646 ms, enqueue 0.0773682 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.190308 ms - Host latency: 0.235059 ms (end to end 0.286572 ms, enqueue 0.164502 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200928 ms - Host latency: 0.245972 ms (end to end 0.25686 ms, enqueue 0.230273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.196704 ms - Host latency: 0.241821 ms (end to end 0.254126 ms, enqueue 0.227832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.218091 ms - Host latency: 0.264771 ms (end to end 0.275488 ms, enqueue 0.24624 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199634 ms - Host latency: 0.245142 ms (end to end 0.256616 ms, enqueue 0.229883 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163037 ms - Host latency: 0.205493 ms (end to end 0.217432 ms, enqueue 0.188013 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152954 ms - Host latency: 0.194043 ms (end to end 0.204443 ms, enqueue 0.174927 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152832 ms - Host latency: 0.193164 ms (end to end 0.204199 ms, enqueue 0.175488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152637 ms - Host latency: 0.193359 ms (end to end 0.204565 ms, enqueue 0.175024 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152637 ms - Host latency: 0.193359 ms (end to end 0.204614 ms, enqueue 0.173804 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158276 ms - Host latency: 0.202393 ms (end to end 0.223218 ms, enqueue 0.143604 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.160156 ms - Host latency: 0.201611 ms (end to end 0.21272 ms, enqueue 0.138745 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159155 ms - Host latency: 0.200757 ms (end to end 0.212524 ms, enqueue 0.13877 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158838 ms - Host latency: 0.19978 ms (end to end 0.211719 ms, enqueue 0.137305 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158496 ms - Host latency: 0.199536 ms (end to end 0.210156 ms, enqueue 0.136743 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159058 ms - Host latency: 0.200098 ms (end to end 0.211377 ms, enqueue 0.137427 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15603 ms - Host latency: 0.197314 ms (end to end 0.253052 ms, enqueue 0.116992 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.197021 ms (end to end 0.283081 ms, enqueue 0.113208 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195898 ms (end to end 0.281396 ms, enqueue 0.113281 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153931 ms - Host latency: 0.194751 ms (end to end 0.27771 ms, enqueue 0.110498 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195679 ms (end to end 0.280103 ms, enqueue 0.10686 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195679 ms (end to end 0.277417 ms, enqueue 0.105908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.19624 ms (end to end 0.278149 ms, enqueue 0.0975342 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156177 ms - Host latency: 0.197632 ms (end to end 0.22998 ms, enqueue 0.133447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196118 ms (end to end 0.264062 ms, enqueue 0.127856 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196216 ms (end to end 0.293042 ms, enqueue 0.128882 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196558 ms (end to end 0.293433 ms, enqueue 0.128662 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.195996 ms (end to end 0.294849 ms, enqueue 0.128149 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.195825 ms (end to end 0.287866 ms, enqueue 0.129199 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.196313 ms (end to end 0.279077 ms, enqueue 0.0999268 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195532 ms (end to end 0.281738 ms, enqueue 0.0967773 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1552 ms - Host latency: 0.197168 ms (end to end 0.281299 ms, enqueue 0.0962402 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196338 ms (end to end 0.282056 ms, enqueue 0.0966309 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.196094 ms (end to end 0.27959 ms, enqueue 0.0968994 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196655 ms (end to end 0.283032 ms, enqueue 0.0959717 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196826 ms (end to end 0.279956 ms, enqueue 0.0863281 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196191 ms (end to end 0.278589 ms, enqueue 0.0792236 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.196436 ms (end to end 0.277783 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196045 ms (end to end 0.27876 ms, enqueue 0.0788818 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.19646 ms (end to end 0.276636 ms, enqueue 0.0793945 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195923 ms (end to end 0.278271 ms, enqueue 0.0796387 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196167 ms (end to end 0.28208 ms, enqueue 0.0795898 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.19519 ms (end to end 0.282739 ms, enqueue 0.0789063 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195654 ms (end to end 0.281372 ms, enqueue 0.0793457 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196411 ms (end to end 0.285107 ms, enqueue 0.0781494 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.196924 ms (end to end 0.284937 ms, enqueue 0.078833 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196411 ms (end to end 0.284717 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196802 ms (end to end 0.284985 ms, enqueue 0.0779297 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195264 ms (end to end 0.280151 ms, enqueue 0.0781982 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.19585 ms (end to end 0.28042 ms, enqueue 0.0783936 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.195801 ms (end to end 0.281519 ms, enqueue 0.0797607 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195898 ms (end to end 0.280542 ms, enqueue 0.0803955 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196045 ms (end to end 0.280884 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195972 ms (end to end 0.28208 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.196045 ms (end to end 0.28208 ms, enqueue 0.0774414 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196338 ms (end to end 0.280396 ms, enqueue 0.0787842 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195386 ms (end to end 0.279541 ms, enqueue 0.0798584 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.19563 ms (end to end 0.280615 ms, enqueue 0.0780518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195581 ms (end to end 0.280029 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195142 ms (end to end 0.279883 ms, enqueue 0.0777588 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195825 ms (end to end 0.279541 ms, enqueue 0.0773926 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195459 ms (end to end 0.279199 ms, enqueue 0.0787842 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195312 ms (end to end 0.27998 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195728 ms (end to end 0.279639 ms, enqueue 0.0781738 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195679 ms (end to end 0.279541 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195605 ms (end to end 0.280005 ms, enqueue 0.07771 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.192188 ms - Host latency: 0.237695 ms (end to end 0.279663 ms, enqueue 0.180273 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.192603 ms - Host latency: 0.237061 ms (end to end 0.250146 ms, enqueue 0.204907 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196387 ms (end to end 0.277075 ms, enqueue 0.0803955 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196021 ms (end to end 0.277441 ms, enqueue 0.0791748 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195654 ms (end to end 0.276147 ms, enqueue 0.0805664 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196387 ms (end to end 0.277368 ms, enqueue 0.0797607 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196265 ms (end to end 0.277148 ms, enqueue 0.0800781 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196338 ms (end to end 0.281128 ms, enqueue 0.0831543 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195996 ms (end to end 0.281494 ms, enqueue 0.0835693 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195776 ms (end to end 0.279297 ms, enqueue 0.0834961 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196924 ms (end to end 0.281982 ms, enqueue 0.0828857 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196777 ms (end to end 0.280957 ms, enqueue 0.0829834 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195898 ms (end to end 0.280737 ms, enqueue 0.0829102 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155176 ms - Host latency: 0.196802 ms (end to end 0.281909 ms, enqueue 0.0831543 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195117 ms (end to end 0.281494 ms, enqueue 0.0791748 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195459 ms (end to end 0.281274 ms, enqueue 0.0791992 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154272 ms - Host latency: 0.195703 ms (end to end 0.280981 ms, enqueue 0.0794189 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195264 ms (end to end 0.280591 ms, enqueue 0.0796875 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196069 ms (end to end 0.28064 ms, enqueue 0.0795654 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195459 ms (end to end 0.279956 ms, enqueue 0.0804932 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.195874 ms (end to end 0.279517 ms, enqueue 0.0781006 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195557 ms (end to end 0.280249 ms, enqueue 0.0780762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.19563 ms (end to end 0.279736 ms, enqueue 0.0783447 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195923 ms (end to end 0.278711 ms, enqueue 0.077417 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.19541 ms (end to end 0.280078 ms, enqueue 0.0779541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195581 ms (end to end 0.279932 ms, enqueue 0.0778076 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195679 ms (end to end 0.279395 ms, enqueue 0.0777344 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195654 ms (end to end 0.279077 ms, enqueue 0.0783203 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195361 ms (end to end 0.278784 ms, enqueue 0.0780029 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.167456 ms - Host latency: 0.210278 ms (end to end 0.289136 ms, enqueue 0.10332 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.219653 ms - Host latency: 0.266406 ms (end to end 0.276489 ms, enqueue 0.245288 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.219971 ms - Host latency: 0.265918 ms (end to end 0.274585 ms, enqueue 0.244653 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.29585 ms - Host latency: 0.346704 ms (end to end 0.356763 ms, enqueue 0.318335 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.245239 ms - Host latency: 0.293604 ms (end to end 0.30603 ms, enqueue 0.272388 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183154 ms - Host latency: 0.227954 ms (end to end 0.240332 ms, enqueue 0.185278 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162988 ms - Host latency: 0.206689 ms (end to end 0.22146 ms, enqueue 0.150439 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164502 ms - Host latency: 0.208691 ms (end to end 0.220996 ms, enqueue 0.150122 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163818 ms - Host latency: 0.209009 ms (end to end 0.221289 ms, enqueue 0.15083 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165259 ms - Host latency: 0.210718 ms (end to end 0.222974 ms, enqueue 0.149585 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161377 ms - Host latency: 0.205054 ms (end to end 0.215649 ms, enqueue 0.149927 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158423 ms - Host latency: 0.208374 ms (end to end 0.224146 ms, enqueue 0.15437 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157251 ms - Host latency: 0.208521 ms (end to end 0.221729 ms, enqueue 0.159229 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158203 ms - Host latency: 0.210059 ms (end to end 0.223755 ms, enqueue 0.157837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156055 ms - Host latency: 0.209326 ms (end to end 0.2198 ms, enqueue 0.164185 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155884 ms - Host latency: 0.208911 ms (end to end 0.221973 ms, enqueue 0.159009 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.199683 ms (end to end 0.248877 ms, enqueue 0.126147 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195532 ms (end to end 0.281421 ms, enqueue 0.112646 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196045 ms (end to end 0.285132 ms, enqueue 0.112964 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.19624 ms (end to end 0.284424 ms, enqueue 0.113574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196094 ms (end to end 0.283691 ms, enqueue 0.113867 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195874 ms (end to end 0.279785 ms, enqueue 0.112866 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.19541 ms (end to end 0.282544 ms, enqueue 0.110889 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.195508 ms (end to end 0.268774 ms, enqueue 0.0880859 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.19563 ms (end to end 0.273364 ms, enqueue 0.0888916 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195483 ms (end to end 0.275244 ms, enqueue 0.0883057 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.195605 ms (end to end 0.277173 ms, enqueue 0.0874512 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195923 ms (end to end 0.277393 ms, enqueue 0.0896484 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153882 ms - Host latency: 0.194775 ms (end to end 0.271338 ms, enqueue 0.0880371 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153857 ms - Host latency: 0.195068 ms (end to end 0.274121 ms, enqueue 0.0820801 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153809 ms - Host latency: 0.195142 ms (end to end 0.244702 ms, enqueue 0.0807617 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196411 ms (end to end 0.242773 ms, enqueue 0.0786865 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196045 ms (end to end 0.252368 ms, enqueue 0.0855713 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154297 ms - Host latency: 0.195581 ms (end to end 0.280957 ms, enqueue 0.0947754 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.196777 ms (end to end 0.282666 ms, enqueue 0.100464 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196509 ms (end to end 0.281738 ms, enqueue 0.101343 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155298 ms - Host latency: 0.19729 ms (end to end 0.283325 ms, enqueue 0.100635 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196509 ms (end to end 0.283081 ms, enqueue 0.100708 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.196118 ms (end to end 0.28335 ms, enqueue 0.100659 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170044 ms - Host latency: 0.213086 ms (end to end 0.292188 ms, enqueue 0.121997 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.214038 ms - Host latency: 0.260693 ms (end to end 0.271704 ms, enqueue 0.240259 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199829 ms - Host latency: 0.24519 ms (end to end 0.257153 ms, enqueue 0.230298 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.197388 ms - Host latency: 0.242603 ms (end to end 0.254785 ms, enqueue 0.228101 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181763 ms - Host latency: 0.22627 ms (end to end 0.23501 ms, enqueue 0.209985 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.17627 ms - Host latency: 0.222754 ms (end to end 0.231592 ms, enqueue 0.204419 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171021 ms - Host latency: 0.215137 ms (end to end 0.22251 ms, enqueue 0.198535 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171411 ms - Host latency: 0.21582 ms (end to end 0.222778 ms, enqueue 0.19917 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163696 ms - Host latency: 0.206104 ms (end to end 0.217725 ms, enqueue 0.16001 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162622 ms - Host latency: 0.209009 ms (end to end 0.224023 ms, enqueue 0.158301 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164819 ms - Host latency: 0.207422 ms (end to end 0.221631 ms, enqueue 0.150806 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.186694 ms - Host latency: 0.232373 ms (end to end 0.243945 ms, enqueue 0.200195 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.182764 ms - Host latency: 0.227759 ms (end to end 0.237769 ms, enqueue 0.212573 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.176172 ms - Host latency: 0.220508 ms (end to end 0.229028 ms, enqueue 0.205151 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.17395 ms - Host latency: 0.21897 ms (end to end 0.226758 ms, enqueue 0.20166 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.173779 ms - Host latency: 0.217896 ms (end to end 0.225977 ms, enqueue 0.202222 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.172485 ms - Host latency: 0.216431 ms (end to end 0.224048 ms, enqueue 0.200708 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170605 ms - Host latency: 0.214746 ms (end to end 0.222998 ms, enqueue 0.192041 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.160498 ms - Host latency: 0.203003 ms (end to end 0.215527 ms, enqueue 0.141235 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161694 ms - Host latency: 0.203467 ms (end to end 0.215039 ms, enqueue 0.141211 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.160815 ms - Host latency: 0.202808 ms (end to end 0.215894 ms, enqueue 0.139844 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161108 ms - Host latency: 0.202319 ms (end to end 0.214404 ms, enqueue 0.139893 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161279 ms - Host latency: 0.202441 ms (end to end 0.215015 ms, enqueue 0.140747 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159814 ms - Host latency: 0.201489 ms (end to end 0.215918 ms, enqueue 0.131982 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155396 ms - Host latency: 0.197144 ms (end to end 0.283105 ms, enqueue 0.104126 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.196191 ms (end to end 0.283691 ms, enqueue 0.102783 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196167 ms (end to end 0.28479 ms, enqueue 0.102881 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.196021 ms (end to end 0.28396 ms, enqueue 0.103442 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195312 ms (end to end 0.284546 ms, enqueue 0.102612 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155298 ms - Host latency: 0.196436 ms (end to end 0.286084 ms, enqueue 0.102563 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.197583 ms (end to end 0.279443 ms, enqueue 0.0909912 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195703 ms (end to end 0.272461 ms, enqueue 0.0851074 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153687 ms - Host latency: 0.19502 ms (end to end 0.269556 ms, enqueue 0.0859375 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153833 ms - Host latency: 0.194849 ms (end to end 0.270117 ms, enqueue 0.0855713 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.19624 ms (end to end 0.272192 ms, enqueue 0.0860352 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153516 ms - Host latency: 0.194922 ms (end to end 0.268994 ms, enqueue 0.0852783 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155786 ms - Host latency: 0.197217 ms (end to end 0.274365 ms, enqueue 0.085376 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.196216 ms (end to end 0.2729 ms, enqueue 0.0824463 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.197461 ms (end to end 0.276099 ms, enqueue 0.0819092 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195825 ms (end to end 0.274756 ms, enqueue 0.0810791 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195996 ms (end to end 0.273926 ms, enqueue 0.0819092 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195679 ms (end to end 0.27666 ms, enqueue 0.0809082 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154126 ms - Host latency: 0.194995 ms (end to end 0.276074 ms, enqueue 0.0805908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155835 ms - Host latency: 0.197803 ms (end to end 0.2771 ms, enqueue 0.0815918 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.197217 ms (end to end 0.277686 ms, enqueue 0.0813477 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196875 ms (end to end 0.278052 ms, enqueue 0.0800781 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1552 ms - Host latency: 0.197119 ms (end to end 0.27727 ms, enqueue 0.0798096 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.196118 ms (end to end 0.267505 ms, enqueue 0.0809814 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196558 ms (end to end 0.27937 ms, enqueue 0.0804932 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155225 ms - Host latency: 0.196777 ms (end to end 0.27854 ms, enqueue 0.0804932 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154468 ms - Host latency: 0.195581 ms (end to end 0.277051 ms, enqueue 0.0810303 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155737 ms - Host latency: 0.197046 ms (end to end 0.278906 ms, enqueue 0.0799561 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196582 ms (end to end 0.279248 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.179126 ms - Host latency: 0.223389 ms (end to end 0.286694 ms, enqueue 0.136255 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.217627 ms - Host latency: 0.264893 ms (end to end 0.276074 ms, enqueue 0.2448 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.203247 ms - Host latency: 0.249365 ms (end to end 0.25918 ms, enqueue 0.231274 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.201416 ms - Host latency: 0.247217 ms (end to end 0.257715 ms, enqueue 0.230029 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.20144 ms - Host latency: 0.246704 ms (end to end 0.258398 ms, enqueue 0.231079 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.167822 ms - Host latency: 0.217261 ms (end to end 0.228418 ms, enqueue 0.18623 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152979 ms - Host latency: 0.201538 ms (end to end 0.211841 ms, enqueue 0.169604 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152832 ms - Host latency: 0.202881 ms (end to end 0.213672 ms, enqueue 0.167407 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152612 ms - Host latency: 0.202734 ms (end to end 0.213525 ms, enqueue 0.167798 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152759 ms - Host latency: 0.203125 ms (end to end 0.214185 ms, enqueue 0.167578 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.19834 ms (end to end 0.239063 ms, enqueue 0.133276 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15542 ms - Host latency: 0.196948 ms (end to end 0.290186 ms, enqueue 0.119971 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196313 ms (end to end 0.285645 ms, enqueue 0.119263 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181885 ms - Host latency: 0.227954 ms (end to end 0.264551 ms, enqueue 0.206128 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.186865 ms - Host latency: 0.232153 ms (end to end 0.242041 ms, enqueue 0.215576 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.183911 ms - Host latency: 0.229077 ms (end to end 0.239697 ms, enqueue 0.21355 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.189136 ms - Host latency: 0.234473 ms (end to end 0.244067 ms, enqueue 0.217603 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.186914 ms - Host latency: 0.232324 ms (end to end 0.241797 ms, enqueue 0.214697 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.185571 ms - Host latency: 0.230566 ms (end to end 0.240161 ms, enqueue 0.214233 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.171851 ms - Host latency: 0.215356 ms (end to end 0.227051 ms, enqueue 0.168359 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164429 ms - Host latency: 0.208887 ms (end to end 0.220972 ms, enqueue 0.148438 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164771 ms - Host latency: 0.208447 ms (end to end 0.220044 ms, enqueue 0.147656 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165771 ms - Host latency: 0.208325 ms (end to end 0.220142 ms, enqueue 0.146826 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.165845 ms - Host latency: 0.209204 ms (end to end 0.220557 ms, enqueue 0.147241 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164062 ms - Host latency: 0.206763 ms (end to end 0.219312 ms, enqueue 0.140601 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196533 ms (end to end 0.279346 ms, enqueue 0.10625 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155786 ms - Host latency: 0.197534 ms (end to end 0.264111 ms, enqueue 0.1125 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196338 ms (end to end 0.28313 ms, enqueue 0.108008 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.195996 ms (end to end 0.280762 ms, enqueue 0.11189 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.19585 ms (end to end 0.279541 ms, enqueue 0.107739 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195801 ms (end to end 0.279053 ms, enqueue 0.10686 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.196045 ms (end to end 0.274414 ms, enqueue 0.0913086 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196509 ms (end to end 0.264258 ms, enqueue 0.085791 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153931 ms - Host latency: 0.195142 ms (end to end 0.271094 ms, enqueue 0.0865967 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195728 ms (end to end 0.270361 ms, enqueue 0.0869873 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154102 ms - Host latency: 0.195508 ms (end to end 0.268823 ms, enqueue 0.086377 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196338 ms (end to end 0.273169 ms, enqueue 0.0873779 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154492 ms - Host latency: 0.196289 ms (end to end 0.272632 ms, enqueue 0.0867432 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196216 ms (end to end 0.273145 ms, enqueue 0.0866211 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155566 ms - Host latency: 0.198096 ms (end to end 0.275659 ms, enqueue 0.087207 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15437 ms - Host latency: 0.195776 ms (end to end 0.273291 ms, enqueue 0.0834717 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196484 ms (end to end 0.27417 ms, enqueue 0.0842773 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196265 ms (end to end 0.273022 ms, enqueue 0.0838135 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196313 ms (end to end 0.273804 ms, enqueue 0.0833008 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196533 ms (end to end 0.277026 ms, enqueue 0.083667 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.196973 ms (end to end 0.277002 ms, enqueue 0.0836914 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195679 ms (end to end 0.277686 ms, enqueue 0.0852539 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196167 ms (end to end 0.277148 ms, enqueue 0.0830811 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195605 ms (end to end 0.276465 ms, enqueue 0.0823486 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195508 ms (end to end 0.277319 ms, enqueue 0.0828613 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.216919 ms - Host latency: 0.264697 ms (end to end 0.280933 ms, enqueue 0.231567 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.203076 ms - Host latency: 0.248633 ms (end to end 0.259375 ms, enqueue 0.231519 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.22168 ms - Host latency: 0.26853 ms (end to end 0.278784 ms, enqueue 0.24856 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.191479 ms - Host latency: 0.236401 ms (end to end 0.247485 ms, enqueue 0.218042 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.159009 ms - Host latency: 0.202783 ms (end to end 0.210791 ms, enqueue 0.187964 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161279 ms - Host latency: 0.204663 ms (end to end 0.21145 ms, enqueue 0.189087 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158398 ms - Host latency: 0.201855 ms (end to end 0.210718 ms, enqueue 0.188354 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158203 ms - Host latency: 0.201733 ms (end to end 0.210059 ms, enqueue 0.187524 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15957 ms - Host latency: 0.202051 ms (end to end 0.210547 ms, enqueue 0.17688 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157202 ms - Host latency: 0.198193 ms (end to end 0.218994 ms, enqueue 0.130469 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155688 ms - Host latency: 0.197144 ms (end to end 0.227905 ms, enqueue 0.132983 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156201 ms - Host latency: 0.197681 ms (end to end 0.234741 ms, enqueue 0.131079 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195996 ms (end to end 0.241968 ms, enqueue 0.130591 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.19585 ms (end to end 0.241187 ms, enqueue 0.130518 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15542 ms - Host latency: 0.196655 ms (end to end 0.241724 ms, enqueue 0.131372 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.195801 ms (end to end 0.275513 ms, enqueue 0.102295 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156128 ms - Host latency: 0.198608 ms (end to end 0.285034 ms, enqueue 0.100488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196387 ms (end to end 0.282471 ms, enqueue 0.0990723 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196118 ms (end to end 0.282739 ms, enqueue 0.0994385 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.194238 ms - Host latency: 0.240234 ms (end to end 0.285449 ms, enqueue 0.192261 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.213037 ms - Host latency: 0.259473 ms (end to end 0.27124 ms, enqueue 0.241919 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.206421 ms - Host latency: 0.252393 ms (end to end 0.262476 ms, enqueue 0.233423 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.214893 ms - Host latency: 0.261621 ms (end to end 0.271851 ms, enqueue 0.241943 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200342 ms - Host latency: 0.245288 ms (end to end 0.253882 ms, enqueue 0.226709 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.172241 ms - Host latency: 0.215894 ms (end to end 0.225854 ms, enqueue 0.202563 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.160205 ms - Host latency: 0.202539 ms (end to end 0.21145 ms, enqueue 0.186304 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.152979 ms - Host latency: 0.194409 ms (end to end 0.204126 ms, enqueue 0.17959 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153955 ms - Host latency: 0.195703 ms (end to end 0.204761 ms, enqueue 0.180859 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196851 ms (end to end 0.207178 ms, enqueue 0.160571 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196313 ms (end to end 0.231323 ms, enqueue 0.128955 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154175 ms - Host latency: 0.195532 ms (end to end 0.256226 ms, enqueue 0.129443 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.195972 ms (end to end 0.250488 ms, enqueue 0.13042 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.197046 ms (end to end 0.244775 ms, enqueue 0.1302 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195654 ms (end to end 0.249512 ms, enqueue 0.130762 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196606 ms (end to end 0.249268 ms, enqueue 0.130005 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.196313 ms (end to end 0.276978 ms, enqueue 0.10127 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.196362 ms (end to end 0.281201 ms, enqueue 0.0995361 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196875 ms (end to end 0.276685 ms, enqueue 0.0987061 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155225 ms - Host latency: 0.197241 ms (end to end 0.283472 ms, enqueue 0.100952 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.196191 ms (end to end 0.281958 ms, enqueue 0.100537 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155225 ms - Host latency: 0.196924 ms (end to end 0.28479 ms, enqueue 0.0996338 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.196899 ms (end to end 0.283154 ms, enqueue 0.0949463 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153882 ms - Host latency: 0.195337 ms (end to end 0.270972 ms, enqueue 0.0855957 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154175 ms - Host latency: 0.195654 ms (end to end 0.270947 ms, enqueue 0.0855225 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.196045 ms (end to end 0.273486 ms, enqueue 0.0866211 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154248 ms - Host latency: 0.195996 ms (end to end 0.271167 ms, enqueue 0.0859375 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153809 ms - Host latency: 0.195581 ms (end to end 0.268237 ms, enqueue 0.0858643 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155054 ms - Host latency: 0.196899 ms (end to end 0.268726 ms, enqueue 0.0864746 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.197241 ms (end to end 0.273828 ms, enqueue 0.0862305 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198682 ms - Host latency: 0.245776 ms (end to end 0.2896 ms, enqueue 0.175781 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.20625 ms - Host latency: 0.252612 ms (end to end 0.264038 ms, enqueue 0.235547 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.203613 ms - Host latency: 0.249927 ms (end to end 0.261743 ms, enqueue 0.233667 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.210815 ms - Host latency: 0.257715 ms (end to end 0.268994 ms, enqueue 0.239404 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.187036 ms - Host latency: 0.232056 ms (end to end 0.242163 ms, enqueue 0.216553 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.184912 ms - Host latency: 0.229858 ms (end to end 0.239868 ms, enqueue 0.214014 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.181079 ms - Host latency: 0.225806 ms (end to end 0.235693 ms, enqueue 0.210156 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.160327 ms - Host latency: 0.203809 ms (end to end 0.21106 ms, enqueue 0.188452 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15874 ms - Host latency: 0.202441 ms (end to end 0.210229 ms, enqueue 0.18772 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.158472 ms - Host latency: 0.201929 ms (end to end 0.211182 ms, enqueue 0.188745 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157935 ms - Host latency: 0.20188 ms (end to end 0.209985 ms, enqueue 0.187256 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.163599 ms - Host latency: 0.205884 ms (end to end 0.21875 ms, enqueue 0.152759 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155737 ms - Host latency: 0.196777 ms (end to end 0.227002 ms, enqueue 0.129468 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153979 ms - Host latency: 0.19502 ms (end to end 0.245752 ms, enqueue 0.12998 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154053 ms - Host latency: 0.195435 ms (end to end 0.254297 ms, enqueue 0.130347 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153491 ms - Host latency: 0.194727 ms (end to end 0.26377 ms, enqueue 0.129932 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15376 ms - Host latency: 0.195044 ms (end to end 0.256714 ms, enqueue 0.130664 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.197607 ms (end to end 0.265894 ms, enqueue 0.117456 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195752 ms (end to end 0.276709 ms, enqueue 0.094873 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196313 ms (end to end 0.281787 ms, enqueue 0.0958252 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196582 ms (end to end 0.275049 ms, enqueue 0.0953369 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155298 ms - Host latency: 0.196289 ms (end to end 0.27959 ms, enqueue 0.0966064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195972 ms (end to end 0.282104 ms, enqueue 0.0955811 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155615 ms - Host latency: 0.19751 ms (end to end 0.280127 ms, enqueue 0.0951172 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.196167 ms (end to end 0.279639 ms, enqueue 0.0904053 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195874 ms (end to end 0.275928 ms, enqueue 0.0796143 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153955 ms - Host latency: 0.194971 ms (end to end 0.274756 ms, enqueue 0.0795654 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196021 ms (end to end 0.274121 ms, enqueue 0.0796875 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195435 ms (end to end 0.275488 ms, enqueue 0.0798096 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154517 ms - Host latency: 0.195581 ms (end to end 0.274951 ms, enqueue 0.0796875 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.19624 ms (end to end 0.276196 ms, enqueue 0.0812256 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154687 ms - Host latency: 0.195801 ms (end to end 0.280225 ms, enqueue 0.0788086 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196045 ms (end to end 0.280859 ms, enqueue 0.0788818 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.195337 ms (end to end 0.280591 ms, enqueue 0.0790039 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.195825 ms (end to end 0.280786 ms, enqueue 0.0789551 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.195557 ms (end to end 0.279565 ms, enqueue 0.0782471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.196289 ms (end to end 0.281421 ms, enqueue 0.0788574 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196118 ms (end to end 0.281299 ms, enqueue 0.0777832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154004 ms - Host latency: 0.195288 ms (end to end 0.280151 ms, enqueue 0.0779785 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195654 ms (end to end 0.281543 ms, enqueue 0.077832 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154395 ms - Host latency: 0.195508 ms (end to end 0.288623 ms, enqueue 0.0776611 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.196118 ms (end to end 0.287354 ms, enqueue 0.0772461 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.196045 ms (end to end 0.286694 ms, enqueue 0.078418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196191 ms (end to end 0.286133 ms, enqueue 0.0790283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.170093 ms - Host latency: 0.213501 ms (end to end 0.29397 ms, enqueue 0.101514 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.202686 ms - Host latency: 0.248755 ms (end to end 0.259521 ms, enqueue 0.232031 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.20271 ms - Host latency: 0.248169 ms (end to end 0.258691 ms, enqueue 0.231738 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.19939 ms - Host latency: 0.244897 ms (end to end 0.256567 ms, enqueue 0.229834 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198608 ms - Host latency: 0.243945 ms (end to end 0.255566 ms, enqueue 0.228662 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200049 ms - Host latency: 0.245581 ms (end to end 0.256006 ms, enqueue 0.228491 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.198633 ms - Host latency: 0.243921 ms (end to end 0.255273 ms, enqueue 0.228491 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.180688 ms - Host latency: 0.226074 ms (end to end 0.239575 ms, enqueue 0.188867 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15896 ms - Host latency: 0.201904 ms (end to end 0.232446 ms, enqueue 0.13501 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155591 ms - Host latency: 0.196802 ms (end to end 0.286377 ms, enqueue 0.108716 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.196313 ms (end to end 0.288843 ms, enqueue 0.10791 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.195972 ms (end to end 0.285547 ms, enqueue 0.10874 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155396 ms - Host latency: 0.196484 ms (end to end 0.288159 ms, enqueue 0.107837 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.196338 ms (end to end 0.286743 ms, enqueue 0.11123 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.196924 ms (end to end 0.289453 ms, enqueue 0.115796 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155103 ms - Host latency: 0.196655 ms (end to end 0.285522 ms, enqueue 0.1177 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155029 ms - Host latency: 0.196338 ms (end to end 0.289819 ms, enqueue 0.116943 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155444 ms - Host latency: 0.197339 ms (end to end 0.289429 ms, enqueue 0.115967 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.1552 ms - Host latency: 0.196851 ms (end to end 0.292114 ms, enqueue 0.116895 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155127 ms - Host latency: 0.196729 ms (end to end 0.291064 ms, enqueue 0.116064 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154932 ms - Host latency: 0.196826 ms (end to end 0.282886 ms, enqueue 0.102197 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155615 ms - Host latency: 0.197681 ms (end to end 0.280786 ms, enqueue 0.0925781 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154224 ms - Host latency: 0.195508 ms (end to end 0.275903 ms, enqueue 0.0908691 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154321 ms - Host latency: 0.19541 ms (end to end 0.277197 ms, enqueue 0.0915283 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.206982 ms - Host latency: 0.253955 ms (end to end 0.273389 ms, enqueue 0.227173 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.208203 ms - Host latency: 0.253784 ms (end to end 0.262939 ms, enqueue 0.235059 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.200049 ms - Host latency: 0.245483 ms (end to end 0.257959 ms, enqueue 0.229346 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.201489 ms - Host latency: 0.247046 ms (end to end 0.257251 ms, enqueue 0.229541 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.199609 ms - Host latency: 0.245215 ms (end to end 0.256738 ms, enqueue 0.229517 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164941 ms - Host latency: 0.21748 ms (end to end 0.231226 ms, enqueue 0.171753 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156519 ms - Host latency: 0.207666 ms (end to end 0.222412 ms, enqueue 0.162207 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.156641 ms - Host latency: 0.208203 ms (end to end 0.223071 ms, enqueue 0.16438 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157544 ms - Host latency: 0.210742 ms (end to end 0.225073 ms, enqueue 0.161353 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15835 ms - Host latency: 0.211108 ms (end to end 0.225073 ms, enqueue 0.162085 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155688 ms - Host latency: 0.200854 ms (end to end 0.227344 ms, enqueue 0.139868 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155371 ms - Host latency: 0.196875 ms (end to end 0.281445 ms, enqueue 0.12085 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155322 ms - Host latency: 0.196973 ms (end to end 0.288428 ms, enqueue 0.121875 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154907 ms - Host latency: 0.196289 ms (end to end 0.290308 ms, enqueue 0.12085 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155273 ms - Host latency: 0.197266 ms (end to end 0.291602 ms, enqueue 0.11958 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155249 ms - Host latency: 0.196899 ms (end to end 0.291528 ms, enqueue 0.120093 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155176 ms - Host latency: 0.197119 ms (end to end 0.288257 ms, enqueue 0.114038 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.195947 ms (end to end 0.283984 ms, enqueue 0.101392 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195776 ms (end to end 0.283887 ms, enqueue 0.100269 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154346 ms - Host latency: 0.195801 ms (end to end 0.281226 ms, enqueue 0.100488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196289 ms (end to end 0.282153 ms, enqueue 0.100366 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154614 ms - Host latency: 0.195923 ms (end to end 0.280054 ms, enqueue 0.0996094 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154443 ms - Host latency: 0.195776 ms (end to end 0.280176 ms, enqueue 0.100684 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155688 ms - Host latency: 0.198267 ms (end to end 0.282764 ms, enqueue 0.10105 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.19585 ms (end to end 0.280005 ms, enqueue 0.0992676 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155151 ms - Host latency: 0.196875 ms (end to end 0.28208 ms, enqueue 0.0993896 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154419 ms - Host latency: 0.19541 ms (end to end 0.28125 ms, enqueue 0.0984131 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196289 ms (end to end 0.272217 ms, enqueue 0.100244 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154565 ms - Host latency: 0.195923 ms (end to end 0.281665 ms, enqueue 0.0999756 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154785 ms - Host latency: 0.195923 ms (end to end 0.280371 ms, enqueue 0.100488 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.157666 ms - Host latency: 0.203418 ms (end to end 0.252441 ms, enqueue 0.136865 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.16377 ms - Host latency: 0.208618 ms (end to end 0.220947 ms, enqueue 0.150562 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.162549 ms - Host latency: 0.207739 ms (end to end 0.222095 ms, enqueue 0.152954 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.164038 ms - Host latency: 0.208325 ms (end to end 0.221436 ms, enqueue 0.151172 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.161963 ms - Host latency: 0.207935 ms (end to end 0.220142 ms, enqueue 0.152002 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15752 ms - Host latency: 0.199023 ms (end to end 0.220166 ms, enqueue 0.13418 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154663 ms - Host latency: 0.195557 ms (end to end 0.243066 ms, enqueue 0.129346 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153564 ms - Host latency: 0.194922 ms (end to end 0.256226 ms, enqueue 0.129272 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155054 ms - Host latency: 0.196484 ms (end to end 0.283228 ms, enqueue 0.128467 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154199 ms - Host latency: 0.195264 ms (end to end 0.275269 ms, enqueue 0.132471 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.153809 ms - Host latency: 0.19458 ms (end to end 0.254248 ms, enqueue 0.128101 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154858 ms - Host latency: 0.196533 ms (end to end 0.281641 ms, enqueue 0.119678 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154541 ms - Host latency: 0.196265 ms (end to end 0.280957 ms, enqueue 0.0973145 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155005 ms - Host latency: 0.196606 ms (end to end 0.279272 ms, enqueue 0.0954102 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154956 ms - Host latency: 0.196729 ms (end to end 0.278711 ms, enqueue 0.096582 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154883 ms - Host latency: 0.196582 ms (end to end 0.28103 ms, enqueue 0.0961426 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196631 ms (end to end 0.279761 ms, enqueue 0.0955322 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155054 ms - Host latency: 0.196484 ms (end to end 0.281885 ms, enqueue 0.0954102 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155176 ms - Host latency: 0.196948 ms (end to end 0.279785 ms, enqueue 0.0812256 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154712 ms - Host latency: 0.196094 ms (end to end 0.279443 ms, enqueue 0.080542 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196216 ms (end to end 0.278198 ms, enqueue 0.0806152 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196558 ms (end to end 0.279297 ms, enqueue 0.080957 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196606 ms (end to end 0.280615 ms, enqueue 0.0813232 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15498 ms - Host latency: 0.196411 ms (end to end 0.278931 ms, enqueue 0.0803223 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154834 ms - Host latency: 0.196094 ms (end to end 0.278687 ms, enqueue 0.0800781 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154736 ms - Host latency: 0.196289 ms (end to end 0.278149 ms, enqueue 0.0805908 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154639 ms - Host latency: 0.195923 ms (end to end 0.278613 ms, enqueue 0.0812012 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15459 ms - Host latency: 0.195898 ms (end to end 0.279004 ms, enqueue 0.0802979 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155078 ms - Host latency: 0.19668 ms (end to end 0.278589 ms, enqueue 0.0799316 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15415 ms - Host latency: 0.195483 ms (end to end 0.277612 ms, enqueue 0.0811279 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.154761 ms - Host latency: 0.196143 ms (end to end 0.276538 ms, enqueue 0.0814209 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.15481 ms - Host latency: 0.196411 ms (end to end 0.278491 ms, enqueue 0.0804932 ms)
[12/28/2021-10:07:51] [I] Average on 10 runs - GPU latency: 0.155518 ms - Host latency: 0.196729 ms (end to end 0.28103 ms, enqueue 0.0794678 ms)
[12/28/2021-10:07:51] [I] 
[12/28/2021-10:07:51] [I] === Performance summary ===
[12/28/2021-10:07:51] [I] Throughput: 5806.92 qps
[12/28/2021-10:07:51] [I] Latency: min = 0.191406 ms, max = 0.466675 ms, mean = 0.201973 ms, median = 0.196045 ms, percentile(99%) = 0.275635 ms
[12/28/2021-10:07:51] [I] End-to-End Host Latency: min = 0.196686 ms, max = 0.497559 ms, mean = 0.269491 ms, median = 0.277481 ms, percentile(99%) = 0.305908 ms
[12/28/2021-10:07:51] [I] Enqueue Time: min = 0.0748901 ms, max = 0.432861 ms, mean = 0.108483 ms, median = 0.0831299 ms, percentile(99%) = 0.25708 ms
[12/28/2021-10:07:51] [I] H2D Latency: min = 0.0360107 ms, max = 0.0529785 ms, mean = 0.037567 ms, median = 0.0374756 ms, percentile(99%) = 0.0405273 ms
[12/28/2021-10:07:51] [I] GPU Compute Time: min = 0.151611 ms, max = 0.397583 ms, mean = 0.159947 ms, median = 0.154785 ms, percentile(99%) = 0.229004 ms
[12/28/2021-10:07:51] [I] D2H Latency: min = 0.00268555 ms, max = 0.0205078 ms, mean = 0.00445802 ms, median = 0.00378418 ms, percentile(99%) = 0.0148926 ms
[12/28/2021-10:07:51] [I] Total Host Walltime: 3.00056 s
[12/28/2021-10:07:51] [I] Total GPU Compute Time: 2.78691 s
[12/28/2021-10:07:51] [I] Explanations of the performance metrics are printed in the verbose logs.
[12/28/2021-10:07:51] [V] 
[12/28/2021-10:07:51] [V] === Explanations of the performance metrics ===
[12/28/2021-10:07:51] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.
[12/28/2021-10:07:51] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.
[12/28/2021-10:07:51] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.
[12/28/2021-10:07:51] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.
[12/28/2021-10:07:51] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.
[12/28/2021-10:07:51] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.
[12/28/2021-10:07:51] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.
[12/28/2021-10:07:51] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.
[12/28/2021-10:07:51] [V] End-to-End Host Latency: the duration from when the H2D of a query is called to when the D2H of the same query is completed, which includes the latency to wait for the completion of the previous query. This is the latency of a query if multiple queries are enqueued consecutively.
[12/28/2021-10:07:51] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8001] # trtexec --onnx=default_model.onnx --calib=model.calib --int8 --saveEngine=tmp.trt --verbose
[12/28/2021-10:07:51] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2046, GPU 4009 (MiB)
